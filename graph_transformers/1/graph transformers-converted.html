<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>5e0d968f-cc1c-4601-86f3-944adaa84f93</title><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 .s1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 24pt; }
 .s2 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s4 { color: #7F7F7F; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 20pt; }
 .s5 { color: #7F7F7F; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 20pt; }
 .s6 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .h3 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 h1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 28.5pt; }
 .p, p { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; margin:0pt; }
 .a, a { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s7 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s8 { color: black; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s9 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s10 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 6.5pt; }
 .s11 { color: black; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6.5pt; }
 .s12 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; }
 .s13 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; }
 .s14 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 6.5pt; }
 .s15 { color: black; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; }
 .s16 { color: black; font-family:Century, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; }
 .s17 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: 2pt; }
 .s18 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: 2pt; }
 .s19 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 4pt; vertical-align: 1pt; }
 .s20 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: -1pt; }
 .s21 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s22 { color: black; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s23 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: -1pt; }
 .s24 { color: black; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -1pt; }
 .s25 { color: black; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .h2, h2 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s26 { color: black; font-family:Century, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s27 { color: black; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 3pt; }
 .s28 { color: black; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: 3pt; }
 .s29 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: 3pt; }
 .s30 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s31 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s32 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; vertical-align: 2pt; }
 .s33 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 7pt; }
 .s34 { color: black; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 5pt; }
 .s35 { color: black; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s36 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s37 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s38 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -10pt; }
 .s39 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 5pt; }
 .s40 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 6.5pt; vertical-align: -3pt; }
 .s41 { color: black; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; }
 .s42 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; vertical-align: -4pt; }
 .s43 { color: black; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -4pt; }
 .s44 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -4pt; }
 .s45 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -2pt; }
 .s46 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 2pt; }
 .s47 { color: black; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s48 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: -2pt; }
 .s49 { color: black; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: 2pt; }
 .s50 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s51 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -7pt; }
 .s52 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: underline; font-size: 10pt; vertical-align: -10pt; }
 .s53 { color: black; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -6pt; }
 .s54 { color: black; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 1pt; }
 .s55 { color: black; font-family:"Lucida Handwriting"; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; }
 .s56 { color: black; font-family:"Lucida Handwriting"; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: -2pt; }
 .s57 { color: black; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -1pt; }
 .s58 { color: black; font-family:"Lucida Handwriting"; font-style: normal; font-weight: normal; text-decoration: none; font-size: 3.5pt; vertical-align: -1pt; }
 .s59 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 4.5pt; vertical-align: -1pt; }
 .s60 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 9pt; }
 .s61 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -1pt; }
 .s62 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 12pt; }
 .s63 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s64 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 2pt; }
 .s65 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 7pt; }
 .s66 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: -3pt; }
 .s67 { color: black; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -2pt; }
 .s68 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -2pt; }
 .s69 { color: black; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -2pt; }
 .s70 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 20pt; }
 .s71 { color: black; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 1pt; }
 .s72 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 23pt; }
 .s73 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 1pt; }
 .s74 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 10pt; }
 .s75 { color: black; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 10pt; }
 .s76 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 10pt; }
 .s77 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: bold; text-decoration: underline; font-size: 10pt; }
 .s78 { color: black; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 6pt; vertical-align: -1pt; }
 .s79 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 10pt; }
 .s80 { color: black; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -5pt; }
 .s81 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 1pt; }
 .s82 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 7pt; vertical-align: 1pt; }
 .s83 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: 4pt; }
 .s84 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: 3pt; }
 .s85 { color: black; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 6pt; vertical-align: 3pt; }
 .s86 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 5pt; vertical-align: 4pt; }
 .s87 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: 4pt; }
 .s88 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: bold; text-decoration: underline; font-size: 7pt; vertical-align: 1pt; }
 .s89 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 4.5pt; vertical-align: 3pt; }
 .s90 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 5pt; }
 .s91 { color: black; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -4pt; }
 .s92 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 6.5pt; vertical-align: 3pt; }
 .s93 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: 8pt; }
 .s94 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 7pt; vertical-align: 5pt; }
 .s95 { color: black; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 7pt; vertical-align: 6pt; }
 .s96 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -1pt; }
 .s97 { color: black; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -3pt; }
 .s98 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: bold; text-decoration: underline; font-size: 6.5pt; vertical-align: 3pt; }
 .s99 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 5pt; vertical-align: 8pt; }
 .s100 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: bold; text-decoration: underline; font-size: 7pt; vertical-align: 5pt; }
 .s101 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s102 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -5pt; }
 .s103 { color: black; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: -1pt; }
 .s104 { color: black; font-family:"Bookman Old Style", serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 5pt; vertical-align: 3pt; }
 .s105 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; }
 .s106 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 4.5pt; vertical-align: 3pt; }
 .s107 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 7pt; }
 .s108 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 4.5pt; vertical-align: 2pt; }
 .s109 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 7pt; }
 .s110 { color: black; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -1pt; }
 .s111 { color: black; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: -9pt; }
 .s112 { color: black; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 7pt; vertical-align: 1pt; }
 .s113 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 7pt; vertical-align: 1pt; }
 .s114 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 21pt; }
 .s115 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; vertical-align: -5pt; }
 .s116 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 5pt; }
 .s117 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 4pt; }
 .s118 { color: black; font-family:"Palatino Linotype", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 4pt; }
 .s119 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 4pt; }
 .s120 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s121 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s122 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -4pt; }
 .s123 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 4pt; }
 .s124 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -8pt; }
 .s125 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -4pt; }
 .s126 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s127 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: 4pt; }
 .s128 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; }
 .s129 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; }
 .s130 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; }
 .s131 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: -4pt; }
 .s132 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 8pt; }
 .s133 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s134 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 1pt; }
 .s135 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s136 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 h4 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 8pt; }
 li {display: block; }
 #l1 {padding-left: 0pt;counter-reset: c1 1; }
 #l1> li>*:first-child:before {counter-increment: c1; content: counter(c1, upper-roman)". "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l1> li:first-child>*:first-child:before {counter-increment: c1 0;  }
 #l2 {padding-left: 0pt;counter-reset: d1 1; }
 #l2> li>*:first-child:before {counter-increment: d1; content: counter(d1, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l2> li:first-child>*:first-child:before {counter-increment: d1 0;  }
 #l3 {padding-left: 0pt;counter-reset: e1 1; }
 #l3> li>*:first-child:before {counter-increment: e1; content: counter(e1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l3> li:first-child>*:first-child:before {counter-increment: e1 0;  }
 #l4 {padding-left: 0pt;counter-reset: f1 1; }
 #l4> li>*:first-child:before {counter-increment: f1; content: counter(f1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l4> li:first-child>*:first-child:before {counter-increment: f1 0;  }
 #l5 {padding-left: 0pt;counter-reset: f2 1; }
 #l5> li>*:first-child:before {counter-increment: f2; content: counter(f2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l5> li:first-child>*:first-child:before {counter-increment: f2 0;  }
 #l6 {padding-left: 0pt;counter-reset: g1 1; }
 #l6> li>*:first-child:before {counter-increment: g1; content: counter(g1, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l6> li:first-child>*:first-child:before {counter-increment: g1 0;  }
 #l7 {padding-left: 0pt;counter-reset: h1 1; }
 #l7> li>*:first-child:before {counter-increment: h1; content: counter(h1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l7> li:first-child>*:first-child:before {counter-increment: h1 0;  }
 #l8 {padding-left: 0pt; }
 #l8> li>*:first-child:before {content: "• "; color: black; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 #l9 {padding-left: 0pt;counter-reset: i1 1; }
 #l9> li>*:first-child:before {counter-increment: i1; content: counter(i1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l9> li:first-child>*:first-child:before {counter-increment: i1 0;  }
 #l10 {padding-left: 0pt;counter-reset: i2 1; }
 #l10> li>*:first-child:before {counter-increment: i2; content: counter(i2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l10> li:first-child>*:first-child:before {counter-increment: i2 0;  }
 #l11 {padding-left: 0pt;counter-reset: i2 1; }
 #l11> li>*:first-child:before {counter-increment: i2; content: counter(i2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l11> li:first-child>*:first-child:before {counter-increment: i2 0;  }
 #l12 {padding-left: 0pt;counter-reset: i2 1; }
 #l12> li>*:first-child:before {counter-increment: i2; content: counter(i2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l12> li:first-child>*:first-child:before {counter-increment: i2 0;  }
 #l13 {padding-left: 0pt;counter-reset: i2 1; }
 #l13> li>*:first-child:before {counter-increment: i2; content: counter(i2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l13> li:first-child>*:first-child:before {counter-increment: i2 0;  }
 #l14 {padding-left: 0pt;counter-reset: j1 1; }
 #l14> li>*:first-child:before {counter-increment: j1; content: counter(j1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l14> li:first-child>*:first-child:before {counter-increment: j1 0;  }
 #l15 {padding-left: 0pt;counter-reset: j2 1; }
 #l15> li>*:first-child:before {counter-increment: j2; content: counter(j2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l15> li:first-child>*:first-child:before {counter-increment: j2 0;  }
 #l16 {padding-left: 0pt;counter-reset: j2 1; }
 #l16> li>*:first-child:before {counter-increment: j2; content: counter(j2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l16> li:first-child>*:first-child:before {counter-increment: j2 0;  }
 #l17 {padding-left: 0pt;counter-reset: j2 1; }
 #l17> li>*:first-child:before {counter-increment: j2; content: counter(j2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l17> li:first-child>*:first-child:before {counter-increment: j2 0;  }
 #l18 {padding-left: 0pt;counter-reset: j2 1; }
 #l18> li>*:first-child:before {counter-increment: j2; content: counter(j2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l18> li:first-child>*:first-child:before {counter-increment: j2 0;  }
 #l19 {padding-left: 0pt;counter-reset: j2 1; }
 #l19> li>*:first-child:before {counter-increment: j2; content: counter(j2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l19> li:first-child>*:first-child:before {counter-increment: j2 0;  }
 li {display: block; }
 #l20 {padding-left: 0pt;counter-reset: k1 1; }
 #l20> li>*:first-child:before {counter-increment: k1; content: "["counter(k1, decimal)"] "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l20> li:first-child>*:first-child:before {counter-increment: k1 0;  }
 #l21 {padding-left: 0pt;counter-reset: l1 1; }
 #l21> li>*:first-child:before {counter-increment: l1; content: counter(l1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l21> li:first-child>*:first-child:before {counter-increment: l1 0;  }
 li {display: block; }
 #l22 {padding-left: 0pt;counter-reset: m1 194; }
 #l22> li>*:first-child:before {counter-increment: m1; content: "["counter(m1, decimal)"] "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l22> li:first-child>*:first-child:before {counter-increment: m1 0;  }
 #l23 {padding-left: 0pt;counter-reset: l1 2; }
 #l23> li>*:first-child:before {counter-increment: l1; content: counter(l1, upper-latin)". "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l23> li:first-child>*:first-child:before {counter-increment: l1 0;  }
 table, tbody {vertical-align: top; overflow: visible; }
</style></head><body><p class="s1" style="padding-top: 4pt;padding-left: 71pt;text-indent: 0pt;text-align: center;">Graph Transformers: A Survey</p><p style="text-indent: 0pt;text-align: left;"><span><a href="https://orcid.org/0000-0002-8661-1544"><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="10" height="10" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAKCAYAAACNMs+9AAAABmJLR0QA/wD/AP+gvaeTAAAACXBIWXMAAA7EAAAOxAGVKw4bAAABS0lEQVQYlV2QOy9DYRyHn//bc6StEj1RJ02MTTSkiVtsaGI1G5FIN2GXdJIYbMIXMFj4DBKNRcRlENeIuAxSpOhBe+r0fQ1CxDM9w295fqK1BuDh7XD4/HF9tvR+MgDgNHbudiTGltpiPQUA0VpzXFydOy2uzSuxpBa88Isok0nm8l3uxIIUyweDm5fTW04krSJ2K/feDoJQN7WftR5JrWRDo1PNy55/m442uITtOJ3uOO0tQ7TFevA/n6kET+IHZccqfZz1AYgolFjU6h57d4sAdLmTlCoXPH+c9Sv+IX9ciU1I7G93oul9A2DAGIOITTzcQUs4xWv1isD4xKPpvd8YS0VUSDUQthzsUCO1uke5egOgR1IrWZWIdW9nkrl8oCvaD17Ma/XKPL0fmXL12gA6k8zlE7HubTHGAFD09rPnjxsz/w93m3oLAF/oYovYVcViUwAAAABJRU5ErkJgggAA"/></td></tr></table></a></span></p><p class="s2" style="padding-top: 7pt;padding-left: 85pt;text-indent: -49pt;line-height: 106%;text-align: left;">Ahsan Shehzad<span><a href="https://orcid.org/0000-0003-2116-2183"><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="10" height="10" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAKCAYAAACNMs+9AAAABmJLR0QA/wD/AP+gvaeTAAAACXBIWXMAAA7EAAAOxAGVKw4bAAABQElEQVQYlX2Qv0sCcRyG3+/nLk9PD1TI04ymApUEUXGLwr0/wK0ht4qgpsApaCoa2pqipb8iKdr8QYNhDi5OnYdYeN11nnXfpuzH0LO+7/I8jHMOANCN+5VH/Wrv2erkASAkJxrJSOlkNpC5Y4yBcc7xoF0ctPuXhwSRJu4rvmFuOlauLEc3jpg2aq5Wu9tVRZon2RNF36jjN8wtLp4VhfXN0Kkx7qVkjwqvEEQ2vouQbwmKtACA483R4bhmUBya7QIAEBMhkAcAR+vpHB/cQWZuCyO7h6HZLhD+gYEgkg8AQGF/qvZzFEhCRMlBVfJ4sbuwJwOE/aka00aNtWp353qGZBJIQkCKQyQvLEfHaNybypCq5G7SsXJl4lqu/T7EwGxBM+rTUzpWrqhK9pb9Cb4/tDo5AAjLiWYyUjr+Cv4JfwF/IqAcilQAAAAASUVORK5CYIIA"/></td></tr></table></a></span>, Feng Xia<span><a href="https://orcid.org/0000-0002-8324-1859"><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="10" height="10" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAKCAYAAACNMs+9AAAABmJLR0QA/wD/AP+gvaeTAAAACXBIWXMAAA7EAAAOxAGVKw4bAAABLklEQVQYlX2Qv0sCYRyHP9/3fmgnr6TJXS4tBV7QDYfOQrT2FzgF5dbYFjk0NLUVzUHQ1p8QgWOpEQQqOTnEnYhSh2fX6b1NWTb0rJ/P8jwkhAAA9LzHYrN3fTD023kASGm5+rpeOtW5XQUAEkLg2bk8bLpXx0QyC6cefqDIypaPNpZ3Tsh9bxRvO/t3i/FVFlNScL0HzEPR1tr5prS9y8+8oGtqqgFNWYKpl5BJWEjGVsAgww9dCiZvGTbwWwUAYCRDYjFEIsTT6wVe+jcweB4ECUO/nWf4B4mpYKQAAFhaM2vzYxwGL8DgBQzHHUzFB1Jarj6TUaQEkymOBTUDVUpi9OnAC7ozGaZzu2pl9yrhdBSNJ30M/BYc7352srLlis7tKv0N/i2X1sza7+BfLWF8eA3gf5AAAAAASUVORK5CYIIA"/></td></tr></table></a></span>, <i>Senior Member, IEEE</i>, Shagufta Abid, Ciyuan Peng, <i>Graduate Student Member, IEEE</i>, Shuo Yu, <i>Member, IEEE</i>, Dongyu Zhang, and Karin Verspoor</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 1pt;text-indent: 0pt;text-align: left;"><a href="#bookmark71" class="s5">arXiv:2407.09777v1 [cs.LG] 13 </a>Jul 2024</p><p style="text-indent: 0pt;text-align: left;"/><p class="s6" style="padding-top: 6pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;">Abstract<span class="h3">—Graph transformers are a recent advancement in machine learning, offering a new class of neural network models for graph-structured data. The synergy between transformers and graph learning demonstrates strong performance and ver- satility across various graph-related tasks. This survey provides an in-depth review of recent progress and challenges in graph transformer research. We begin with foundational concepts of graphs and transformers. We then explore design perspec- tives of graph transformers, focusing on how they integrate graph inductive biases and graph attention mechanisms into the transformer architecture. Furthermore, we propose a taxonomy classifying graph transformers based on depth, scalability, and pre-training strategies, summarizing key principles for effective development of graph transformer models. Beyond technical analysis, we discuss the applications of graph transformer models for node-level, edge-level, and graph-level tasks, exploring their potential in other application scenarios as well. Finally, we identify remaining challenges in the field, such as scalability and efficiency, generalization and robustness, interpretability and explainability, dynamic and complex graphs, as well as data quality and diversity, charting future directions for graph transformer research.</span></p><p class="s6" style="padding-top: 6pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;">Index Terms<span class="h3">—Graph transformer, attention, graph neural network, representation learning, graph learning, network em- bedding</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="text-indent: 0pt;text-align: left;">G</h1><p style="text-indent: 0pt;text-align: left;"/><ol id="l1"><li data-list-text="I."><p style="padding-left: 105pt;text-indent: -10pt;text-align: left;"><a name="bookmark0">&zwnj;</a>Introduction</p><p style="padding-top: 7pt;padding-left: 31pt;text-indent: 0pt;text-align: justify;">RAPHS, as data structures with high expressiveness, are widely used to present complex data in diverse domains,</p><p style="padding-left: 7pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark60" class="a">such as social media, knowledge graphs, biology, chemistry, and transportation networks </a><a href="#bookmark61" class="a">[1]. They capture both structural and semantic information from data, facilitating various tasks, such as recommendation </a><a href="#bookmark62" class="a">[2], question answering </a><a href="#bookmark63" class="a">[3], anomaly detection </a><a href="#bookmark64" class="a">[4], sentiment analysis </a><a href="#bookmark65" class="a">[5], text generation </a><a href="#bookmark66" class="a">[6], and information retrieval </a><a href="#bookmark67" class="a">[7]. To effectively deal with graph- structured data, researchers have developed various graph learning models, such as graph neural networks (GNNs), learn- ing meaningful representations of nodes, edges and graphs </a><a href="#bookmark68" class="a">[8]. Particularly, GNNs following the message-passing framework iteratively aggregate neighboring information and update node representations, leading to impressive performance on various graph-based tasks </a>[9]. Applications ranging from information</p><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-left: 7pt;text-indent: 7pt;text-align: justify;">A. Shehzad, S. Abid, and D. Zhang are with School of Soft- ware, Dalian University of Technology, Dalian 116620, China (e-mail:</p><p class="s8" style="padding-left: 7pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">{<span class="s7">ahsan.shehzad;shagufta.abid</span>}<a href="mailto:zhangdongyu@dlut.edu.cn" class="s9" target="_blank">@outlook.com; zhangdongyu@dlut.edu.cn)</a></p><p style="padding-left: 7pt;text-indent: 7pt;text-align: justify;"><a href="mailto:f.xia@ieee.org" class="s9" target="_blank">F. Xia and K. Verspoor are with School of Computing Technologies, RMIT University, Melbourne, VIC 3000, Australia (e-mail: </a><a href="mailto:karin.verspoor@rmit.edu.au" class="s9" target="_blank">f.xia@ieee.org; karin.verspoor@rmit.edu.au)</a></p><p style="padding-left: 7pt;text-indent: 7pt;text-align: justify;"><a href="mailto:ciyuan.p@ieee.org" class="s9" target="_blank">C. Peng is with the Institute of Innovation, Science and Sustain- ability, Federation University Australia, Ballarat 3353, Australia (e-mail: ciyuan.p@ieee.org)</a></p><p style="padding-left: 7pt;text-indent: 7pt;text-align: justify;"><a href="mailto:shuo.yu@ieee.org" class="s9" target="_blank">S. Yu is with School of Computer Science and Technology, Dalian Univer- sity of Technology, Dalian 116024, China (e-mail: shuo.yu@ieee.org)</a></p><p class="s7" style="padding-left: 15pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">Corresponding author: Feng Xia</p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark69" class="a">extraction to recommender systems have benefited from GNN modelling of knowledge graphs </a><a href="#bookmark69">[10].</a></p><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark70" class="a">More recently, the graph transformer, as a newly arisen and potent graph learning method, has attracted great attention in both academic and industrial communities </a><a href="#bookmark71" class="a">[11], </a><a href="#bookmark72" class="a">[12]. Graph transformer research is inspired by the success of transformers in natural language processing (NLP) </a><a href="#bookmark73" class="a">[13] and computer vision (CV) </a><a href="#bookmark74" class="a">[14], coupled with the demonstrated value of GNNs. Graph transformers incorporate graph inductive bias (e.g., prior knowledge or assumptions about graph properties) to ef- fectively process graph data </a><a href="#bookmark75" class="a">[15]. Furthermore, they can adapt to dynamic and heterogeneous graphs, leveraging both node and edge features and attributes. </a><a href="#bookmark76" class="a">[16]. Various adaptations and expansions of graph transformers have shown their superiority in tackling diverse challenges of graph learning, such as large- scale graph processing </a>[17]. Furthermore, graph transformers have been successfully employed in various domains and applications, demonstrating their effectiveness and versatility.</p><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark77" class="a">Existing surveys do not adequately cover the latest advance- ments and comprehensive applications of graph transformers. In addition, most do not provide a systematic taxonomy of graph transformer models. For instance, Chen et al. </a><a href="#bookmark71" class="a">[18] focused primarily on the utilization of GNNs and graph transformers in CV, but they failed to summarize the taxonomy of graph transformer models and ignored other domains, such as NLP. Similarly, Mu¨ller et al. </a><a href="#bookmark78" class="a">[12] offered an overview of graph transformers and their theoretical properties, but they did not provide a comprehensive review of existing methods or evaluate their performance on various tasks. Lastly, Min et al. </a>[19] concentrated on the architectural design aspect of graph transformers, offering a systematic evaluation of different components on different graph benchmarks, but they did not include significant applications of graph transformers or discuss open issues in this field.</p><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;">To fill these gaps, this survey aims to present a compre- hensive and systematic review of recent advancements and challenges in graph transformer research from both design and application perspectives. In comparison to existing surveys, our main contributions are as follows:</p><ol id="l2"><li data-list-text="1)"><p style="padding-top: 6pt;padding-left: 32pt;text-indent: -14pt;text-align: justify;">We provide a comprehensive review of the design per- spectives of graph transformers, including graph induc- tive bias and graph attention mechanisms. We classify these techniques into different types and discuss their advantages and limitations.</p></li><li data-list-text="2)"><p style="padding-left: 32pt;text-indent: -14pt;text-align: justify;">We present a novel taxonomy of graph transformers based on their depth, scalability, and pre-training strat- egy. We also provide a guide to choosing effective graph transformer architectures for different tasks and scenarios.</p></li><li data-list-text="3)"><p style="padding-top: 4pt;padding-left: 32pt;text-indent: -14pt;text-align: justify;">We review the application perspectives of graph trans- formers in various graph learning tasks, as well as the application scenarios in other domains, such as NLP and CV tasks.</p></li><li data-list-text="4)"><p style="padding-left: 32pt;text-indent: -14pt;text-align: justify;">We identify the crucial open issues and future directions of graph transformer research, such as the scalabil- ity, generalization, interpretability, and explainability of models, efficient temporal graph learning, and data- related issues.</p></li></ol><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark6" class="a">An overview of this paper is depicted in Figure </a><a href="#bookmark4" class="a">1. The subsequent survey is structured as follows: Section </a><a href="#bookmark17" class="a">II intro- duces notations and preliminaries pertaining to graphs and transformers. Section </a><a href="#bookmark26" class="a">III delves into the design perspectives of graph transformers that encompass graph inductive bias and graph attention mechanisms. Section </a><a href="#bookmark47" class="a">IV presents a taxon- omy of graph transformers categorizing them based on their depth, scalability and pre-training strategy. Additionally, a guide is provided for selecting appropriate graph transformer models for diverse tasks and domains. Section </a><a href="#bookmark56" class="a">V explores the application perspectives of graph transformers on various node-level, edge-level, and graph-level tasks, along with other application scenarios. Section </a><a href="#bookmark58" class="a">VI identifies open issues and future directions for research on graph transformers. Lastly, Section </a>VII concludes the paper and highlights its main contributions.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="II."><p style="padding-left: 69pt;text-indent: -14pt;text-align: left;"><a name="bookmark1">&zwnj;</a>Notations and Preliminaries<a name="bookmark4">&zwnj;</a></p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark5" class="a">In this section, we present fundamental notations and con- cepts utilized throughout this survey paper. Additionally, we provide a concise summary of current methods for graph learning and self-attention mechanisms which serve as the basis for graph transformers. Table </a>I includes mathematical notations used in this paper.</p><p style="padding-top: 2pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-left: 83pt;text-indent: 0pt;line-height: 9pt;text-align: center;"><a name="bookmark5">&zwnj;</a>TABLE I</p><p class="s7" style="padding-left: 83pt;text-indent: 0pt;line-height: 9pt;text-align: center;">Notations in this paper</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 1pt;text-align: left;"/><p class="s10" style="padding-left: 7pt;text-indent: 0pt;line-height: 7pt;text-align: left;"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Notation &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Definition &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p class="s11" style="padding-left: 46pt;text-indent: -13pt;line-height: 85%;text-align: left;">G <span class="s12">= (</span>V, E<span class="s12">)    </span><span class="s13">A graph with node set </span>V <span class="s13">and edge set </span>E N       <span class="s13">Number of nodes</span></p><p class="s11" style="padding-left: 46pt;text-indent: 0pt;line-height: 7pt;text-align: left;">M      <span class="s13">Number of edges</span></p><p class="s17" style="padding-left: 32pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><span class="s14">A </span><span class="s15">∈ </span><span class="s16">R</span>N<span class="s18">×</span>N     <span class="s13">Adjacency matrix of graph </span><span class="s11">G</span></p><p class="s14" style="padding-left: 31pt;text-indent: 0pt;line-height: 8pt;text-align: left;">X <span class="s15">∈ </span><span class="s16">R</span><span class="s17">N</span><span class="s18">×</span><span class="s17">d</span><span class="s19">n    </span><span class="s13">Node feature matrix, </span>x<span class="s20">i </span><span class="s15">∈ </span>X</p><p class="s17" style="padding-left: 32pt;text-indent: 0pt;line-height: 7pt;text-align: left;"><span class="s14">F </span><span class="s15">∈ </span><span class="s16">R</span>M<span class="s18">×</span>d<span class="s19">e     </span><span class="s13">Edge feature matrix</span></p><p class="s21" style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">E <span class="s22">=  </span>e<span class="s23">1</span>, e<span class="s23">2</span>, . . . , e<span class="s24">M</span><span class="s25">  </span><span class="p">is edge set with </span>M <span class="p">edges. Edge </span>e<span class="s24">k</span><span class="s25"> </span><span class="s22">= (</span>v<span class="s24">i</span>, v<span class="s24">j</span><span class="s22">) </span><span class="p">indicates the connection between node </span>v<span class="s24">i</span><span class="s25"> </span><span class="p">and node </span>v<span class="s24">j</span><span class="p">, where </span>i, j   <span class="s22">1</span>, <span class="s22">2</span>, . . . , N  <span class="p">and </span>k   <span class="s22">1</span>, <span class="s22">2</span>, . . . , M <span class="p">. A graph can be represented by an adjacency matrix </span><b>A  </b><span class="s26">R</span><span class="s27">N</span><span class="s28">×</span><span class="s27">N</span><span class="s25"> </span><span class="p">, where </span>A<span class="s24">ij</span><span class="s25"> </span><span class="p">indicates the presence or absence of an edge between node </span>v<span class="s24">i</span><span class="s25"> </span><span class="p">and node </span>v<span class="s24">j</span><span class="p">. Alternatively, a graph can be represented by the edge list </span>E  <span class="s26">R</span><span class="s27">M</span><span class="s28">×</span><span class="s29">2</span><span class="p">, where each row of </span>E <span class="p">contains the indices of two nodes connected by an edge. A graph can also have node features and edge features that describe the attributes or properties of nodes and edges, respectively. The features of the nodes can be represented by a feature matrix </span><b>X  </b><span class="s26">R</span><span class="s27">N</span><span class="s28">×</span><span class="s27">d</span><span class="s17">n </span><span class="p">, where </span>d<span class="s24">n</span><span class="s25"> </span><span class="p">is the dimension of the node features. The edge features can be represented by a feature tensor </span><b>F  </b><span class="s26">R</span><span class="s27">M</span><span class="s28">×</span><span class="s27">d</span><span class="s17">e </span><span class="p">, where </span>d<span class="s24">e</span><span class="s25"> </span><a href="#bookmark79" class="a">is the dimension of the edge features </a><a href="#bookmark79">[20].</a></p><p class="s30" style="text-indent: 0pt;line-height: 12pt;text-align: left;">∈</p><p style="text-indent: 0pt;text-align: left;"/><p class="s30" style="text-indent: 0pt;line-height: 12pt;text-align: left;">∈</p><p style="text-indent: 0pt;text-align: left;"/><p class="s30" style="text-indent: 0pt;line-height: 12pt;text-align: left;">∈</p><p style="text-indent: 0pt;text-align: left;"/><p class="s30" style="text-indent: 0pt;line-height: 12pt;text-align: left;">∈</p><p style="text-indent: 0pt;text-align: left;"/><p class="s30" style="text-indent: 0pt;line-height: 12pt;text-align: left;">{     }</p><p style="text-indent: 0pt;text-align: left;"/><p class="s30" style="text-indent: 0pt;line-height: 12pt;text-align: left;">∈ {     }    ∈ {     }</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark80" class="a">Graph learning refers to the task of acquiring low- dimensional vector representations, also known as embeddings for nodes, edges, or the entire graph. These embeddings are designed to capture both structural and semantic information of the graph. GNNs are a type of neural network model that excels at learning from graph-structured data. They achieve this by propagating information along edges and aggregating information from neighboring nodes </a>[21]. GNNs can be cat- egorized into two main groups: spectral methods and spatial methods.</p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark81" class="a">Spectral methods are based on graph signal processing and graph Fourier transform, implementing convolution operations on graphs in the spectral domain </a>[22]. The Fourier graph</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">transform is defined as <span class="s31">X</span><span class="s32">ˆ </span><span class="s22">= </span><b>U</b><span class="s27">T</span><span class="s25"> </span><b>XU</b>, where <span class="s31">X</span><span class="s32">ˆ </span>is the spec-</p><p style="text-indent: 0pt;line-height: 18pt;text-align: left;">and <b>D </b>is diagonal degree matrix with <i>D  </i><span class="s22">= </span><span class="s33">Σ</span><span class="s34">N  </span><i>A </i>.</p><p style="text-indent: 0pt;text-align: left;"/><h2 style="padding-left: 7pt;text-indent: 0pt;text-align: justify;"><span class="p">tral representation of node feature matrix </span>X <span class="p">and </span>U <span class="p">is the eigenvector matrix of the normalized graph Laplacian matrix </span>L <span class="s22">= </span>I<span class="s24">N</span><span class="s25"> </span><span class="s30">− </span>D<span class="s28">−</span><span class="s29">1</span><span class="s27">/</span><span class="s29">2</span>AD<span class="s28">−</span><span class="s29">1</span><span class="s27">/</span><span class="s29">2</span><span class="p">, where </span>I<span class="s24">N</span><span class="s25"> </span><span class="p">is the identity matrix</span></h2><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">j<span class="s36">=1</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="padding-top: 1pt;text-indent: 0pt;line-height: 9pt;text-align: right;">ii       ij</p><p style="padding-left: 7pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark82" class="a">Spectral methods can capture global information about the graph, but they suffer from high computational complexity, poor scalability, and the lack of generalization to unseen graphs </a><a href="#bookmark82">[23].</a></p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark83" class="a">Spatial methods are based on message-passing and neigh- borhood aggregation, implementing convolution operations on graphs in the spatial domain </a>[24]. The message-passing framework is defined as:</p><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">v</p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">v</p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">u</p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">v</p><p style="text-indent: 0pt;text-align: left;"/><p class="s37" style="padding-left: 88pt;text-indent: 0pt;line-height: 61%;text-align: left;">,  <span class="s38">M        </span></p><p class="s39" style="padding-left: 5pt;text-indent: 0pt;line-height: 4pt;text-align: left;">v</p><p class="s11" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">ϕ      <span class="s13">An update function for node states</span></p><p class="s15" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: left;">⊕       <span class="s13">An aggregation function for neighbor states</span></p><p class="s11" style="text-indent: 0pt;line-height: 8pt;text-align: left;">N <span class="s12">(</span>v<span class="s12">)     </span><span class="s13">Neighbor set of node </span>v</p><p style="text-indent: 0pt;text-align: left;"/><p class="s41" style="text-indent: 0pt;line-height: 64%;text-align: right;"><span class="s40">h</span>(<span class="s39">l</span>)</p><p class="s13" style="padding-left: 39pt;text-indent: 0pt;text-align: left;">Hidden state of node <span class="s11">v </span>at layer <span class="s11">l</span></p><p class="s36" style="padding-left: 44pt;text-indent: 0pt;line-height: 40%;text-align: left;"><span class="s42">h</span>(<span class="s35">l</span>+1) <span class="s43">= </span><span class="s44">ϕ </span><span class="s45"></span><span class="s42">h</span>(<span class="s35">l</span>)<span class="s44">,</span></p><p class="s29" style="padding-bottom: 3pt;padding-left: 29pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s21">f </span><span class="s22">(</span><span class="h2">h</span>(<span class="s27">l</span>)<span class="s21">, </span><span class="h2">h</span>(<span class="s27">l</span>)<span class="s21">, </span><span class="h2">e</span><span class="s24">uv</span><span class="s22">)</span><span class="s46"></span><span class="s21">,   </span><span class="p">(1)</span></p><p class="s35" style="text-indent: 0pt;line-height: 8pt;text-align: left;">u<span class="s47">∈N </span><span class="s36">(</span>v<span class="s36">)</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="text-indent: 0pt;line-height: 7pt;text-align: right;">f      <span class="s13">A message function for the node and edge states</span></p><p class="s14" style="text-indent: 0pt;line-height: 7pt;text-align: right;">Q<span class="s11">, </span>K<span class="s11">, </span>V     <span class="s13">Query, key and value matrices for self-attention</span></p><p class="s11" style="padding-left: 46pt;text-indent: 0pt;line-height: 8pt;text-align: left;">d<span class="s20">k       </span><span class="s13">Dimension of query and key matrices</span></p><p class="s14" style="padding-left: 46pt;text-indent: 0pt;line-height: 8pt;text-align: left;">p<span class="s48">i      </span><span class="s13">Positional encoding of node </span><span class="s11">v</span><span class="s20">i</span></p><p class="s11" style="padding-left: 40pt;text-indent: 0pt;line-height: 7pt;text-align: left;">d<span class="s12">(</span>i, j<span class="s12">)    </span><span class="s13">The shortest path distance between node </span>v<span class="s20">i </span><span class="s13">and node </span>v<span class="s20">j</span></p><p class="s20" style="padding-left: 45pt;text-indent: 0pt;line-height: 7pt;text-align: left;"><span class="s14">e</span>ij      <span class="s13">Edge feature between node </span><span class="s11">v</span>i <span class="s13">and node </span><span class="s11">v</span>j</p><p class="s11" style="padding-left: 45pt;text-indent: 0pt;line-height: 7pt;text-align: left;">a<span class="s20">ij     </span><span class="s13">Attention score between node </span>v<span class="s20">i </span><span class="s13">and node </span>v<span class="s20">j</span></p><p class="s11" style="padding-left: 43pt;text-indent: 0pt;line-height: 8pt;text-align: left;">W, b      <span class="s13">Learnable parameters for the self-attention layer</span></p><p class="s14" style="padding-left: 12pt;text-indent: 0pt;line-height: 9pt;text-align: left;">L <span class="s12">= </span>I<span class="s20">N </span><span class="s15">− </span>D<span class="s18">−</span><span class="s49">1</span><span class="s17">/</span><span class="s49">2</span>AD<span class="s18">−</span><span class="s49">1</span><span class="s17">/</span><span class="s49">2  </span><span class="s13">Normalized graph Laplacian matrix</span></p><p class="s14" style="padding-bottom: 2pt;padding-left: 47pt;text-indent: 0pt;line-height: 7pt;text-align: left;">U       <span class="s13">Eigenvectors matrix of </span>L</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 1pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l3"><li data-list-text="A."><p class="s21" style="padding-left: 21pt;text-indent: -13pt;text-align: left;"><a name="bookmark2">&zwnj;</a>Graphs and Graph Neural Networks</p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 9pt;text-align: left;">A graph is a data structure consisting of a set of nodes (or vertices) <i>V </i>and a set of edges (or links) <i>E </i>that connect pairs</p><p class="s21" style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;line-height: 88%;text-align: left;"><span class="p">of nodes. Formally, a graph can be defined as </span>G <span class="s22">= (</span>V, E<span class="s22">)</span><span class="p">, where </span>V <span class="s22">= </span><span class="s30">{</span>v<span class="s23">1</span>, v<span class="s23">2</span>, . . . , v<span class="s24">N</span><span class="s25"> </span><span class="s30">} </span><span class="p">is node set with </span>N <span class="p">nodes and</span></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">where <b>h</b><span class="s29">(</span><span class="s27">l</span><span class="s29">)</span><span class="s50"> </span>is the hidden state of node <i>v </i>at layer <i>l</i>, <i>ϕ </i>is an update function and  is an aggregation function.  <span class="s22">(</span><i>v</i><span class="s22">) </span>is the set of neighbors of node <i>v </i>and <i>f </i>is a message function that depends on node states and edge features. <b>e</b><span class="s24">uv</span><span class="s25"> </span>is the feature vector of the edge between nodes <i>u </i>and <i>v</i><a href="#bookmark84" class="a">. Spatial methods can capture local information of the graph, but they have limitations in modeling long-range dependencies, complex interactions and heterogeneous structures </a><a href="#bookmark84">[25].</a></p><p style="padding-top: 6pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">v</p><p style="text-indent: 0pt;text-align: left;"/><p class="s30" style="text-indent: 0pt;line-height: 12pt;text-align: left;">⊕           N</p><p style="text-indent: 0pt;text-align: left;"/></li><li data-list-text="B."><p class="s21" style="padding-left: 21pt;text-indent: -13pt;text-align: justify;"><a name="bookmark3">&zwnj;</a>Self-attention and transformers</p></li></ol><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark85" class="a">Self-attention is a mechanism that enables a model to learn to focus on pertinent sections of input or output sequences </a>[26]. It calculates a weighted sum of all elements in a</p><p style="padding-top: 6pt;text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 19pt;text-indent: 0pt;text-align: left;"><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="635" height="150" src="data:image/jpg;base64,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"/></td></tr><tr><td><img width="635" height="150" src="data:image/jpg;base64,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"/></td></tr></table></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-left: 7pt;text-indent: 0pt;text-align: left;"><a name="bookmark6">&zwnj;</a>Fig. 1. Organization of this paper</p><p style="padding-top: 4pt;text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;">sequence with weights determined by the similarity between each element and a query vector. Formally, self-attention is defined as:</p><h2 style="text-indent: 0pt;text-align: left;"><span class="p">Attention</span><span class="s22">(</span>Q<i>, </i>K<i>, </i>V<span class="s22">) = </span><span class="p">softmax</span></h2><p style="text-indent: 0pt;text-align: left;"/><p class="s30" style="text-indent: 0pt;line-height: 70%;text-align: left;">√<span class="s51">d</span></p><p style="text-indent: 0pt;text-align: left;"/><h2 style="text-indent: 0pt;text-align: left;">V<i>,  </i><span class="p">(2)</span></h2><p style="text-indent: 0pt;text-align: left;"/><p class="s37" style="padding-left: 169pt;text-indent: 0pt;text-align: left;"> <span class="s52">QK</span><span class="s53">T </span>!</p><h2 style="padding-top: 6pt;padding-left: 7pt;text-indent: 0pt;line-height: 81%;text-align: justify;"><span class="p">of two main components: an encoder and a decoder. The en- coder takes an input sequence </span>X <span class="s22">= </span><span class="s30">{</span>x<span class="s23">1</span><i>, </i>x<span class="s23">2</span><i>, . . . , </i>x<span class="s24">N</span><span class="s25"> </span><span class="s30">} </span><span class="p">and gen- erates a sequence of hidden states </span>Z <span class="s22">= </span><span class="s30">{</span>z<span class="s23">1</span><i>, </i>z<span class="s23">2</span><i>, . . . , </i>z<span class="s24">N</span><span class="s25"> </span><span class="s30">}</span><span class="p">. The decoder takes an output sequence </span>Y <span class="s22">= </span><span class="s30">{</span>y<span class="s23">1</span><i>, </i>y<span class="s23">2</span><i>, . . . , </i>y<span class="s24">N</span><span class="s25"> </span><span class="s30">} </span><span class="p">and generates a sequence of hidden states </span>S <span class="s22">= </span><span class="s30">{</span>s<span class="s23">1</span><i>, </i>s<span class="s23">2</span><i>, . . . , </i>s<span class="s24">N</span><span class="s25"> </span><span class="s30">}</span><span class="p">.</span></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">k</p><p style="padding-left: 193pt;text-indent: 0pt;line-height: 7pt;text-align: left;"/><p style="text-indent: 0pt;line-height: 11pt;text-align: left;">The decoder also uses an attention mechanism to attend to the</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;">where <b>Q</b>, <b>K </b>and <b>V </b>are query, key and value matri- ces,respectively. <i>d</i><span class="s24">k</span><span class="s25"> </span>is the dimension of the query and key matrices. Self-attention can capture long-range dependencies,</p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">encoder&#39;s hidden states. Formally, the encoder and decoder are defined as:</p><h2 style="padding-top: 8pt;padding-left: 69pt;text-indent: 0pt;line-height: 9pt;text-align: left;">z<span class="s24">i</span><span class="s25"> </span><span class="s22">= </span><span class="p">EncoderLayer</span><span class="s22">(</span>x<span class="s24">i</span><i>, </i>Z<span class="s24">&lt;i</span><span class="s22">)</span><i>,</i></h2><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;">global context and variable-length sequences without using recurrence or convolution.</p><h2 style="padding-top: 5pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">s<span class="s24">j</span><span class="s25"> </span><span class="s22">= </span><span class="p">DecoderLayer</span><span class="s22">(</span>y<span class="s24">j</span><i>, </i>S<span class="s24">&lt;j</span></h2><p style="padding-left: 66pt;text-indent: 0pt;line-height: 9pt;text-align: left;">(3)</p><p class="s21" style="text-indent: 0pt;line-height: 10pt;text-align: left;">, <b>Z</b><span class="s22">)</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 10pt;text-indent: 0pt;text-align: left;"><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="338" height="263" src="data:image/jpg;base64,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"/></td></tr></table></span></p><p style="padding-top: 6pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark72" class="s9" name="bookmark7">Fig. 2. An illustration of transformer architecture [13].</a></p><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 9pt;text-align: left;"><a href="#bookmark72" class="a">Transformers are neural network models that use self- attention as the main building block </a>[13]. Transformers consist</p><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark86" class="a">Here, EncoderLayer and DecoderLayer are composed of mul- tiple self-attention and feed-forward sublayers. Transformers can achieve state-of-the-art results on various tasks, such as machine translation </a><a href="#bookmark87" class="a">[27], text mining </a><a href="#bookmark88" class="a">[28], </a>[29], document</p><p style="padding-left: 7pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark89" class="a">comprehending </a><a href="#bookmark90" class="a">[30], </a><a href="#bookmark91" class="a">[31], image retrieval </a><a href="#bookmark62" class="a">[32], visual question answering </a><a href="#bookmark92" class="a">[3], image generation </a><a href="#bookmark93" class="a">[33], and recommendation systems </a><a href="#bookmark94" class="a">[34], </a><a href="#bookmark7" class="a">[35]. An overview of vanilla transformer is shown in Figure </a><a href="#bookmark7">2.</a></p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark95" class="a">Graph transformers integrate graph inductive bias into trans- former to acquire knowledge from graph-structured data </a><a href="#bookmark71" class="a">[36]. By employing self-attention mechanisms on nodes and edges, graph transformers can effectively capture both local and global information of the graph. In particular, graph trans- formers exhibit the ability to handle heterogeneous graphs containing diverse types of nodes and edges, as well as complex graphs featuring higher-order structures </a><a href="#bookmark96" class="a">[12], </a><a href="#bookmark96">[37].</a></p><p style="padding-top: 3pt;text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="III."><p style="padding-left: 33pt;text-indent: -18pt;text-align: left;"><a name="bookmark8">&zwnj;</a>Design Perspectives of graph transformers<a name="bookmark17">&zwnj;</a></p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;">In this section, we discuss the primary architectures of graph transformers, aiming to explore their design perspectives in depth. Particularly, we will focus on two key components:</p><p style="text-indent: 0pt;text-align: left;"><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="329" height="179" src="data:image/jpg;base64,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"/></td></tr></table></span></p><p style="padding-top: 4pt;padding-left: 270pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark99" class="a">In NLP, each token receives a feature vector that captures its relative position and relationship to other words in the sequence </a><a href="#bookmark100" class="a">[40]. Likewise, graph transformers assign feature vectors to nodes based on their distance and relationships with other nodes in the graph </a>[41]. This encoding technique aims to preserve the local connectivity and neighborhood information of nodes, which is critical for tasks like node classification, link prediction, and graph generation.</p><p style="padding-left: 270pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark70" class="a">A proficient approach for integrating local node positional information involves the utilization of one-hot vectors. These vectors represent the hop distance between a node and its neighboring nodes </a><a href="#bookmark70">[11]:</a></p><p class="s7" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;"><a name="bookmark18">&zwnj;</a>Fig. 3. Graph Inductive bias: (A) and (B) are local and global node positional biases, respectively. (C) and (D) are local and global edge structural biases, respectively. (E) and (F) are local and global attention biases, respectively. Notably, message-passing bias is a kind of local attention bias.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: justify;">graph inductive biases and graph attention mechanisms, to un- derstand how these elements shape graph transformer models’ capabilities.</p><p style="padding-top: 4pt;text-indent: 0pt;text-align: left;"><br/></p><ol id="l4"><li data-list-text="A."><p class="s21" style="padding-left: 21pt;text-indent: -13pt;text-align: justify;"><a name="bookmark9">&zwnj;</a>Graph Inductive Bias</p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark75" class="a">Unlike Euclidean data, such as texts and images, graph data is non-Euclidean data, which has intricate structures and lacks a fixed order and dimensionality, posing difficulties in directly applying standard transformers on graph data </a>[16]. To address this issue, graph transformers incorporate graph inductive bias</p><p class="s22" style="padding-top: 5pt;padding-left: 15pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><span class="h2">p</span><span class="s24">i</span><span class="s25"> </span>= [<span class="s21">I</span>(<span class="s21">d</span>(<span class="s21">i, j</span>) = 1)<span class="s21">, I</span>(<span class="s21">d</span>(<span class="s21">i, j</span>) = 2)<span class="s21">, ..., I</span>(<span class="s21">d</span>(<span class="s21">i, j</span>) = <span class="s21">max</span>)]<span class="s21">.</span></p><p style="padding-left: 247pt;text-indent: 0pt;line-height: 11pt;text-align: left;">(5)</p><p class="s21" style="padding-left: 7pt;text-indent: 0pt;line-height: 12pt;text-align: justify;"><span class="p">In this equation, </span>d<span class="s22">(</span>i, j<span class="s22">) </span><span class="p">represents the shortest path distance between node </span>v<span class="s24">i</span><span class="s25"> </span><span class="p">and node </span>v<span class="s24">j</span><span class="s25"> </span><span class="p">and </span>I <span class="p">is an indicator function that returns 1 if its argument is true and 0 otherwise. The maximum hop distance is denoted by </span>max<a href="#bookmark101" class="a">. This encoding technique was utilized by Velickovic et al. </a><a href="#bookmark102" class="a">[42] to enhance their Graph Attention Networks (GATs) with relative position-aware self- attention. Another way to incorporate local node positional encodings in a graph is by using learnable embeddings that capture the relationship between two nodes </a><a href="#bookmark103" class="a">[43], </a><span class="p">[44]. This approach is particularly useful when a node has multiple neighbors with different edge types or labels. In this case, the local node positional encoding can be learned based on these edge features:</span></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;">to encode the structural information of graphs and achieve effective generalization of transformers across new tasks and</p><p class="s22" style="padding-top: 5pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><span class="h2">p</span><span class="s24">i</span><span class="s25"> </span>= [<span class="s21">f </span>(<span class="h2">e</span></p><p style="padding-top: 2pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s54" style="text-indent: 0pt;text-align: left;">ij<span class="s55">1</span></p><p class="s22" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">)<span class="s21">, f </span>(<span class="h2">e</span><span class="s24">ij</span><span class="s56">2</span></p><p class="s22" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">)<span class="s21">, ..., f </span>(<span class="h2">e</span><span class="s24">ij</span><span class="s48">l</span></p><p class="s22" style="padding-top: 7pt;text-indent: 0pt;text-align: left;">)]<span class="s21">,     </span><span class="p">(6)</span></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark18" class="a" name="bookmark10">domains. In this section, we explore the design perspectives of graph transformers through the lens of graph inductive bias. We classify graph inductive bias into four categories: node positional bias, edge structural bias, message-passing bias, and attention bias (see Figure. </a><a href="#bookmark18">3).</a></p><ol id="l5"><li data-list-text="1)"><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Node Positional Bias: <a href="#bookmark97" class="a">Node positional bias is a crucial inductive bias for graph transformers because it provides information about the relative or absolute positions of nodes in a graph </a><span class="p">[38]. Formally, given a graph </span>G <span class="s22">= (</span>V, E<span class="s22">) </span><span class="p">with </span>N</p><p class="s21" style="padding-left: 7pt;text-indent: 0pt;line-height: 12pt;text-align: justify;"><span class="p">nodes and </span>M <span class="p">edges, each node </span>v<span class="s24">i</span><span class="s25"> </span><span class="s30">∈ </span>V <span class="p">has a feature vector</span></p><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">n</p><p style="text-indent: 0pt;text-align: left;"/><p class="s25" style="padding-left: 7pt;text-indent: 0pt;line-height: 12pt;text-align: justify;"><span class="h2">x</span><span class="s24">i</span> <span class="s30">∈ </span><span class="s26">R</span><span class="s27">d</span> <span class="p">. A graph transformer aims to learn a new feature</span></p><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;">where <b>e</b><span class="s24">ij</span><span class="s25"> </span>is edge feature between node <i>v</i><span class="s24">i</span><span class="s25"> </span>and node <i>v</i><span class="s24">j</span>, <i>f </i>is a learnable function that maps edge features to embeddings and <i>l </i>is the number of neighbors considered.</p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark96" class="a">To enhance the implementation of local node positional encodings in a broader context, a viable strategy is leveraging graph kernels or similarity functions to evaluate the structural similarity between two nodes within the graph </a><a href="#bookmark104" class="a">[37], </a>[45]. For instance, when a node exhibits three neighbors with unique subgraph patterns or motifs in their vicinity, its local node positional encoding can be computed as a vector of kernel values between the node and its neighboring nodes:</p><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">k</p><p style="text-indent: 0pt;text-align: left;"/><p class="s25" style="padding-left: 7pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span class="p">vector </span><span class="h2">h</span><span class="s24">i</span> <span class="s30">∈ </span><span class="s26">R</span><span class="s27">d</span> <span class="p">for each node by applying a series of self-</span></p><p class="s22" style="padding-top: 7pt;padding-left: 7pt;text-indent: 0pt;line-height: 2pt;text-align: left;"><span class="h2">p </span>= [<span class="s21">K</span>(<span class="s21">G , G </span>)<span class="s21">, K</span>(<span class="s21">G , G </span>)<span class="s21">, ..., K</span>(<span class="s21">G , G </span>)]<span class="s21">.  </span><span class="p">(7)</span></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 11pt;text-align: left;">attention layers. A self-attention layer can be defined as:</p><p class="s57" style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">i     <span class="s35">i  j</span><span class="s58">1</span></p><p class="s54" style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">i  j<span class="s55">2</span></p><p class="s35" style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">i  j<span class="s59">l</span></p><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">n</p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">i</p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">ij</p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">j</p><p style="text-indent: 0pt;text-align: left;"/><h2 style="padding-top: 3pt;padding-bottom: 2pt;padding-left: 88pt;text-indent: 0pt;text-align: left;">h <span class="s22">= </span><span class="s60">Σ </span><i>a W </i>x <span class="s22">+ </span><i>b,        </i><span class="s61">(4)</span></h2><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">j<span class="s36">=1</span></p><p style="padding-left: 111pt;text-indent: 0pt;line-height: 7pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">In this equation, <i>G</i><span class="s24">i</span><span class="s25"> </span>refers to the subgraph formed by node <i>v</i><span class="s24">i</span></p><p style="text-indent: 0pt;text-align: left;">and its neighbors. The function <i>K </i>is a graph kernel function that measures the similarity between two subgraphs. Mialon et</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 7pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">where <i>a</i><span class="s24">ij</span><span class="s25"> </span>is the attention score between nodes <i>v</i><span class="s24">i</span><span class="s25"> </span>and <i>v</i><span class="s24">j</span>,</p><p style="padding-left: 7pt;text-indent: 0pt;text-align: justify;">measureing the relevance or similarity of their features. <i>W </i>and <i>b </i><a href="#bookmark98" class="a">are learnable parameters. However this self-attention mechanism does not consider the structural and positional information of nodes, which is crucial for capturing graph semantics and inductive biases </a>[39]. Node positional encoding is a way to address this challenge by providing additional positional features to nodes, reflecting their relative or absolute positions in the graph.</p><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Local Node Positional Encodings. <span class="p">Building on the success of relative positional encodings in NLP, graph transformers leverage a similar concept for local node positional encodings.</span></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark105" class="a">al. </a>[46] utilized this approach in their GraphiT model, which incorporates positive definite kernels on graphs as relative positional encodings for graph transformers.</p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Local node positional encodings enhance the self-attention mechanism of graph transformers by integrating structural and topological information of nodes. This encoding approach offers the advantage of preserving the sparsity and locality of graph structure, resulting in enhanced efficiency and inter- pretability. However, a limitation of this method is its restricted ability to capture long-range dependencies or global properties of graphs, which are essential for tasks such as graph matching or alignment.</p><p class="s21" style="padding-top: 4pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;">Global Node Positional Encodings. <a href="#bookmark106" class="a">The concept of global node positional encodings is inspired by the use of absolute positional encodings in NLP </a><a href="#bookmark107" class="a">[47]. In NLP, as mentioned previously, every token receives a feature vector indicating its position within a sequence. Extending this idea to graph transformers, each node can be assigned a feature vector representing its position within the embedding space of the graph </a><span class="p">[48]. The objective of this encoding technique is to encapsulate the overall geometry and spectrum of graphs, thereby unveiling its intrinsic properties and characteristics.</span></p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark98" class="a">One method for obtaining global node positional encodings is to leverage eigenvectors or eigenvalues of a matrix repre- sentation, such as adjacency matrix or Laplacian matrix </a>[39]. For instance, if a node’s coordinates lie within the first <i>k </i>eigenvectors of graph Laplacian, its global node positional encoding can be represented by the coordinate vector:</p><p class="s24" style="padding-top: 3pt;padding-left: 7pt;text-indent: 79pt;text-align: justify;"><span class="h2">p</span>i<span class="s25"> </span><span class="s22">= [</span><span class="s21">u</span>i<span class="s23">1</span><span class="s21">, u</span>i<span class="s23">2</span><span class="s21">, ..., u</span>ik<span class="s22">]</span><span class="s21">,        </span><span class="p">(8)</span></p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;">where <i>u</i><span class="s24">ij</span><span class="s25"> </span>is <i>j</i>-th component of <i>i</i><a href="#bookmark108" class="a">-th eigenvector of the graph Laplacian matrix. One alternative approach to incorporating global node positional encodings is by utilizing diffusion or random walk techniques, such as personalized PageRank or heat kernel </a><a href="#bookmark109" class="a">[49]. For example, if a node possesses a probability distribution over all other nodes in the graph, following a random walk </a>[50], its global node positional encoding can be represented by this probability vector:</p><p class="s21" style="padding-left: 7pt;text-indent: 78pt;line-height: 17pt;text-align: justify;"><b>p</b><span class="s24">i</span><span class="s25"> </span><span class="s22">= [</span>π<span class="s24">i</span><span class="s23">1</span>, π<span class="s24">i</span><span class="s23">2</span>, ..., π<span class="s24">iN</span><span class="s25"> </span><span class="s22">]</span>,       <span class="p">(9) where </span>π<span class="s24">ij</span><span class="s25"> </span><span class="p">is the probability of reaching node </span>v<span class="s24">j</span><span class="s25"> </span><span class="p">from node </span>v<span class="s24">i</span></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: justify;">after performing a random walk on the graph.</p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark110" class="a">A more prevalent approach to implementing global node positional encodings is to utilize graph embedding or dimen- sionality reduction techniques that map nodes to a lower- dimensional space while maintaining a sense of similarity or distance </a>[51]. For instance, if a node possesses coordinates in a space derived from applying multi-dimensional scaling or graph neural networks to the graph, its global node positional encoding can be represented by that coordinate vector:</p><p class="s24" style="padding-top: 4pt;padding-left: 7pt;text-indent: 81pt;text-align: justify;"><span class="h2">p</span>i<span class="s25"> </span><span class="s22">= [</span><span class="s21">y</span>i<span class="s23">1</span><span class="s21">, y</span>i<span class="s23">2</span><span class="s21">, ..., y</span>ik<span class="s22">]</span><span class="s21">,        </span><span class="p">(10)</span></p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;">where <i>y</i><span class="s24">ij</span><span class="s25"> </span>is <i>j</i>-th component of <i>i</i>-th node embedding in a <i>k</i>- dimensional space, which can be obtained by minimizing the objective function that preserves graph structure:</p><p class="s35" style="text-indent: 0pt;line-height: 22pt;text-align: left;"><span class="s62">Σ </span><span class="s54">ij </span><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="3" height="13" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAANCAYAAABsItTPAAAABmJLR0QA/wD/AP+gvaeTAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAIklEQVQImWNgYGCYxQABm5gYGBi4oRxuJgYkMGCc31D2bwDvhwNxAEA8LAAAAABJRU5ErkJgggAA"/></td></tr></table></span><span class="s63"> </span>i <span class="s64">− </span>j</p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="padding-top: 3pt;padding-left: 47pt;text-indent: 0pt;text-align: center;">N</p><p style="text-indent: 0pt;text-align: left;"><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="3" height="13" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAANCAYAAABsItTPAAAABmJLR0QA/wD/AP+gvaeTAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAIklEQVQImWNgYGCYxQABm5gYGBi4oRxuJgYkMGCc31D2bwDvhwNxAEA8LAAAAABJRU5ErkJgggAA"/></td></tr></table></span></p><p style="text-indent: 0pt;text-align: left;"><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="3" height="13" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAANCAYAAABsItTPAAAABmJLR0QA/wD/AP+gvaeTAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAIklEQVQImWNgYGCYxQABm5gYGBi4oRxuJgYkMGCc31D2bwDvhwNxAEA8LAAAAABJRU5ErkJgggAA"/></td></tr></table></span></p><p class="s22" style="padding-left: 83pt;text-indent: 0pt;line-height: 11pt;text-align: left;">min   <span class="h2">w  y  y  </span><span class="s29">2</span><span class="s21">.        </span><span class="p">(11)</span></p><p class="s65" style="text-indent: 0pt;line-height: 5pt;text-align: center;">Y</p><p class="s35" style="padding-left: 47pt;text-indent: 0pt;line-height: 8pt;text-align: center;">i,j<span class="s36">=1</span></p><p style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 93%;text-align: justify;">Here, <b>w</b><span class="s24">ij</span><span class="s25"> </span>is a weight matrix that reflects the similarity or distance between node <i>v</i><span class="s24">i</span><span class="s25"> </span>and node <i>v</i><span class="s24">j</span>, <i>Y </i>is the matrix of node embeddings.</p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">The primary aim of global node positional encodings is to improve the representation of node attributes in graph trans- formers by incorporating geometric and spectral information from graphs. This encoding method offers the advantage of capturing long-range dependencies and overall graph charac- teristics, benefiting tasks like graph matching and alignment. However, a drawback of this encoding approach is that it could undermine the sparsity and locality of graph structures, potentially impacting efficiency and interpretability.</p></li><li data-list-text="2)"><p class="s21" style="padding-top: 4pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a name="bookmark11">&zwnj;</a>Edge Structural Bias: <a href="#bookmark111" class="a">In the realm of graph trans- formers, edge structural bias is crucial for extracting and understanding complex information within graph structure </a><a href="#bookmark114" class="a">[52]. Edge structural bias is versatile and can represent various aspects of graph structure, including node distances, edge types, edge directions and local sub-structures. Empirical evidence has shown that edge structural encodings can improve the effectiveness of graph transformers </a><a href="#bookmark114">[53]–[55].</a></p><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Local Edge Structural Encodings. <a href="#bookmark111" class="a">Local edge structural encodings capture the local structure of a graph by encoding relative position or distance between two nodes </a><a href="#bookmark115" class="a">[52]. These en- codings borrow ideas from relative positional encodings used in NLP and CV, where they are used for modeling sequential or spatial order of tokens or pixels </a><span class="p">[56]. However, in the context of graphs, the concept of relative position or distance between nodes becomes ambiguous due to the presence of multiple connecting paths with varying lengths or weights. Consequently, the scientific community has proposed various methods to define and encode this information specifically for graph structures.</span></p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark105" class="a">GraphiT </a>[46] introduces local edge structural encodings to graph transformers. It promotes the use of positive definite kernels on graphs to measure node similarity considering the shortest path distance between them. The kernel function is defined as:</p><p class="s21" style="padding-left: 78pt;text-indent: 0pt;line-height: 14pt;text-align: left;">k<span class="s22">(</span>u, v<span class="s22">) = exp(</span><span class="s30">−</span>αd<span class="s22">(</span>u, v<span class="s22">))</span>,      <span class="p">(12)</span></p><p class="s21" style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;"><span class="p">where </span>u <span class="p">and </span>v <span class="p">are two nodes in a graph, </span>d<span class="s22">(</span>u, v<span class="s22">) </span><span class="p">is their shortest path distance and </span>α <span class="p">is a hyperparameter that controls decay rate. The kernel function is then used to modify the self-attention score between two nodes as:</span></p><h2 style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;line-height: 10pt;text-align: center;">QK<span class="s27">T</span></h2><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">k</p><p style="text-indent: 0pt;text-align: left;"/><p class="s22" style="padding-left: 20pt;text-indent: 0pt;line-height: 69%;text-align: left;"><span class="p">Attention</span>(<span class="h2">Q</span><span class="s21">, </span><span class="h2">K</span><span class="s21">, </span><span class="h2">V</span>) = <span class="p">softmax</span>( <span class="s30">√</span><span class="s51">d </span>+ <span class="s21">k</span>(<span class="h2">Q</span><span class="s21">, </span><span class="h2">K</span>))<span class="h2">V</span><span class="s21">,  </span><span class="p">(13)</span></p><p class="s21" style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;"><span class="p">where </span>k<span class="s22">(</span><b>Q</b>, <b>K</b><span class="s22">) </span><a href="#bookmark116" class="a">is the matrix of kernel values computed for each pair of nodes. EdgeBERT </a><span class="p">[57] proposes using edge features as additional input tokens for graph transformers. These edge features are obtained by applying a learnable function to the source and target node features of each edge. The resulting edge features are then concatenated with node features and fed into a standard transformer encoder.</span></p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark117" class="a">More recently, the Edge-augmented Graph Transformer (EGT) </a>[58] introduces residual edge channels as a mecha- nism to directly process and output both structural and node information. The residual edge channels are matrices that store edge information for each pair of nodes. They are initialized with either an adjacency matrix or the shortest path matrix and updated at each transformer layer by applying residual connections. These channels are then used to adjust the self- attention score between two nodes</p><h2 style="padding-top: 2pt;padding-left: 166pt;text-indent: 0pt;line-height: 10pt;text-align: left;">QK<span class="s27">T</span></h2><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">k</p><p style="text-indent: 0pt;text-align: left;"/><h2 style="padding-left: 23pt;text-indent: 0pt;line-height: 69%;text-align: left;"><span class="p">Attention</span><span class="s22">(</span>Q<i>, </i>K<i>, </i>V<i>, </i>R<span class="s24">e</span><span class="s22">) = </span><span class="p">softmax</span><span class="s22">( </span><span class="s30">√</span><i>d </i><span class="s22">+ </span>R<span class="s24">e</span><span class="s22">)</span>V<i>,  </i><span class="p">(14)</span></h2><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;line-height: 13pt;text-align: justify;">where <b>R</b><span class="s24">e</span><span class="s25"> </span>is the residual edge channel matrix.</p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Although local edge structural encodings can capture de- tailed structural information, they may have limitations in cap- turing overall structural information. This can lead to increased computational complexity or memory usage as pairwise infor- mation needs to be computed and stored. Additionally, the</p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;">effectiveness of these encodings may vary depending on the selection and optimization of encoding or kernel function for different graphs and tasks.</p><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Global Edge Structural Encodings. <a href="#bookmark118" class="a">Global edge structural encodings aim to capture the overall structure of a graph. Unlike NLP and CV domains, the exact position of a node in graphs is not well-defined because there is no natural order or coordinate system </a><span class="p">[59]. Several approaches have been suggested to tackle this issue.</span></p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark119" class="a">GPT-GNN </a><a href="#bookmark70" class="a">[60] is an early work that utilizes graph pooling and unpooling operations to encode the hierarchical structure of a graph. It reduces graph size by grouping similar nodes and then restoring the original size by assigning cluster features to individual nodes. This approach results in a multiscale representation of graphs and has demonstrated enhanced per- formance on diverse tasks. Graphormer </a><a href="#bookmark100" class="a">[11] uses spectral graph theory to encode global structure. It uses eigenvectors of normalized Laplacian matrix as global positional encodings for nodes. This method can capture global spectral features (e.g., connectivity, centrality and community structure). Park et al. </a>[41] extended Graphormer by using singular value decomposition (SVD) to encode global structure. They utilized the left singular matrix of the adjacency matrix as global</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;"><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="333" height="163" src="data:image/png;base64,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"/></td></tr></table></span></p><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-left: 7pt;text-indent: 0pt;text-align: justify;"><a name="bookmark19">&zwnj;</a>Fig. 4. Incorporation of message-passing bias in transformer model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark95" class="a">self-attention layers </a>[36]. This technique aims to augment node features with local structural information, making them more compatible with global self-attention. Preprocessing can maintain the integrity of the original transformer architecture without making any modifications and utilize message-passing modules from existing GNNs. It can be mathematically defined as follows:</p><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">v</p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">v</p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">u</p><p style="text-indent: 0pt;text-align: left;"/><p class="s22" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span class="h2">h</span><span class="s29">(</span><span class="s27">t</span><span class="s29">)</span><span class="s50"> </span>= <span class="s21">f </span>(<span class="h2">h</span><span class="s29">(</span><span class="s27">t</span><span class="s28">−</span><span class="s29">1)</span><span class="s21">, </span><span class="s30">{</span><span class="h2">h</span><span class="s29">(</span><span class="s27">t</span><span class="s28">−</span><span class="s29">1)</span><span class="s50"> </span>: <span class="s21">u </span><span class="s30">∈ N </span>(<span class="s21">v</span>)<span class="s30">}</span><span class="s21">, </span><span class="s30">{</span><span class="h2">e</span><span class="s24">uv</span><span class="s25"> </span>: <span class="s21">u </span><span class="s30">∈ N </span>(<span class="s21">v</span>)<span class="s30">}</span>)<span class="s21">,</span></p><p class="s29" style="padding-left: 10pt;text-indent: 0pt;line-height: 8pt;text-align: justify;"><span class="h2">h</span>(<span class="s27">t</span>+1)<span class="s50"> </span><span class="s22">= </span><span class="p">SelfAttention</span><span class="s22">(</span><span class="h2">h</span>(<span class="s27">t</span>)<span class="s21">, </span><span class="s30">{</span><span class="h2">h</span>(<span class="s27">t</span>)<span class="s50"> </span><span class="s22">: </span><span class="s21">u </span><span class="s30">∈ </span><span class="s21">V </span><span class="s30">}</span><span class="s22">)</span><span class="s21">.</span></p><p class="s27" style="padding-left: 7pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><span class="p">positional encodings for nodes. This approach can handle both   </span>v<span class="s25">          </span>v<span class="s25">   </span>u</p><p style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 10pt;text-align: left;">(16)</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">symmetric and asymmetric matrices.</p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a name="bookmark12">&zwnj;</a>Global edge structural encodings excel at capturing coarse- grained structural information at the graph level, benefiting tasks that require global understanding. However, they may struggle with capturing fine-grained node-level information and can lose data or introduce noise during encoding. In addition, their effectiveness may depend on the choice of encoding technique and matrix representations.</p></li><li data-list-text="3)"><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Message-passing Bias: <a href="#bookmark120" class="a">Message-passing bias is a crucial inductive bias for graph transformers to facilitate learning from the local structure of graphs </a><a href="#bookmark98" class="a">[61]. This bias enables graph transformers to exchange information between nodes and edges, thereby capturing dependencies and interactions among graph elements. Moreover, message-passing bias helps graph transformers in overcoming certain limitations of standard transformer architecture, such as the quadratic complexity of self-attention, the absence of positional information and chal- lenges associated with handling sparse graphs </a><span class="p">[39]. Formally, message-passing bias can be expressed as follows:</span></p><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;">Here, <b>h</b><span class="s29">(0)</span><span class="s50"> </span><span class="s22">= </span><b>x</b><span class="s24">v</span>, <b>x</b><span class="s24">v</span><span class="s25"> </span>is input feature of node <i>v</i>, SelfAttention is a function that performs self-attention over all nodes.</p><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">v</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">The limitation of preprocessing is that it applies message- passing only once before self-attention layers. This approach may not fully capture intricate interactions between nodes and edges on various scales. Additionally, the preprocessing step may introduce redundancy and inconsistency between the message-passing module and the self-attention layer as they both serve similar functions of aggregating information from neighboring elements.</p><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Interleaving. <a href="#bookmark75" class="a">Interleaving refers to the technique employed in graph transformer architecture that involves alternating message-passing operations and self-attention layers </a><a href="#bookmark121" class="a">[16], </a><a href="#bookmark122" class="a">[62], </a><span class="p">[63]. The objective of this technique is to achieve a balance between local and global information processing, thereby enabling multi-hop reasoning over graphs. By integrat- ing message-passing modules into core components of graph transformers, interleaving enhances their expressive power and flexibility. It can be mathematically defined as follows:</span></p><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">v</p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">v</p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">u</p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">uv</p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">v</p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">v</p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">u</p><p style="text-indent: 0pt;text-align: left;"/><p class="s29" style="padding-top: 6pt;padding-left: 20pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><span class="h2">h</span>(<span class="s27">t</span>+1)<span class="s50"> </span><span class="s22">= </span><span class="s21">f </span><span class="s22">(</span><span class="h2">h</span>(<span class="s27">t</span>)<span class="s21">, </span><span class="s30">{</span><span class="h2">h</span>(<span class="s27">t</span>)<span class="s50"> </span><span class="s22">: </span><span class="s21">u </span><span class="s30">∈ N </span><span class="s22">(</span><span class="s21">v</span><span class="s22">)</span><span class="s30">}</span><span class="s21">, </span><span class="s30">{</span><span class="h2">e</span></p><p class="s22" style="padding-top: 6pt;padding-left: 10pt;text-indent: 0pt;line-height: 14pt;text-align: left;">: <span class="s21">u </span><span class="s30">∈ N </span>(<span class="s21">v</span>)<span class="s30">}</span>)<span class="s21">.</span></p><p class="s29" style="padding-top: 4pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span class="h2">h</span>(<span class="s27">t</span>+1)<span class="s50"> </span><span class="s22">= </span><span class="s21">θ </span><span class="s22">+ </span><span class="p">SelfAttention</span><span class="s22">(</span><span class="h2">h</span>(<span class="s27">t</span>)<span class="s21">, </span><span class="s30">{</span><span class="h2">h</span>(<span class="s27">t</span>)<span class="s50"> </span><span class="s22">: </span><span class="s21">u </span><span class="s30">∈ </span><span class="s21">V </span><span class="s30">}</span><span class="s22">)</span><span class="s21">,</span></p><p style="text-indent: 0pt;line-height: 11pt;text-align: right;">(15)</p><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">v</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 7pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">Here, <b>h</b><span class="s29">(</span><span class="s27">t</span><span class="s29">)</span><span class="s50"> </span>is the representation of node <i>v </i>at layer <i>t</i>, <i>u </i>is a neighboring node of node <i>v</i>, <b>e</b><span class="s24">uv</span><span class="s25"> </span>is the feature of the edge between nodes <i>v </i>and <i>u</i>, and <i>f </i><a href="#bookmark19" class="a">is the function that aggregates information from neighbors and edges. Incorpo- ration of message-passing bias in graph transformers can be achieved through three primary approaches: preprocessing, interleaving and post-processing (as illustrated in Figure </a>4 ). These approaches vary in how they combine message-passing operations with self-attention layers within transformer archi- tecture.</p><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Preprocessing. <span class="p">Preprocessing involves applying message- passing operations to node features before feeding them to</span></p><p class="s22" style="padding-left: 91pt;text-indent: 0pt;text-align: center;"><span class="s21">θ </span>= <span class="s21">f </span>(<span class="h2">h</span><span class="s29">(</span><span class="s27">t</span><span class="s29">)</span><span class="s21">, </span><span class="s30">{</span><span class="h2">h</span><span class="s29">(</span><span class="s27">t</span><span class="s29">)</span><span class="s50"> </span>: <span class="s21">u </span><span class="s30">∈ N </span>(<span class="s21">v</span>)<span class="s30">}</span><span class="s21">, </span><span class="s30">{</span><span class="h2">e</span><span class="s24">uv</span><span class="s25"> </span>: <span class="s21">u </span><span class="s30">∈ N </span>(<span class="s21">v</span>)<span class="s30">}</span>)<span class="s21">.</span></p><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">v  u</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;line-height: 11pt;text-align: left;">(17)</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;">One drawback of the interleaving technique is its impact on the complexity and computational requirements of graph transformers. This is due to the need for additional parameters and operations compared to pre-processing or post-processing. Furthermore, interleaving can potentially lead to interference and conflict between the message-passing module and the self- attention layer as they each update node representations in distinct manners.</p><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Post-processing. <a href="#bookmark117" class="a">Post-processing refers to the technique of applying message-passing operations to node representations obtained from self-attention layers </a><a href="#bookmark123" class="a">[58], </a><span class="p">[64]. The purpose</span></p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;">of this approach is to refine and adjust node representations based on underlying graph structure, thereby enhancing their interpretability and robustness. By doing so, this method aims to improve the quality and utility of node representations for downstream tasks and applications. It can be mathematically defined as follows:</p><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">v            v   u</p><p style="text-indent: 0pt;text-align: left;"/><p class="s29" style="padding-top: 5pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="h2">h</span>(<span class="s27">t</span>+1)<span class="s50"> </span><span class="s22">= </span><span class="p">SelfAttention</span><span class="s22">(</span><span class="h2">h</span>(<span class="s27">t</span>)<span class="s21">, </span><span class="s30">{</span><span class="h2">h</span>(<span class="s27">t</span>)<span class="s50"> </span><span class="s22">: </span><span class="s21">u </span><span class="s30">∈ </span><span class="s21">V </span><span class="s30">}</span><span class="s22">)</span><span class="s21">,</span></p></li></ol></li><li data-list-text="B."><p class="s21" style="padding-top: 4pt;padding-left: 21pt;text-indent: -13pt;text-align: justify;"><a name="bookmark14">&zwnj;</a>Graph Attention Mechanisms</p></li></ol><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark130" class="a">Graph attention mechanisms play an important role in the construction of graph transformers </a><a href="#bookmark131" class="a">[71]. By dynamically as- signing varying weights to nodes and edges in the graph, these mechanisms enable transformers to prioritize and emphasize the most relevant and important elements for a given task </a>[72]. Specifically, a graph attention mechanism is a function that</p><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">i<span class="s63">          </span>i</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 7pt;text-indent: 0pt;line-height: 7pt;text-align: left;">maps each node <i>v </i><span class="s30">∈ </span><i>V </i>to a vector <b>h </b><span class="s30">∈ </span><span class="s26">R</span><span class="s27">d</span><span class="s17">k </span>:</p><p class="s29" style="padding-left: 16pt;text-indent: 0pt;line-height: 6pt;text-align: left;"><span class="h2">h</span>(<span class="s27">T</span><span class="s25"> </span>+1)<span class="s50"> </span><span class="s22">= </span><span class="s21">f </span><span class="s22">(</span><span class="h2">h</span>(<span class="s27">T</span><span class="s25"> </span>)<span class="s21">, </span><span class="s30">{</span><span class="h2">h</span>(<span class="s27">T</span><span class="s25"> </span>)<span class="s50"> </span><span class="s22">: </span><span class="s21">u </span><span class="s30">∈ N </span><span class="s22">(</span><span class="s21">v</span><span class="s22">)</span><span class="s30">}</span><span class="s21">, </span><span class="s30">{</span><span class="h2">e</span></p><p class="s22" style="padding-left: 10pt;text-indent: 0pt;line-height: 6pt;text-align: left;">: <span class="s21">u </span><span class="s30">∈ N </span>(<span class="s21">v</span>)<span class="s30">}</span>)<span class="s21">.</span></p><p class="s35" style="padding-left: 21pt;text-indent: 0pt;line-height: 8pt;text-align: left;">v       v    u</p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 0pt;text-align: left;">Here,</p><p class="s35" style="padding-left: 7pt;text-indent: 0pt;line-height: 6pt;text-align: left;">uv</p><p style="text-indent: 0pt;line-height: 10pt;text-align: right;">(18)</p><p class="s24" style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><span class="h2">h</span>i<span class="s25"> </span><span class="s22">= </span><span class="s21">f</span>n<span class="s22">(</span><span class="h2">x</span>i<span class="s21">, </span><span class="s30">{</span><span class="h2">x</span>j<span class="s30">}</span>v<span class="s66">j </span><span class="s67">∈N </span><span class="s68">(</span><span class="s69">v</span><span class="s66">i</span><span class="s68">)</span><span class="s21">, </span><span class="s30">{</span><span class="s21">A</span>ij<span class="s30">}</span>v<span class="s66">j </span><span class="s67">∈N </span><span class="s68">(</span><span class="s69">v</span><span class="s66">i</span><span class="s68">)</span><span class="s22">)</span><span class="s21">,   </span><span class="p">(21)</span></p><p class="s21" style="padding-left: 33pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">T <span class="p">is the final layer of the graph transformer.</span></p><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a name="bookmark13">&zwnj;</a>The drawback of post-processing is its limited application of message-passing after self-attention layers, potentially failing to capture intricate semantics and dynamics of graph data. Additionally, post-processing runs the risk of introducing noise and distortion to node representations as it has the potential to overwrite or override information acquired by self-attention layers.</p><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">4) Attention Bias: <a href="#bookmark98" class="a">Attention bias enables graph transform- ers to effectively incorporate graph structure information into the attention mechanism without message-passing or posi- tional encodings </a><a href="#bookmark124" class="a">[39], </a><span class="p">[65]. Attention bias modifies attention scores between nodes based on their relative positions or distances in the graph. It can be categorized as either local or global depending on whether it focuses on the local</span></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 91%;text-align: justify;">where <i>f</i><span class="s24">n</span><span class="s25"> </span>is a nonlinear transformation, <b>x</b><span class="s24">i</span><span class="s25"> </span>is the input feature vector of node <i>v</i><span class="s24">i</span>, <b>x</b><span class="s24">j</span><span class="s25"> </span>is the input feature vector of node <i>v</i><span class="s24">j</span>. The function <i>f</i><span class="s24">n</span><span class="s25"> </span>can be decomposed into two parts: an attention function and an aggregation function.</p><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 9pt;line-height: 91%;text-align: justify;">The attention function computes a scalar weight for each neighbor of node <i>v</i><span class="s24">i</span>, denoted by <i>α</i><span class="s24">ij</span>, which reflects the importance or relevance of node <i>v</i><span class="s24">j</span><span class="s25"> </span>for node <i>v</i><span class="s24">i</span>:</p><p class="s24" style="padding-top: 5pt;padding-left: 7pt;text-indent: 39pt;text-align: left;"><span class="s21">α</span>ij<span class="s25"> </span><span class="s22">= </span><span class="p">softmax</span>i<span class="s22">(</span><span class="p">LeakyReLU</span><span class="s22">(</span><span class="h2">W</span>a<span class="s22">[</span><span class="h2">x</span>i<span class="s30">||</span><span class="h2">x</span>j<span class="s22">]))</span><span class="s21">,  </span><span class="p">(22)</span></p><p class="s30" style="text-indent: 0pt;line-height: 12pt;text-align: left;">∈                 ||</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 5pt;padding-left: 7pt;text-indent: 0pt;line-height: 92%;text-align: justify;">where <b>W</b><span class="s24">a</span><span class="s25">  </span><span class="s26">R</span><span class="s29">1</span><span class="s28">×</span><span class="s29">2</span><span class="s27">d</span><span class="s17">n </span>is a learnable weight matrix,  is the concatenation operation, and softmax<span class="s24">i</span>normalizes weights over all neighbors of node <i>v</i><span class="s24">i</span>. The aggregation function combines the weighted features of the neighbors to obtain the output representation of node <i>v</i><span class="s24">i</span>:</p><p style="padding-left: 7pt;text-indent: 0pt;text-align: justify;">neighborhood or global topology of the graph.</p><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Local Attention Bias. <span class="p">Local attention bias limits the atten- tion to a local neighborhood surrounding each node. This con- cept is analogous to the message-passing mechanism observed</span></p><h2 style="padding-left: 7pt;text-indent: 0pt;line-height: 30pt;text-align: left;">h<span class="s24">i</span><span class="s25"> </span><span class="s22">= </span>W<span class="s24">h</span><span class="s25"> </span><span class="s70">,</span><span class="s46"></span>x<span class="s24">i</span><span class="s25"> </span><span class="s22">+</span></h2><p class="s71" style="padding-top: 7pt;text-indent: 0pt;text-align: left;"><span class="s54">v</span><span class="s39">j </span>∈<span class="s72">Σ</span>N <span class="s73">(</span><span class="s54">v</span><span class="s39">i</span><span class="s73">)</span></p><p class="s24" style="text-indent: 0pt;line-height: 30pt;text-align: left;"><span class="s21">α</span>ij<span class="h2">x</span>j<span class="s70"></span><span class="s46"> </span><span class="s21">,    </span><span class="p">(23)</span></p><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark125" class="a">in GNNs </a>[66]. It can be mathematically defined as follows:</p><p class="s75" style="padding-top: 5pt;padding-left: 83pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span class="s74"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>exp(<span class="s76">g</span>(<span class="s77">x</span><span class="s78">i</span><span class="s76">, </span><span class="s77">x</span><span class="s78">j</span>) <span class="s79">· </span><span class="s76">b</span><span class="s78">ij</span>) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 11pt;text-align: left;">where <b>W</b><span class="s24">h</span><span class="s25"> </span>is another learnable weight matrix. Graph attention</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 11pt;text-align: left;">mechanisms have the potential to extend their application</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 5pt;text-align: left;">beyond nodes to edges. This can be achieved by substitut-</p><p class="s54" style="text-indent: 0pt;line-height: 8pt;text-align: left;">k<span class="s71">∈N </span><span class="s73">(</span>v<span class="s39">i</span><span class="s73">)</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">i</p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">k</p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">ik</p><p style="text-indent: 0pt;text-align: left;"/><p class="s21" style="padding-left: 56pt;text-indent: 0pt;line-height: 11pt;text-align: left;">α<span class="s24">ij</span><span class="s25"> </span><span class="s22">= </span><span class="s37">Σ</span></p><p class="s22" style="padding-top: 3pt;padding-left: 30pt;text-indent: 0pt;line-height: 15pt;text-align: left;">exp(<span class="s21">g</span>(<span class="s31">x </span><span class="s21">, </span><span class="s31">x </span>) <span class="s30">· </span><span class="s21">b</span></p><p class="s80" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">) <span class="s21">,   </span><span class="p">(19)</span></p><p style="padding-top: 6pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">ing nodes with edges and utilizing edge features instead of</p><p style="padding-top: 6pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;">where <i>α</i><span class="s24">ij</span><span class="s25"> </span>is the attention score between node <i>v</i><span class="s24">i</span><span class="s25"> </span>and node <i>v</i><span class="s24">j</span>, <span class="s31">x</span><span class="s24">i</span><span class="s25"> </span>and <span class="s31">x</span><span class="s24">j</span><span class="s25"> </span>are their node features, <i>g </i>is a function that computes the similarity between two nodes, such as dot-product and linear transformation. <i>b</i><span class="s24">ij</span><span class="s25"> </span>is a local attention bias term that modifies attention score based on the distance between node <i>v</i><span class="s24">i</span><span class="s25"> </span>and node <i>v</i><span class="s24">j</span><a href="#bookmark95" class="a">. The local attention bias term can be either a binary mask that only allows attention within a certain hop distance </a><a href="#bookmark101" class="a">[36], </a><a href="#bookmark126" class="a">[42], </a><a href="#bookmark123" class="a">[67] or a decay function that decreases attention score with increasing distance </a><a href="#bookmark127" class="a">[64], </a><a href="#bookmark127">[68].</a></p><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Global Attention Bias. <a href="#bookmark128" class="a">Global attention bias integrates global topology information into the attention mechanism independent of message-passing and positional encodings </a><span class="p">[69]. It can be mathematically defined as follows:</span></p><p class="s76" style="padding-top: 5pt;padding-left: 54pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><span class="s75"> &nbsp;&nbsp; exp(</span>g<span class="s75">(</span><span class="s77">x</span><span class="s78">i</span>, <span class="s77">x</span><span class="s78">j</span><span class="s75">) + </span>c<span class="s75">(</span><span class="s77">A</span>, <span class="s77">D</span>, <span class="s77">L</span>, <span class="s77">P</span><span class="s75">)</span><span class="s78">ij</span><span class="s75">) &nbsp;&nbsp;&nbsp;</span></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark132" class="a" name="bookmark15">node features </a><a href="#bookmark133" class="a">[73]. Furthermore, stacking of graph attention mechanisms in multiple layers allows for the utilization of output representations from one layer as input features for the subsequent layer </a><a href="#bookmark133">[74].</a></p><ol id="l6"><li data-list-text="1)"><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Global Attention Mechanisms: <a href="#bookmark134" class="a">Global attention mecha- nisms can determine how each node calculates its attention weights across all other nodes in the graph </a><span class="p">[75]. Global attention mechanisms can be broadly categorized into two types: quadratic attention mechanisms and linear attention mechanisms.</span></p><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Quadratic Attention Mechanisms. <span class="p">Quadratic attention mech- anisms are derived from the conventional self-attention for- mula. This formula calculates attention weights by applying a softmax function to scale the dot product between the query and key vectors of each node:</span></p><p class="s21" style="padding-left: 26pt;text-indent: 0pt;line-height: 10pt;text-align: left;">α<span class="s24">ij</span><span class="s25"> </span><span class="s22">=</span></p><p class="s21" style="padding-left: 1pt;text-indent: 0pt;line-height: 19pt;text-align: left;"><span class="s33">Σ</span><span class="s34">N  </span><span class="s22">exp(</span>g<span class="s22">(</span><span class="s31">x</span><span class="s24">i</span>, <span class="s31">x</span><span class="s24">k</span><span class="s22">) + </span>c<span class="s22">(</span><span class="s31">A</span>, <span class="s31">D</span>, <span class="s31">L</span>, <span class="s31">P</span><span class="s22">)</span><span class="s24">ik</span><span class="s22">)</span></p><p class="s81" style="text-indent: 0pt;line-height: 11pt;text-align: left;">.  <span class="p">(20)</span></p><p class="s82" style="padding-top: 5pt;padding-left: 55pt;text-indent: 0pt;text-align: center;">q<span class="s83">⊤</span>k<span class="s39">j</span></p><p style="padding-left: 89pt;text-indent: 0pt;line-height: 2pt;text-align: left;"/><p class="s22" style="text-indent: 0pt;line-height: 10pt;text-align: left;">exp( <span class="s71">√</span><span class="s84">i </span>)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s37" style="text-indent: 0pt;line-height: 10pt;text-align: left;">Σ</p><p style="text-indent: 0pt;text-align: left;"/><p class="s22" style="text-indent: 0pt;line-height: 10pt;text-align: left;">exp( <span class="s71">√</span><span class="s84">i  </span>)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s21" style="padding-left: 26pt;text-indent: 0pt;line-height: 9pt;text-align: left;">α<span class="s24">ij</span><span class="s25"> </span><span class="s22">= </span><u>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </u><span class="s85">d</span><span class="s86">k &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class="s87"> </span>.       <span class="p">(24)</span></p><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">k<span class="s36">=1</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;">Here, <i>c </i>is the function that computes the global attention bias term, modifying attention score based on some graph-specific</p><p class="s35" style="padding-left: 7pt;text-indent: 0pt;line-height: 8pt;text-align: left;">N</p><p class="s35" style="padding-left: 7pt;text-indent: 0pt;line-height: 9pt;text-align: left;">n<span class="s36">=1</span></p><p class="s88" style="padding-left: 7pt;text-indent: 0pt;line-height: 10pt;text-align: left;">q<span class="s89">⊤</span>k<span class="s90">n</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="padding-left: 17pt;text-indent: 0pt;line-height: 9pt;text-align: left;">d<span class="s59">k</span></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: justify;">matrices or vectors, such as adjacency matrix <span class="s31">A</span>, degree matrix <span class="s31">D</span>, Laplacian matrix <span class="s31">L</span>, and PageRank vector <span class="s31">P</span><a href="#bookmark105" class="a">. The global attention bias term can be additive or multiplicative to the similarity function </a><a href="#bookmark129" class="a">[46], [70]. Generally, global attention bias can enhance the global structure awareness and expressive power of graph transformers </a><a href="#bookmark129">[70].</a></p><p class="s21" style="padding-left: 7pt;text-indent: 0pt;line-height: 12pt;text-align: justify;"><span class="p">The computational complexity of this method is </span>O<span class="s22">(</span>N <span class="s29">2</span><span class="s22">)</span><span class="p">.</span></p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark101" class="a">One of the pioneering studies that has introduced quadratic attention mechanisms for graph transformers is the work by Velickovic et al. </a><a href="#bookmark135" class="a">[42]. The authors proposed the use of multiple attention heads to capture different types of rela- tions between nodes. Choromanski et al. </a>[76] introduced the</p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;">Graph Kernel Attention Transformer (GKAT), an approach that integrates graph kernels, structural priors, and efficient transformer architectures. GKAT leverages graph kernels as positional encodings to capture the structural characteristics of the graph while employing low-rank decomposition techniques to minimize the memory requirements of quadratic attention:</p><p style="text-indent: 0pt;text-align: left;"/><p class="s67" style="text-indent: 0pt;line-height: 81%;text-align: left;">√<span class="s39">i   </span><span class="s47">⊤ </span><span class="s91">j</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s22" style="padding-top: 4pt;padding-left: 105pt;text-indent: 0pt;line-height: 11pt;text-align: left;">exp( <span class="s92">q</span><span class="s93">⊤</span><span class="s94">k</span><span class="s87">j </span>+ <span class="s31">p p </span>)</p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;">randomly sampled from a Gaussian distribution and a uniform distribution, respectively.</p><p style="padding-left: 7pt;text-indent: 9pt;line-height: 12pt;text-align: justify;"><a href="#bookmark141" class="a">Locality-sensitive linear attention mechanisms are based on the concept of employing hashing functions to partition the query and key vectors into discrete buckets that facilitate local computation of their inner products </a><a href="#bookmark142" class="a">[81]–[83]. Kitaev et al. </a>[84] introduced the Reformer model which uses locality- sensitive hashing (LSH) as a hashing function to approximate</p><p class="s21" style="padding-left: 62pt;text-indent: 0pt;line-height: 2pt;text-align: left;">α  <span class="s22">= </span><u>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </u><span class="s85">d</span><span class="s86">k &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class="s95">i &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>,</p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">n<span class="s36">=1</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s67" style="text-indent: 0pt;line-height: 7pt;text-align: left;">√<span class="s39">i</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">d<span class="s59">k</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">i</p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">n</p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="padding-left: 68pt;text-indent: 0pt;line-height: 27%;text-align: left;">ij  <span class="s96">Σ</span><span class="s97">N</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">N</p><p style="padding-left: 92pt;text-indent: 0pt;line-height: 7pt;text-align: left;"/><p class="s22" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">exp( <span class="s98">q</span><span class="s99">⊤</span><span class="s100">k</span><span class="s86">n</span><span class="s87"> </span>+ <span class="s31">p</span><span class="s28">⊤</span><span class="s31">p </span>)</p><p style="padding-left: 68pt;text-indent: 0pt;line-height: 10pt;text-align: left;">self-attention:</p><p style="text-indent: 0pt;line-height: 11pt;text-align: left;">(25)</p><p class="s82" style="text-indent: 0pt;line-height: 7pt;text-align: left;">q k<span class="s39">j</span></p><p style="padding-left: 39pt;text-indent: 0pt;text-align: left;"><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 3pt;">	</span></p><p style="padding-left: 203pt;text-indent: 0pt;line-height: 2pt;text-align: left;"/><p class="s37" style="text-indent: 0pt;line-height: 10pt;text-align: left;">Σ</p><p style="text-indent: 0pt;text-align: left;"/><p class="s22" style="padding-left: 67pt;text-indent: 0pt;text-align: left;"><span class="s31">z</span><span class="s24">i</span><span class="s25"> </span>=  <span class="s21">α</span><span class="s24">ij</span>(<span class="s31">UV</span><span class="s28">⊤</span>)<span class="s24">ij</span></p><p class="s35" style="padding-top: 1pt;padding-left: 109pt;text-indent: 0pt;text-align: center;">j<span class="s36">=1</span></p><p class="s21" style="text-indent: 0pt;line-height: 13pt;text-align: right;">α<span class="s24">ij</span><span class="s25"> </span><span class="s22">=</span></p><p class="s35" style="padding-left: 47pt;text-indent: 0pt;line-height: 50%;text-align: center;">d<span class="s59">k</span><span class="s101">   </span><span class="s102">,</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s37" style="text-indent: 0pt;line-height: 10pt;text-align: left;">Σ</p><p style="text-indent: 0pt;text-align: left;"/><p class="s24" style="text-indent: 0pt;line-height: 10pt;text-align: left;">n<span class="s103">∈</span>B<span class="s25"> </span><span class="s22">exp( </span><span class="s71">√</span><span class="s104">i </span><span class="s84"> </span><span class="s22">)</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s105" style="text-indent: 0pt;line-height: 5pt;text-align: right;">⊤</p><p class="s22" style="text-indent: 0pt;line-height: 10pt;text-align: left;">exp( <span class="s71">√</span><span class="s84">i</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s22" style="text-indent: 0pt;line-height: 10pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s106" style="padding-left: 44pt;text-indent: 0pt;line-height: 9pt;text-align: center;"><span class="s88">q</span>⊤<span class="s88">k</span><span class="s90">n</span></p><p class="s39" style="padding-left: 28pt;text-indent: 0pt;line-height: 9pt;text-align: left;">i    <span class="s35">d</span><span class="s59">k</span></p><p style="padding-top: 7pt;padding-left: 52pt;text-indent: 0pt;text-align: left;">(27)</p><p style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">where <b>z</b><span class="s24">i</span><span class="s25"> </span>is the output vector of node <i>v</i><span class="s24">i</span>, <span class="s31">p</span><span class="s24">i</span><span class="s25"> </span>is the positional encoding vector of node <i>v</i><span class="s24">i</span>, which is computed by applying a graph kernel function to node features. <span class="s31">U </span>and <span class="s31">V </span><a href="#bookmark95" class="a">are low- rank matrices that approximate value matrix in the GKAT model. GKAT also introduced a novel kernel-based masking scheme to control the sparsity of the attention matrix. Yun et al. </a>[36] introduced Graph Transformer Networks (GTN), an approach that uses multiple layers of quadratic attention to acquire hierarchical representations of graphs. GTN further enhances attention computation by incorporating edge features through the use of bilinear transformations instead of concate- nation. This innovative technique demonstrates the potential for improved graph representation learning in the field of graph analysis.</p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Quadratic attention mechanisms possess a significant advan- tage in capturing extensive dependencies and global informa- tion within graphs, thereby offering potential benefits for vari- ous graph learning tasks. Nonetheless, these mechanisms have some limitations, including their computationally expensive nature, high memory consumption, and susceptibility to noise and outliers. Additionally, quadratic attention mechanisms may not effectively preserve and leverage local information within graphs which holds significance in certain domains and tasks.</p><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Linear Attention Mechanisms. <span class="p">Linear attention mechanisms employ different approximations and hashing techniques to decrease the computational complexity of self-attention from </span>O<span class="s22">(</span>N <span class="s29">2</span><span class="s22">) </span><span class="p">to </span>O<span class="s22">(</span>N <a href="#bookmark136" style=" color: black; font-family:Garamond, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">) </a><span class="p">[77]. These mechanisms can be categorized into two subtypes: kernel-based linear attention mechanisms and locality-sensitive linear attention mechanisms.</span></p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark137" class="a">Kernel-based linear attention mechanisms leverage the con- cept of employing kernel functions to map query and key vectors into a feature space that allows efficient computation of their inner products </a><a href="#bookmark138" class="a">[78], </a><a href="#bookmark139" class="a">[79]. In the work by Katharopoulos et al. </a>[80], Linear Transformer Networks (LTN) were introduced. LTN utilizes random Fourier features as kernel functions to approximate self-attention:</p><p class="s22" style="padding-left: 81pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><span class="s21">B</span><span class="s24">i</span><span class="s25"> </span>= <span class="s30">{</span><span class="s21">j</span><span class="s30">|</span><span class="s21">h</span>(<span class="s31">q</span><span class="s24">i</span>) = <span class="s21">h</span>(<span class="s31">k</span><span class="s24">j</span>)<span class="s30">}</span><span class="s21">,</span></p><p style="padding-top: 5pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;">where <i>h </i>is an LSH function that maps the input vector into a bucket index and <i>B</i><span class="s24">i</span><span class="s25"> </span>is a set of nodes that share the same bucket index with node <i>v</i><span class="s24">i</span>. RGN also incorporated edge features into the attention computation using a shared query- key projection matrix.</p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a name="bookmark16">&zwnj;</a>Linear attention mechanisms offer a significant advantage in terms of reducing computational cost and memory us- age in self-attention, thereby enabling graph transformers to efficiently process large-scale graphs. However, they do come with certain limitations, including approximation error, hashing collision and loss of global information. Additionally, linear attention mechanisms may not be capable of capturing intricate or nonlinear relationships between nodes.</p></li><li data-list-text="2)"><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Local Attention Mechanisms: <a href="#bookmark143" class="a">Local attention mecha- nisms determine how each node computes its attention weights over a subset of nodes in the graph </a><span class="p">[85]. Local attention mech- anisms can be broadly categorized into two types: message- passing attention mechanisms and spectral attention mecha- nisms.</span></p></li></ol><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Message-passing Attention Mechanisms. <a href="#bookmark144" class="a">Message-passing attention mechanisms are built on the foundational framework of message-passing neural networks (MPNNs) </a><a href="#bookmark98" class="a">[86], which iteratively aggregate messages from neighboring nodes to compute node representations. To enhance MPNNs, message- passing attention mechanisms employ self-attention to com- pute messages or aggregation weights </a><a href="#bookmark120" class="a">[39], </a><a href="#bookmark145" class="a">[61], </a><a href="#bookmark146" class="a">[87], </a><span class="p">[88]. The computational complexity of this method is </span>O<span class="s22">(</span>E<span class="s22">)</span><span class="p">, where </span>E <span class="p">represents the number of edges in the graph.</span></p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark147" class="a">One of the pioneering works in introducing message- passing attention mechanisms for graph transformers is Graph- SAGE </a>[89]. GraphSAGE proposes the use of mean pooling as an aggregation function and self-attention as a combination function. Additionally, GraphSAGE incorporates node sam- pling techniques to efficiently handle large-scale graphs. The function can be mathematically defined as:</p><p class="s39" style="text-indent: 0pt;line-height: 5pt;text-align: left;">i</p><p style="text-indent: 0pt;text-align: left;"/><p class="s39" style="text-indent: 0pt;line-height: 5pt;text-align: left;">j</p><p style="text-indent: 0pt;text-align: left;"/><p class="s36" style="padding-top: 4pt;padding-left: 87pt;text-indent: 0pt;line-height: 5pt;text-align: left;"><span class="s35">ϕ</span>(<span class="s107">q </span>)<span class="s108">⊤</span><span class="s35">ϕ</span>(<span class="s107">k </span>)</p><p class="s29" style="padding-top: 2pt;padding-left: 87pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><span class="s31">h</span>(<span class="s27">l</span>+1)<span class="s50"> </span><span class="s22">= </span><span class="p">ReLU</span><span class="s22">(</span><span class="s31">W</span>(<span class="s27">l</span>)<span class="s22">[</span><span class="s31">h</span>(<span class="s27">l</span>)<span class="s50"> </span><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="3" height="13" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAANCAYAAABsItTPAAAABmJLR0QA/wD/AP+gvaeTAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAIklEQVQImWNgYGCYxQABm5gYGBi4oRxuJgYkMGCc31D2bwDvhwNxAEA8LAAAAABJRU5ErkJgggAA"/></td></tr></table></span><span class="p"> MEAN</span><span class="s22">(</span><span class="s30">{</span><span class="s31">h</span>(<span class="s27">l</span>)<span class="s21">, </span><span class="s30">∀</span><span class="s21">v</span><span class="s24">j</span><span class="s25"> </span><span class="s30">∈ N </span><span class="s22">(</span><span class="s21">v</span><span class="s24">i</span><span class="s22">)</span><span class="s30">}</span><span class="s22">)])</span></p><p class="s22" style="text-indent: 0pt;line-height: 6pt;text-align: right;">exp(</p><p class="s47" style="padding-left: 13pt;text-indent: 0pt;line-height: 6pt;text-align: left;">√<span class="s109"> &nbsp;&nbsp; </span><span class="s63"> </span><span class="s110">)</span></p><p class="s35" style="padding-left: 67pt;text-indent: 0pt;line-height: 6pt;text-align: left;">i             i        j</p><p style="padding-left: 87pt;text-indent: 0pt;line-height: 1pt;text-align: left;"/><p class="s37" style="text-indent: 0pt;line-height: 10pt;text-align: left;">Σ</p><p style="text-indent: 0pt;text-align: left;"/><p class="s21" style="padding-left: 25pt;text-indent: 0pt;line-height: 5pt;text-align: left;">α<span class="s24">ij</span><span class="s25"> </span><span class="s22">= </span><u>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </u><span class="s85">d</span><span class="s86">k &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class="s87"> </span>,                     <span class="s54">L</span></p><p class="s22" style="text-indent: 0pt;line-height: 86%;text-align: left;">exp(  <span class="s71">√</span><span class="s91">d  </span>)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">i</p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="padding-top: 2pt;padding-left: 63pt;text-indent: 0pt;line-height: 9pt;text-align: left;">N</p><p class="s35" style="padding-left: 63pt;text-indent: 0pt;line-height: 6pt;text-align: left;">n<span class="s36">=1</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s37" style="padding-left: 51pt;text-indent: 0pt;line-height: 70%;text-align: left;">r <span class="s111">2</span></p><p class="s113" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span class="s112">ϕ</span>(<span class="s88">q</span><span class="s90">i</span>)<span class="s89">⊤</span><span class="s112">ϕ</span>(<span class="s88">k</span><span class="s90">n</span>)</p><p style="padding-left: 42pt;text-indent: 0pt;line-height: 1pt;text-align: left;"/><p class="s39" style="padding-left: 68pt;text-indent: 0pt;text-align: center;">k</p><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s47" style="padding-left: 18pt;text-indent: 0pt;line-height: 3pt;text-align: left;">⊤          ⊤</p><p style="padding-top: 9pt;text-indent: 0pt;text-align: right;">(26)</p><p class="s47" style="padding-top: 8pt;padding-left: 80pt;text-indent: 0pt;line-height: 3pt;text-align: center;">⊤</p><p class="s31" style="padding-top: 6pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">z<span class="s24">i</span><span class="s25"> </span><span class="s22">=</span></p><p class="s114" style="text-indent: 0pt;line-height: 31pt;text-align: left;">Σ<span class="s35">l</span><span class="s36">=0</span></p><p class="s36" style="padding-top: 4pt;text-indent: 0pt;text-align: left;"><span class="s102">α</span><span class="s53">il</span><span class="s115">h</span>(<span class="s35">l</span>)<span class="s102">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 51pt;text-indent: 0pt;line-height: 5pt;text-align: left;">(28)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 63pt;text-indent: 0pt;line-height: 1pt;text-align: left;"/><p class="s22" style="padding-left: 18pt;text-indent: 0pt;line-height: 22%;text-align: left;"><span class="s21">ϕ</span>(<span class="s31">x</span>) =   [cos(<span class="s21">ω</span><span class="s68">1 </span><span class="s31">x </span>+ <span class="s21">b</span><span class="s23">1</span>)<span class="s21">, ..., </span>cos(<span class="s21">ω</span><span class="s69">m</span><span class="s31">x </span>+ <span class="s21">b</span><span class="s24">m</span>)] <span class="s21">.</span></p><p class="s21" style="padding-left: 63pt;text-indent: 0pt;line-height: 8pt;text-align: left;">m</p><p style="padding-top: 3pt;padding-left: 18pt;text-indent: 0pt;line-height: 13pt;text-align: left;">Here, <span class="s31">h</span><span class="s29">(</span><span class="s27">l</span><span class="s29">)</span><span class="s50"> </span>is hidden vector of node <i>v</i><span class="s24">i</span><span class="s25"> </span>at layer <i>l</i>, <span class="s31">W</span><span class="s29">(</span><span class="s27">l</span><span class="s29">)</span><span class="s50"> </span>is a</p><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">i</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 7pt;text-indent: 0pt;line-height: 94%;text-align: left;">Here, <i>ϕ </i>is a random Fourier feature function that maps input vector into a <i>m</i>-dimensional feature space. <i>ω</i><span class="s24">i</span><span class="s25"> </span>and <i>b</i><span class="s24">i</span><span class="s25"> </span>are</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 92%;text-align: left;">linear transformation matrix, <span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="3" height="13" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAANCAYAAABsItTPAAAABmJLR0QA/wD/AP+gvaeTAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAIklEQVQImWNgYGCYxQABm5gYGBi4oRxuJgYkMGCc31D2bwDvhwNxAEA8LAAAAABJRU5ErkJgggAA"/></td></tr></table></span> is the concatenation operator, MEAN indicates a mean pooling function. <i>α</i><span class="s24">il</span><span class="s25"> </span>means an</p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;">attention weight between node <i>v</i><span class="s24">i</span><span class="s25"> </span>and layer <i>l</i><a href="#bookmark148" class="a">. More recently, Javaloy et al. </a>[90] proposed L-CAT which uses self-attention as both an aggregation function and a combination function. L-CAT also incorporates edge features into the attention com- putation by using bilinear transformations. Message-passing attention mechanisms, while adept at preserving and lever- aging local graph information, are constrained by scalability issues, such as limited expressiveness and graph connectivity dependence. Their capacity to capture long-range dependen- cies and global graph information is also questionable.</p><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Spectral Attention Mechanisms. <a href="#bookmark149" class="a">Spectral attention mech- anisms are founded on the concept of transforming node features into a spectral domain, where the graph structure is encoded by eigenvalues and eigenvectors of the graph Laplacian matrix </a><a href="#bookmark150" class="a">[91]. Spectral attention mechanisms employ self-attention to calculate spectral coefficients and spectral filters </a><span class="p">[92]. The computational complexity of this approach is </span>O<span class="s22">(</span>N <span class="s22">)</span><span class="p">.</span></p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 11pt;text-align: justify;"><a href="#bookmark81" class="a">Wang et al. </a>[22] proposed graph isomorphism networks</p><p style="padding-left: 7pt;text-indent: 0pt;text-align: justify;">(GIN) as a spectral attention mechanism for graph transform- ers, employing an expressive aggregation function of sum pooling and self-attention. GIN also integrates a unique graph readout scheme via set functions to encapsulate global graph characteristics. The equations of GIN are as follows:</p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark152" class="a">representations from data structured in graphs. Inspired by transformer models, which effectively capture long-range de- pendencies in sequential data through self-attention, shallow graph transformers extend this concept to graph data by com- puting self-attention weights based on both node features and graph topology </a>[94]. The primary objective of shallow graph transformers is to achieve exceptional performance while minimizing computational complexity and memory usage.</p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark101" class="a">Shallow graph transformers can be seen as a generalization of graph attention networks (GAT) </a><a href="#bookmark153" class="a">[42]. GAT use a multi-head attention mechanism to calculate node embeddings. However, GAT has some limitations, such as the inability to model edge features and lack of diversity among attention heads </a>[95]. Several GAT extensions have been proposed in the literature to address these issues. For example, GTN by Yun et al.</p><p style="padding-left: 7pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark122" class="a">[36] introduces edge-wise self-attention to incorporate edge information into node embeddings. Ahmad et al. </a><a href="#bookmark27" class="a">[63] proposed the graph attention transformer encoder (GATE), which applies a masked self-attention mechanism to learn different attention patterns for different nodes. GATE also uses a position-wise feed-forward network and dropout to enhance model capacity and generalization. The summary of shallow graph transformer methods is given in Table </a><a href="#bookmark27">II.</a></p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">Shallow graph transformers are efficient and adaptable capa-</p><p class="s117" style="padding-left: 28pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><span class="s31">h</span><span class="s116">(</span><span class="s34">l</span><span class="s116">+1) </span><span class="s22">= </span><span class="p">MLP</span>(<span class="s118">l</span>)<span class="s22">((1 + </span><span class="s21">ϵ</span>(<span class="s118">l</span>)<span class="s22">)</span><span class="s31">h</span><span class="s116">(</span><span class="s34">l</span><span class="s116">) </span><span class="s22">+ </span><span class="s60">Σ</span></p><p class="s36" style="padding-top: 5pt;padding-left: 7pt;text-indent: 0pt;line-height: 35%;text-align: left;"><span class="s115">h</span>(<span class="s35">l</span>)<span class="s80">)</span><span class="s102">,</span></p><p style="padding-top: 1pt;padding-left: 28pt;text-indent: 0pt;line-height: 11pt;text-align: left;">ble of handling various graph learning tasks and different types</p><p class="s35" style="padding-left: 53pt;text-indent: 0pt;line-height: 7pt;text-align: center;">i</p><p style="padding-top: 2pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s37" style="text-indent: 0pt;line-height: 10pt;text-align: left;">Σ</p><p style="text-indent: 0pt;text-align: left;"/><p class="s35" style="padding-left: 72pt;text-indent: 0pt;line-height: 9pt;text-align: left;">L</p><p class="s24" style="padding-left: 46pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><span class="s31">z</span>i<span class="s25"> </span><span class="s22">=  </span><span class="s21">α</span>il</p><p class="s35" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">l<span class="s36">=0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s36" style="text-indent: 0pt;text-align: left;"><span class="s115">h</span>(<span class="s35">l</span>)<span class="s102">,</span></p><p class="s35" style="padding-left: 29pt;text-indent: 0pt;line-height: 7pt;text-align: left;">i j</p><p class="s54" style="padding-left: 50pt;text-indent: 0pt;text-align: left;">j<span class="s71">∈N </span><span class="s73">(</span>v<span class="s39">i</span><span class="s73">)</span></p><p style="padding-top: 10pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s35" style="text-indent: 0pt;line-height: 7pt;text-align: left;">i</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 31pt;text-indent: 0pt;text-align: left;">(29)</p><p style="padding-left: 9pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">of graphs, but their lack of depth and recurrence may limit their ability to capture complex dependencies. Their performance can also be influenced by the choice of mask matrix and the number of attention heads, indicating a need for further research on their optimal design and regularization.</p><p class="s30" style="padding-left: 42pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><span class="s31">z</span><span class="s24">G</span><span class="s25"> </span><span class="s22">= </span><span class="p">READOUT</span><span class="s22">(</span>{<span class="s31">z</span><span class="s24">i</span><span class="s21">, </span>∀<span class="s21">v</span><span class="s24">i</span><span class="s25"> </span>∈ <span class="s21">V </span>}<span class="s22">)</span><span class="s21">.</span></p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;">Here, MLP<span class="s29">(</span><span class="s27">l</span><span class="s29">)</span><span class="s50"> </span>is a multi-layer perceptron, <i>ϵ</i><span class="s29">(</span><span class="s27">l</span><span class="s29">)</span><span class="s50"> </span>is a learnable parameter, READOUT indicates a set function that aggregates node output vectors into a graph output vector. <span class="s31">z</span><span class="s24">G</span><span class="s25"> </span>is the output vector of graph <i>G</i><a href="#bookmark151" class="a">. In addition, Nguyen et al. </a>[93] introduced UGformer, a self-attention-based method for the spectral coefficient computation of each node using eigenval- ues and eigenvectors of the Laplacian graph matrix. UGformer further integrates edge features into spectral computation via bilinear transformations.</p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Spectral attention mechanisms possess the ability to incor- porate the structural information of a graph into the spectral domain, thereby offering potential benefits for certain tasks or domains. However, they are also accompanied by certain limitations, including high computational cost, memory con- sumption and sensitivity to graph size and density.</p></li><li data-list-text="IV."><p style="padding-top: 7pt;padding-left: 56pt;text-indent: -17pt;text-align: left;"><a name="bookmark20">&zwnj;</a>Taxonomy of Graph Transformers<a name="bookmark26">&zwnj;</a></p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;">The past few years have witnessed a surge of interest in graph transformers. This section dives into four key categories dominating the current literature: shallow, deep, scalable, and pre-trained graph transformers. By analyzing representative models within each category, we aim to establish valuable guidelines for designing effective graph transformers.</p><ol id="l7"><li data-list-text="A."><p class="s21" style="padding-top: 10pt;padding-left: 21pt;text-indent: -13pt;text-align: left;"><a name="bookmark21">&zwnj;</a>Shallow Graph Transformers</p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;">Shallow graph transformers represent a class of GNNs that leverage the power of self-attention to acquire node</p><p style="padding-top: 7pt;text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="B."><p class="s21" style="padding-left: 21pt;text-indent: -13pt;text-align: justify;"><a name="bookmark22">&zwnj;</a>Deep Graph Transformers</p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark160" class="a">Deep graph transformers consist of multiple self-attention layers stacked on top of each other, with optional skip con- nections, residual connections or dense connections between layers </a><a href="#bookmark161" class="a">[102]. They are designed to achieve higher performance with increased model depth and complexity </a>[103]. Deep graph transformers extend shallow graph transformers by applying self-attention layers to node features and graph topology hierarchically.</p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark162" class="a">However, deep graph transformers also face several chal- lenges that need to be addressed. One challenge is the dif- ficulty of training deeper models, which can be mitigated by employing techniques, such as PairNorm introduced in DeeperGCN </a><a href="#bookmark152" class="a">[104]. Another challenge is the over-smoothing problem, which can be addressed by using a gated residual connection and a generalized convolution operator as proposed in DeeperGCN. Additionally, the disappearance of global attention capacity and the lack of diversity among attention heads are challenges that can be tackled by approaches like DeepGraph </a>[94]. DeepGraph incorporates substructure tokens and local attention to improve the focus and diversity of global attention.</p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Deep graph transformers, while complex, can achieve top- tier results on various graph learning tasks and adapt to different types of graphs and domains. However, their high computational cost, difficulty in optimization, and sensitivity</p><p class="s7" style="padding-top: 6pt;padding-left: 71pt;text-indent: 0pt;line-height: 9pt;text-align: center;"><a name="bookmark27">&zwnj;</a>TABLE II</p><p class="s7" style="padding-left: 71pt;text-indent: 0pt;line-height: 9pt;text-align: center;">Shallow graph transformers</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 1pt;text-align: left;"/><p style="padding-top: 4pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: right;">Model</p><p class="s63" style="padding-top: 1pt;padding-left: 63pt;text-indent: -17pt;text-align: left;">Graph inductive bias</p><p class="s63" style="padding-left: 46pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Graph attention</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"/><p class="s119" style="padding-left: 51pt;text-indent: 0pt;line-height: 12pt;text-align: left;">mechanisms     <span class="s63">Application          Dataset      Code</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-left: 105pt;text-indent: 0pt;text-align: left;">Node Positional encoding</p><p class="s63" style="padding-left: 17pt;text-indent: 0pt;text-align: justify;">Edge Struc- tural Encod- ing</p><p class="s63" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Message- Passing Bias</p><p class="s63" style="padding-top: 4pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Global attention</p><p class="s63" style="padding-top: 4pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">Local Attention</p><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: right;">availability</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><table style="border-collapse:collapse;margin-left:3pt" cellspacing="0"><tr style="height:26pt"><td style="width:181pt" colspan="3"><p class="s120" style="padding-top: 7pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark75" class="s121">Graph Transformer </a>[16]   Yes (local)  Yes (local)</p></td><td style="width:42pt"><p class="s120" style="padding-top: 3pt;padding-left: 6pt;text-indent: 3pt;text-align: left;">Yes (pre- processing)</p></td></tr><tr style="height:22pt"><td style="width:84pt"><p style="padding-top: 5pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark101" class="s121">GAT [42]</a></p></td><td style="width:52pt"><p class="s120" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:45pt"><p class="s120" style="padding-top: 5pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:42pt"><p class="s120" style="padding-top: 5pt;padding-left: 4pt;text-indent: 0pt;text-align: center;">No</p></td></tr><tr style="height:24pt"><td style="width:84pt"><p style="padding-top: 7pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark153" class="s121">GATv2 [95]</a></p></td><td style="width:52pt"><p class="s120" style="padding-top: 7pt;padding-left: 5pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:45pt"><p class="s120" style="padding-top: 7pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:42pt"><p class="s120" style="padding-top: 7pt;padding-left: 4pt;text-indent: 0pt;text-align: center;">No</p></td></tr><tr style="height:24pt"><td style="width:84pt"><p style="padding-top: 7pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark95" class="s121">GTN [36]</a></p></td><td style="width:52pt"><p class="s120" style="padding-top: 7pt;padding-left: 5pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:45pt"><p class="s120" style="padding-top: 7pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:42pt"><p class="s120" style="padding-top: 7pt;padding-left: 4pt;text-indent: 0pt;text-align: center;">No</p></td></tr><tr style="height:24pt"><td style="width:84pt"><p style="padding-top: 7pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark122" class="s121">GATE [63]</a></p></td><td style="width:52pt"><p class="s120" style="padding-top: 7pt;padding-left: 5pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:45pt"><p class="s120" style="padding-top: 7pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:42pt"><p class="s120" style="padding-top: 7pt;padding-left: 4pt;text-indent: 0pt;text-align: center;">No</p></td></tr><tr style="height:22pt"><td style="width:84pt"><p style="padding-top: 7pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark154" class="s121">PiT [96]</a></p></td><td style="width:52pt"><p class="s120" style="padding-top: 7pt;padding-left: 5pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:45pt"><p class="s120" style="padding-top: 7pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:42pt"><p class="s120" style="padding-top: 7pt;padding-left: 4pt;text-indent: 0pt;text-align: center;">No</p></td></tr><tr style="height:18pt"><td style="width:84pt"><p style="padding-top: 5pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark155" class="s121">WDPruning [97]</a></p></td><td style="width:52pt"><p class="s120" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:45pt"><p class="s120" style="padding-top: 5pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:42pt"><p class="s120" style="padding-top: 5pt;padding-left: 4pt;text-indent: 0pt;text-align: center;">No</p></td></tr><tr style="height:16pt"><td style="width:84pt"><p style="padding-top: 3pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark156" class="s121">SVT [98]</a></p></td><td style="width:52pt"><p class="s120" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:45pt"><p class="s120" style="padding-top: 3pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:42pt"><p class="s120" style="padding-top: 3pt;padding-left: 4pt;text-indent: 0pt;text-align: center;">No</p></td></tr><tr style="height:16pt"><td style="width:84pt"><p style="padding-top: 3pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark157" class="s121">Gubelmann et al. [99]</a></p></td><td style="width:52pt"><p class="s120" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:45pt"><p class="s120" style="padding-top: 3pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:42pt"><p class="s120" style="padding-top: 3pt;padding-left: 4pt;text-indent: 0pt;text-align: center;">No</p></td></tr><tr style="height:26pt"><td style="width:84pt"><p style="padding-top: 7pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark158" class="s121">TADDY [100]</a></p></td><td style="width:52pt"><p class="s120" style="padding-top: 3pt;padding-left: 14pt;padding-right: 6pt;text-indent: -2pt;text-align: left;">Yes (global and local)</p></td><td style="width:45pt"><p class="s120" style="padding-top: 7pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:42pt"><p class="s120" style="padding-top: 7pt;padding-left: 4pt;text-indent: 0pt;text-align: center;">No</p></td></tr><tr style="height:20pt"><td style="width:84pt"><p style="padding-top: 5pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark97" class="s121">Tree Transformer [38]</a></p></td><td style="width:52pt"><p class="s120" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: center;">Yes (local)</p></td><td style="width:45pt"><p class="s120" style="padding-top: 5pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:42pt"><p class="s120" style="padding-top: 5pt;padding-left: 4pt;text-indent: 0pt;text-align: center;">No</p></td></tr><tr style="height:22pt"><td style="width:84pt"><p style="padding-top: 5pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark102" class="s121">GAT-POS [43]</a></p></td><td style="width:52pt"><p class="s120" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: center;">Yes (local)</p></td><td style="width:45pt"><p class="s120" style="padding-top: 5pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:42pt"><p class="s120" style="padding-top: 5pt;padding-left: 4pt;text-indent: 0pt;text-align: center;">No</p></td></tr><tr style="height:24pt"><td style="width:84pt"><p style="padding-top: 7pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark104" class="s121">DGT [45]</a></p></td><td style="width:52pt"><p class="s120" style="padding-top: 7pt;padding-left: 5pt;text-indent: 0pt;text-align: center;">Yes</p></td><td style="width:45pt"><p class="s120" style="padding-top: 7pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:42pt"><p class="s120" style="padding-top: 7pt;padding-left: 4pt;text-indent: 0pt;text-align: center;">No</p></td></tr><tr style="height:24pt"><td style="width:84pt"><p style="padding-top: 7pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark113" class="s121">EDTER [54]</a></p></td><td style="width:52pt"><p class="s120" style="padding-top: 7pt;padding-left: 5pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:45pt"><p class="s120" style="padding-top: 7pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:42pt"><p class="s120" style="padding-top: 7pt;padding-left: 4pt;text-indent: 0pt;text-align: center;">No</p></td></tr><tr style="height:22pt"><td style="width:84pt"><p style="padding-top: 7pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark114" class="s121">BGT-Net [55]</a></p></td><td style="width:52pt"><p class="s120" style="padding-top: 7pt;padding-left: 5pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:45pt"><p class="s120" style="padding-top: 7pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:42pt"><p class="s120" style="padding-top: 7pt;padding-left: 4pt;text-indent: 0pt;text-align: center;">No</p></td></tr><tr style="height:18pt"><td style="width:84pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a href="#bookmark120" class="s121">PMPNet [61]</a></p></td><td style="width:52pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s120" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: center;">No</p></td><td style="width:45pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s120" style="text-indent: 0pt;line-height: 8pt;text-align: center;">No</p></td><td style="width:42pt"><p class="s120" style="padding-top: 5pt;padding-left: 4pt;text-indent: 0pt;text-align: center;">Yes (pre-</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-left: 292pt;text-indent: -7pt;text-align: left;">Yes (message passing)</p><p class="s63" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">Graph representation learning</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="padding-left: 25pt;text-indent: 0pt;text-align: left;">ZINC, PATTERN, CLUSTER</p><p style="padding-top: 4pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="padding-left: 47pt;text-indent: 0pt;text-align: left;">Yes</p><p class="s63" style="padding-top: 4pt;padding-left: 247pt;text-indent: 5pt;text-align: left;">Yes (linear)</p><p class="s63" style="padding-top: 7pt;padding-left: 247pt;text-indent: 5pt;text-align: left;">Yes (linear)</p><p class="s63" style="padding-top: 4pt;padding-left: 22pt;text-indent: -7pt;text-align: left;">Yes (message passing)</p><p class="s63" style="padding-top: 7pt;padding-left: 22pt;text-indent: -7pt;text-align: left;">Yes (message passing)</p><p class="s63" style="padding-left: 9pt;text-indent: 0pt;text-align: justify;">Node classification of graph-structured data</p><p class="s63" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">Graph representation learning</p><p class="s63" style="padding-top: 7pt;padding-left: 10pt;text-indent: 0pt;text-align: left;">Cora, Citeseer, Pubmed, PPI   Yes</p><p style="padding-top: 3pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="padding-left: 10pt;text-indent: 0pt;text-align: left;">OGB, QM9, VARMISUSE   <span class="s119">Yes</span></p><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-top: 3pt;padding-left: 292pt;text-indent: -7pt;text-align: left;">Yes (message passing)</p><p class="s63" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">Node classification on heterogeneous graphs</p><p class="s63" style="padding-top: 7pt;padding-left: 10pt;text-indent: 0pt;text-align: left;">DBLP, ACM, IMDB      Yes</p><p class="s63" style="padding-top: 3pt;padding-left: 247pt;text-indent: 5pt;text-align: left;">Yes (linear)</p><p class="s63" style="padding-top: 7pt;padding-left: 242pt;text-indent: 10pt;text-align: left;">Yes (quadratic)</p><p class="s63" style="padding-top: 3pt;padding-left: 242pt;text-indent: 10pt;text-align: left;">Yes (quadratic)</p><p class="s63" style="padding-left: 59pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Cross-lingual</p><p class="s63" style="padding-left: 59pt;text-indent: -35pt;text-align: left;">No   relation and event extraction</p><p class="s63" style="padding-left: 59pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Image</p><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-left: 59pt;text-indent: -35pt;text-align: left;">No    classification, object detection Image classification and image retrieval</p><p class="s63" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">ACE 2005 corpus       Yes</p><p style="padding-top: 3pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: left;">Yes</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">ImageNet, COCO, ImageNet-A</p><p class="s63" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">CIFAR-10 and ILSVRC-12   No</p><p class="s63" style="padding-left: 242pt;text-indent: 10pt;text-align: left;">Yes (quadratic)</p><p class="s63" style="padding-top: 3pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">No    Image classification  Synthetic data and CIFAR-10   No</p><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: left;">Yes    No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-left: 335pt;text-indent: 0pt;text-align: left;">Natural language inference (NLI)</p><p class="s63" style="padding-top: 3pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">SNLI and MNLI        No</p><p class="s63" style="padding-top: 3pt;padding-left: 247pt;text-indent: 5pt;text-align: left;">Yes (linear)</p><p class="s63" style="padding-top: 7pt;padding-left: 242pt;text-indent: 10pt;text-align: left;">Yes (quadratic)</p><p class="s63" style="padding-top: 3pt;padding-left: 59pt;text-indent: 0pt;text-align: left;">Anomaly detection in dynamic graphs</p><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-top: 3pt;padding-left: 59pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Program translation</p><p class="s63" style="padding-left: 59pt;text-indent: -35pt;text-align: left;">No   and semantic parsing</p><p class="s63" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">UCI Messages, Enron, DBLP, Facebook, Twitter and Reddit.</p><p class="s63" style="padding-left: 8pt;text-indent: 0pt;line-height: 8pt;text-align: left;">For2Lam,</p><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">CoffeeScript-JavaScript, JOBS, GEO, ATIS</p><p class="s63" style="padding-top: 7pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">No</p><p style="padding-top: 7pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="padding-left: 71pt;text-indent: 0pt;text-align: center;">Yes</p><p class="s63" style="padding-left: 247pt;text-indent: 5pt;text-align: left;">Yes (linear)</p><p class="s63" style="padding-top: 7pt;padding-left: 247pt;text-indent: 5pt;text-align: left;">Yes (linear)</p><p class="s63" style="padding-left: 22pt;text-indent: -7pt;text-align: left;">Yes (message passing)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: left;">No</p><p style="padding-left: 29pt;text-indent: 0pt;line-height: 8pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: left;">Node classification</p><p style="padding-left: 9pt;text-indent: 0pt;line-height: 8pt;text-align: left;"/><p style="padding-top: 3pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">Node classification on graphs</p><p class="s63" style="padding-left: 10pt;text-indent: 0pt;text-align: left;">Chameleon, Squirrel, Actor, Cora, Citeseer, Pubmed Actor, Squirrel, Chameleon, Cora, Citeseer,</p><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: left;">Multicue</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-left: 10pt;text-indent: 0pt;text-align: left;">twitch-gamers, ogbn-arxiv, Reddit</p><p class="s63" style="padding-left: 247pt;text-indent: 5pt;text-align: left;">Yes (linear)</p><p class="s63" style="padding-left: 22pt;text-indent: -7pt;text-align: left;">Yes (message passing)</p><p class="s122" style="padding-left: 9pt;text-indent: 0pt;line-height: 92%;text-align: left;">Edge detection   <span class="s63">BSDS500, NYUDv2,</span></p><p class="s63" style="padding-top: 3pt;padding-left: 39pt;text-indent: 0pt;text-align: left;">Yes</p><p style="padding-top: 3pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="text-indent: 0pt;text-align: right;">Yes     No</p><p class="s63" style="padding-top: 3pt;padding-left: 24pt;text-indent: 0pt;text-align: left;">Scene Graph Generation for Images</p><p class="s63" style="padding-left: 23pt;text-indent: 0pt;text-align: left;">Visual Genome (VG), OpenImages (OI), Visual Relationship Detection (VRD)</p><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><table style="border-collapse:collapse;margin-left:3pt" cellspacing="0"><tr style="height:11pt"><td style="width:178pt" colspan="3"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:334pt"><p class="s120" style="padding-left: 12pt;text-indent: 0pt;line-height: 8pt;text-align: left;">processing)        passing)</p></td></tr><tr style="height:12pt"><td style="width:81pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark133" class="s121">GAMLP [74]</a></p></td><td style="width:58pt"><p class="s120" style="padding-top: 1pt;padding-right: 18pt;text-indent: 0pt;text-align: right;">No</p></td><td style="width:39pt"><p class="s120" style="padding-top: 1pt;padding-right: 12pt;text-indent: 0pt;text-align: right;">No</p></td><td style="width:334pt"><p class="s120" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;">No     <span class="s123">Yes   </span>Yes (spectral)  Node classification  14 real-world graph datasets   Yes</p><p class="s120" style="padding-left: 61pt;text-indent: 0pt;line-height: 3pt;text-align: left;">(linear)</p></td></tr><tr style="height:12pt"><td style="width:81pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:58pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:39pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:334pt"><p class="s120" style="padding-top: 1pt;padding-left: 215pt;text-indent: 0pt;text-align: left;">MUTAG, PTC, NCI1,</p></td></tr><tr style="height:12pt"><td style="width:81pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark159" class="s121">SA-GAT [101]</a></p></td><td style="width:58pt"><p class="s120" style="padding-top: 1pt;padding-right: 18pt;text-indent: 0pt;text-align: right;">No</p></td><td style="width:39pt"><p class="s120" style="padding-top: 1pt;padding-right: 12pt;text-indent: 0pt;text-align: right;">No</p></td><td style="width:334pt"><p class="s120" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;">No     <span class="s123">Yes   </span>Yes (spectral)  Graph classification  <span class="s123">NCI109, PROTEINS,      </span>No</p><p class="s120" style="padding-left: 24pt;padding-right: 21pt;text-indent: 0pt;line-height: 3pt;text-align: center;">(linear)                   ENZYMES, DD and</p></td></tr><tr style="height:14pt"><td style="width:81pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:58pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:39pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:334pt"><p class="s120" style="padding-top: 1pt;padding-left: 215pt;text-indent: 0pt;text-align: left;">COLLAB</p></td></tr><tr style="height:24pt"><td style="width:81pt"><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark135" class="s121">GKAT [76]</a></p></td><td style="width:58pt"><p class="s120" style="padding-top: 3pt;padding-right: 18pt;text-indent: 0pt;text-align: right;">No</p></td><td style="width:39pt"><p class="s120" style="padding-top: 3pt;padding-right: 12pt;text-indent: 0pt;text-align: right;">No</p></td><td style="width:334pt"><p class="s124" style="padding-left: 24pt;text-indent: 0pt;line-height: 34%;text-align: left;">No     <span class="s125">Yes     </span>No    <span class="s120">Graph classification  Bioinformatics and social     </span>No</p><p class="s125" style="padding-left: 61pt;text-indent: 0pt;line-height: 62%;text-align: left;">(linear)         <span class="s120">and graph pattern   networks datasets, ImageNet</span></p><p class="s120" style="padding-left: 149pt;text-indent: 0pt;line-height: 6pt;text-align: left;">detection    dataset</p></td></tr><tr style="height:22pt"><td style="width:81pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark137" class="s121">EcoFormer [78]</a></p></td><td style="width:58pt"><p class="s120" style="padding-top: 1pt;padding-right: 18pt;text-indent: 0pt;text-align: right;">No</p></td><td style="width:39pt"><p class="s120" style="padding-top: 1pt;padding-right: 12pt;text-indent: 0pt;text-align: right;">No</p></td><td style="width:334pt"><p class="s125" style="padding-left: 24pt;text-indent: 0pt;line-height: 25%;text-align: left;">No     <span class="s120">Yes     </span>No    <span class="s123">Image classification  </span><span class="s120">CIFAR-100, ImageNet-1K,    </span>Yes</p><p class="s120" style="padding-left: 19pt;padding-right: 21pt;text-indent: 0pt;line-height: 6pt;text-align: center;">(linear)         <span class="s123">and long-range    </span>Long Range Arena</p><p class="s120" style="padding-left: 13pt;padding-right: 35pt;text-indent: 0pt;line-height: 6pt;text-align: center;">tasks</p></td></tr><tr style="height:24pt"><td style="width:81pt"><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark142" class="s121">Reformer [84]</a></p></td><td style="width:58pt"><p class="s120" style="padding-top: 3pt;padding-right: 18pt;text-indent: 0pt;text-align: right;">No</p></td><td style="width:39pt"><p class="s120" style="padding-top: 3pt;padding-right: 12pt;text-indent: 0pt;text-align: right;">No</p></td><td style="width:334pt"><p class="s124" style="padding-left: 24pt;text-indent: 0pt;line-height: 27%;text-align: left;">No     <span class="s125">Yes    Yes (message  NLP and      </span><span class="s120">Synthetic task, enwik8,     </span>Yes</p><p class="s120" style="padding-left: 35pt;padding-right: 21pt;text-indent: 0pt;line-height: 9pt;text-align: center;">(linear)   passing)   generative tasks   <span class="s123">imagenet64, WMT 2014</span></p><p class="s120" style="padding-left: 215pt;text-indent: 0pt;line-height: 6pt;text-align: left;">English-German</p></td></tr><tr style="height:21pt"><td style="width:81pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark143" class="s121">LISA [85]</a></p></td><td style="width:58pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s120" style="padding-top: 1pt;padding-right: 18pt;text-indent: 0pt;text-align: right;">No</p></td><td style="width:39pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s120" style="padding-top: 1pt;padding-right: 12pt;text-indent: 0pt;text-align: right;">No</p></td><td style="width:334pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s124" style="padding-left: 24pt;text-indent: 0pt;line-height: 7%;text-align: left;">No     <span class="s125">Yes     </span>No    <span class="s125">Sequential      </span><span class="s120">Alibaba, Amazon Video     </span>Yes</p><p class="s120" style="padding-left: 215pt;padding-right: 46pt;text-indent: -154pt;line-height: 49%;text-align: left;">(linear)       recommendation  <span class="s123">Games, MovieLens 1M, </span>MovieLens 25M</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"/><p class="s122" style="text-indent: 0pt;line-height: 90%;text-align: right;">No   <span class="s63">Yes (message</span></p><p class="s63" style="padding-top: 3pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">3D object detection  nuScenes           Yes</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark28" class="a">to hyperparameters necessitate further research for optimal design and training. Table </a>III summarizes the methods of deep graph transformers.</p><p style="padding-top: 3pt;text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="C."><p class="s21" style="padding-left: 22pt;text-indent: -14pt;text-align: justify;"><a name="bookmark23">&zwnj;</a>Scalable Graph Transformers</p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark98" class="a">Scalable graph transformers are a category of graph trans- formers that tackle the challenges of scalability and efficiency when applying self-attention to large-scale graphs </a><a href="#bookmark112" class="a">[39], </a><a href="#bookmark112">[53],</a></p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark173" class="a">[114], </a><a href="#bookmark71" class="a">[115]. These transformers are specifically designed to reduce computational cost and memory usage while main- taining or improving performance. To achieve this, various techniques are employed to reduce the complexity of self- attention, such as sparse attention, local attention, and low- rank approximation </a><a href="#bookmark173" class="a">[12], </a>[115]. Scalable graph transformers can be regarded as an enhancement of deep graph transformers addressing challenges, such as over-smoothing and limited</p><p class="s7" style="padding-top: 6pt;padding-left: 71pt;text-indent: 0pt;line-height: 9pt;text-align: center;"><a name="bookmark28">&zwnj;</a>TABLE III</p><p class="s7" style="padding-left: 71pt;text-indent: 0pt;line-height: 9pt;text-align: center;">Deep graph transformers</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 1pt;text-align: left;"/><p style="padding-top: 4pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: right;">Model</p><p class="s63" style="padding-top: 1pt;padding-left: 60pt;text-indent: -17pt;text-align: left;">Graph inductive bias</p><p class="s63" style="padding-top: 1pt;padding-left: 42pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Graph attention</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"/><p class="s119" style="padding-left: 47pt;text-indent: 0pt;line-height: 12pt;text-align: left;">mechanisms    <span class="s63">Application              Datasets     Code</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-left: 93pt;text-indent: 0pt;text-align: left;">Node Positional encoding</p><p class="s63" style="padding-left: 12pt;text-indent: 0pt;text-align: left;">Edge Structural Encoding</p><p class="s63" style="padding-left: 12pt;text-indent: 0pt;text-align: left;">Message- Passing Bias</p><p class="s63" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Global attention</p><p class="s63" style="padding-top: 4pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">Local Attention</p><p class="s63" style="text-indent: 0pt;text-align: right;">availability</p><p class="s63" style="text-indent: 0pt;text-align: left;">classification</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="text-indent: 0pt;text-align: left;">No     No     No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="text-indent: 0pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;text-align: left;"><a href="#bookmark160" class="s126">HG-Transformer </a>[102]    No     No     No    Yes (linear)    No    <span class="s119">Multi-label text</span></p><p class="s63" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;line-height: 8pt;text-align: left;">RCV1, RCV1-2K,</p><p class="s63" style="text-indent: 0pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-left: 13pt;text-indent: 0pt;line-height: 8pt;text-align: left;">AmazonCat-13K</p><p style="padding-left: 13pt;text-indent: 0pt;text-align: left;"><a href="#bookmark161" class="s126">Unfolded Transformer [103]</a></p><p class="s63" style="padding-left: 13pt;text-indent: 10pt;text-align: left;">Yes (quadratic)</p><p class="s63" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">Graph neural networks and NLP</p><p class="s63" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">IMDB and SST2      Yes</p><p class="s63" style="text-indent: 0pt;text-align: left;">(relative)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;text-align: left;"><a href="#bookmark152" class="s126">DeepGraph </a>[94]      <span class="s119">Yes</span></p><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: right;">Yes</p><p class="s63" style="padding-left: 137pt;text-indent: -124pt;text-align: right;">No     No    Yes (linear)   (message passing)</p><p class="s63" style="padding-top: 3pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">Graph representation learning</p><p class="s63" style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">PCQM4M-LSC, ZINC, CLUSTER, PATTERN</p><p class="s63" style="padding-top: 7pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Yes</p><p class="s63" style="text-indent: 0pt;text-align: left;">knowledge graphs</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-left: 13pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark163" class="s126">GraphWriter </a>[105]      No     Yes     No    Yes (linear)    No    <span class="s119">Text generation from</span></p><p class="s63" style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">AGENDA dataset     Yes</p><table style="border-collapse:collapse;margin-left:3pt" cellspacing="0"><tr style="height:25pt"><td style="width:162pt" colspan="3"><p class="s120" style="padding-top: 8pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark164" class="s121">GraphLoG </a>[106]      No     No</p></td><td style="width:41pt"><p class="s120" style="padding-top: 4pt;padding-left: 5pt;text-indent: 2pt;text-align: left;">Yes (pre- processing)</p></td></tr><tr style="height:14pt"><td style="width:68pt"><p style="padding-top: 3pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark99" class="s121">DIET [40]</a></p></td><td style="width:52pt"><p class="s120" style="padding-top: 3pt;padding-right: 16pt;text-indent: 0pt;text-align: right;">Yes</p></td><td style="width:42pt"><p class="s120" style="padding-top: 3pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:41pt"><p class="s120" style="padding-top: 3pt;padding-left: 2pt;text-indent: 0pt;text-align: center;">No</p></td></tr><tr style="height:28pt"><td style="width:68pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark100" class="s121">GRPE [41]</a></p></td><td style="width:52pt"><p class="s120" style="padding-top: 1pt;padding-left: 16pt;padding-right: 6pt;text-indent: 0pt;text-align: center;">Yes (local and global)</p></td><td style="width:42pt"><p class="s120" style="padding-top: 1pt;padding-left: 6pt;padding-right: 5pt;text-indent: 0pt;text-align: center;">Yes (local and global)</p></td><td style="width:41pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s120" style="padding-left: 2pt;text-indent: 0pt;text-align: center;">No</p></td></tr><tr style="height:11pt"><td style="width:68pt"><p style="padding-top: 1pt;padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a href="#bookmark70" class="s121">Graphormer [11]</a></p></td><td style="width:52pt"><p class="s120" style="padding-top: 1pt;padding-right: 16pt;text-indent: 0pt;line-height: 8pt;text-align: right;">No</p></td><td style="width:42pt"><p class="s120" style="padding-top: 1pt;text-indent: 0pt;line-height: 8pt;text-align: center;">Yes</p></td><td style="width:41pt"><p class="s120" style="padding-top: 1pt;padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: center;">No</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-top: 3pt;text-indent: 0pt;line-height: 8pt;text-align: right;">Yes</p><p class="s63" style="text-indent: 0pt;text-align: left;">Yes</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-left: 272pt;text-indent: -37pt;text-align: right;">No    (message passing)</p><p class="s63" style="padding-top: 7pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">Graph representation learning</p><p class="s63" style="padding-top: 7pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">ZINC15, MoleculeNet, Protein ego-networks</p><p class="s63" style="text-indent: 0pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-left: 223pt;text-indent: 10pt;text-align: left;">Yes (quadratic)</p><p class="s63" style="padding-top: 7pt;text-indent: 0pt;text-align: right;">Yes (linear)   No</p><p class="s63" style="padding-left: 21pt;text-indent: 0pt;text-align: left;">cross-lingual generalization, machine translation</p><p class="s63" style="padding-top: 3pt;padding-left: 21pt;text-indent: 0pt;text-align: left;">Graph representation learning</p><p class="s63" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">GLUE, XTREME, WMT 2018</p><p class="s63" style="text-indent: 0pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="text-indent: 0pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">ZINC, MolPCBA, MolHIV, PATTERN, CLUSTER,   No PCQM4M, PCQM4Mv2</p><p class="s63" style="padding-left: 223pt;text-indent: 10pt;text-align: left;">Yes (quadratic)</p><p class="s63" style="padding-left: 55pt;text-indent: 0pt;text-align: left;">Graph representation learning</p><p class="s63" style="padding-left: 31pt;text-indent: 0pt;text-align: left;">PCQM4M-LSC, OGB, ZINC</p><p class="s63" style="padding-top: 3pt;padding-left: 70pt;text-indent: 0pt;text-align: center;">Yes</p><p style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;text-align: left;"><a href="#bookmark85" class="s126">Relation-aware  Self- Attention [26]</a></p><p class="s63" style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">No   Yes (local)   No     <span class="s119">Yes</span></p><p class="s63" style="padding-left: 139pt;text-indent: 0pt;line-height: 8pt;text-align: left;">WMT 2014</p><p class="s63" style="padding-left: 13pt;text-indent: 0pt;line-height: 8pt;text-align: left;">No    Machine translation      English-German and</p><p class="s63" style="text-indent: 0pt;text-align: left;">Yes</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-left: 139pt;text-indent: 0pt;line-height: 8pt;text-align: left;">English-French</p><p class="s63" style="padding-top: 7pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Yes</p><p class="s63" style="text-indent: 0pt;text-align: left;">(quadratic)</p><p style="text-indent: 0pt;text-align: left;"/><table style="border-collapse:collapse;margin-left:3pt" cellspacing="0"><tr style="height:17pt"><td style="width:78pt"><p style="padding-top: 8pt;padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: left;"><a href="#bookmark105" class="s121">GraphiT [46]</a></p></td><td style="width:47pt"><p class="s120" style="padding-left: 15pt;padding-right: 3pt;text-indent: -9pt;line-height: 8pt;text-align: left;">Yes (local and</p></td><td style="width:38pt"><p class="s120" style="padding-top: 8pt;padding-left: 12pt;text-indent: 0pt;line-height: 7pt;text-align: left;">No</p></td><td style="width:28pt"><p class="s120" style="padding-top: 8pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: right;">No</p></td></tr><tr style="height:24pt"><td style="width:78pt"><p style="padding-top: 7pt;text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: left;"><a href="#bookmark107" class="s121">Tree Transformer [48]</a></p></td><td style="width:47pt"><p class="s120" style="padding-left: 15pt;text-indent: -4pt;line-height: 8pt;text-align: left;">global)</p><p class="s120" style="padding-left: 10pt;padding-right: 2pt;text-indent: 4pt;line-height: 8pt;text-align: left;">Yes (global</p></td><td style="width:38pt"><p style="padding-top: 7pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s120" style="padding-left: 12pt;text-indent: 0pt;line-height: 7pt;text-align: left;">No</p></td><td style="width:28pt"><p style="padding-top: 7pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s120" style="padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: right;">No</p></td></tr><tr style="height:10pt"><td style="width:78pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:47pt"><p class="s120" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: center;">and local)</p></td><td style="width:38pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:28pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:18pt"><td style="width:78pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: left;"><a href="#bookmark108" class="s121">MagLapNet [49]</a></p></td><td style="width:47pt"><p class="s120" style="padding-left: 15pt;padding-right: 3pt;text-indent: -9pt;line-height: 8pt;text-align: left;">Yes (local and</p></td><td style="width:38pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s120" style="padding-left: 12pt;text-indent: 0pt;line-height: 7pt;text-align: left;">No</p></td><td style="width:28pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s120" style="padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: right;">No</p></td></tr><tr style="height:10pt"><td style="width:78pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:47pt"><p class="s120" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: center;">global)</p></td><td style="width:38pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:28pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:18pt"><td style="width:78pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: left;"><a href="#bookmark111" class="s121">SAT [52]</a></p></td><td style="width:47pt"><p class="s120" style="padding-left: 15pt;padding-right: 3pt;text-indent: -9pt;line-height: 8pt;text-align: left;">Yes (local and</p></td><td style="width:38pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s120" style="padding-left: 12pt;text-indent: 0pt;line-height: 7pt;text-align: left;">No</p></td><td style="width:28pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s120" style="padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: right;">No</p></td></tr><tr style="height:14pt"><td style="width:78pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:47pt"><p class="s120" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: center;">global)</p></td><td style="width:38pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:28pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:26pt"><td style="width:78pt"><p style="padding-top: 5pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark165" class="s121">Edge Transformer [107]</a></p></td><td style="width:47pt"><p class="s120" style="padding-top: 5pt;padding-right: 5pt;text-indent: 0pt;text-align: center;">Yes</p></td><td style="width:38pt"><p class="s120" style="padding-top: 5pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Yes</p></td><td style="width:28pt"><p class="s120" style="padding-top: 5pt;padding-right: 2pt;text-indent: 0pt;text-align: right;">No</p></td></tr><tr style="height:26pt"><td style="width:78pt"><p style="padding-top: 3pt;text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark121" class="s121">Cai et al. [62]</a></p></td><td style="width:47pt"><p style="padding-top: 3pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s120" style="padding-right: 5pt;text-indent: 0pt;text-align: center;">Yes</p></td><td style="width:38pt"><p style="padding-top: 3pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s120" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">Yes</p></td><td style="width:28pt"><p style="padding-top: 3pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s120" style="padding-right: 2pt;text-indent: 0pt;text-align: right;">No</p></td></tr><tr style="height:22pt"><td style="width:78pt"><p style="padding-top: 5pt;text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: left;"><a href="#bookmark125" class="s121">SAN [66]</a></p></td><td style="width:47pt"><p class="s120" style="padding-top: 4pt;padding-left: 15pt;padding-right: 3pt;text-indent: -9pt;line-height: 8pt;text-align: left;">Yes (local and</p></td><td style="width:38pt"><p style="padding-top: 5pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s120" style="padding-left: 12pt;text-indent: 0pt;line-height: 7pt;text-align: left;">No</p></td><td style="width:28pt"><p style="padding-top: 5pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s120" style="padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: right;">No</p></td></tr><tr style="height:12pt"><td style="width:78pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:47pt"><p class="s120" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: center;">global)</p></td><td style="width:38pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:28pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:13pt"><td style="width:78pt"><p style="padding-top: 3pt;padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a href="#bookmark138" class="s121">KMCA [79]</a></p></td><td style="width:47pt"><p class="s120" style="padding-top: 3pt;padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: center;">No</p></td><td style="width:38pt"><p class="s120" style="padding-top: 3pt;padding-left: 12pt;text-indent: 0pt;line-height: 8pt;text-align: left;">No</p></td><td style="width:28pt"><p class="s120" style="padding-top: 3pt;padding-right: 2pt;text-indent: 0pt;line-height: 8pt;text-align: right;">No</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-top: 4pt;padding-left: 223pt;text-indent: 10pt;text-align: left;">Yes (quadratic)</p><p class="s63" style="padding-left: 89pt;text-indent: 0pt;text-align: center;">Yes (message passing)</p><p class="s63" style="padding-left: 12pt;text-indent: 0pt;text-align: left;">Graph representation learning for classification and regression tasks</p><p class="s63" style="padding-top: 4pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">MUTAG, PROTEINS, PTC, NCI1, ZINC</p><p class="s63" style="padding-top: 3pt;padding-left: 223pt;text-indent: 10pt;text-align: left;">Yes (quadratic)</p><p class="s63" style="padding-left: 89pt;text-indent: 0pt;text-align: center;">Yes (message passing)</p><p class="s63" style="padding-top: 3pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">Code representation learning  <span class="s119">Python150k and</span></p><p class="s63" style="padding-top: 7pt;padding-left: 90pt;text-indent: 0pt;text-align: center;">Yes</p><p class="s63" style="padding-top: 7pt;padding-left: 223pt;text-indent: 10pt;text-align: left;">Yes (quadratic)</p><p style="padding-top: 3pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="padding-left: 223pt;text-indent: 10pt;text-align: left;">Yes (quadratic)</p><p class="s63" style="padding-top: 7pt;padding-left: 14pt;text-indent: 0pt;text-align: center;">Yes (spectral)</p><p class="s63" style="padding-top: 7pt;padding-left: 89pt;text-indent: 0pt;text-align: center;">Yes (message passing)</p><p class="s63" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">Graph learning tasks such as correctness testing of sorting networks and source code understanding</p><p class="s63" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">Graph representation learning for graph and node property prediction</p><p class="s63" style="text-indent: 0pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-left: 11pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Improve systematic</p><p class="s63" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Synthetic graphs, OGB Code2</p><p class="s63" style="text-indent: 0pt;text-align: left;">JavaScript150k</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">ZINC, CLUSTER, PATTERN, OGBG-PPA, OGBG-CODE2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="text-indent: 0pt;text-align: left;">Yes</p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 9pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="padding-left: 17pt;text-indent: 0pt;text-align: left;">Yes</p><p class="s63" style="padding-left: 223pt;text-indent: 10pt;text-align: left;">Yes (quadratic)</p><p class="s63" style="padding-left: 55pt;text-indent: 0pt;text-align: left;">generalization in natural language understanding and relational reasoning tasks</p><p class="s63" style="padding-top: 3pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">CLUTRR, CFQ and COGS  Yes</p><p style="padding-top: 3pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="padding-left: 11pt;text-indent: 0pt;line-height: 8pt;text-align: left;">LDC2015E86,</p><p class="s63" style="padding-left: 223pt;text-indent: 10pt;text-align: left;">Yes (quadratic)</p><p class="s63" style="padding-top: 4pt;padding-left: 23pt;text-indent: 0pt;text-align: left;">No   Graph-to-sequence learning</p><p class="s63" style="padding-left: 12pt;text-indent: 0pt;line-height: 8pt;text-align: left;">LDC2017T10, WMT16</p><p class="s63" style="padding-left: 12pt;text-indent: 0pt;text-align: left;">English-German, WMT16 English-Czech</p><p class="s63" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Yes</p><p class="s63" style="padding-top: 3pt;padding-left: 223pt;text-indent: 10pt;text-align: left;">Yes (quadratic)</p><p class="s63" style="padding-top: 3pt;padding-left: 14pt;text-indent: 8pt;text-align: left;">Yes (spectral)</p><p class="s63" style="padding-top: 3pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Graph representation learning</p><p class="s63" style="padding-left: 31pt;text-indent: 0pt;text-align: left;">ZINC, PATTERN, CLUSTER, MolHIV, MolPCBA</p><p class="s63" style="padding-top: 7pt;padding-left: 76pt;text-indent: 0pt;text-align: center;">Yes</p><p class="s63" style="padding-top: 3pt;padding-left: 223pt;text-indent: 10pt;text-align: left;">Yes (quadratic)</p><p class="s63" style="padding-left: 89pt;text-indent: 0pt;text-align: center;">Yes (message passing)</p><p class="s122" style="padding-top: 3pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">Time series analysis     <span class="s63">Synthetic, FordA, uWave,   </span>No</p><p class="s63" style="text-indent: 0pt;text-align: left;">IMDB, ECG</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 106pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a href="#bookmark166" class="s126">Johnson et al. [108],</a></p><p class="s63" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;text-align: left;"><a href="#bookmark145" class="s126">ABT-MPNN </a>[87]      No     No    <span class="s119">Yes (pre-</span></p><p class="s63" style="padding-left: 86pt;text-indent: 0pt;text-align: center;">Yes (quadratic and linear)</p><p class="s63" style="padding-left: 44pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Molecular property</p><p class="s63" style="padding-left: 44pt;text-indent: -31pt;text-align: left;">No   prediction and biological activity prediction</p><p class="s63" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">Tox21, ClinTox, ToxCast, HIV, QM8, ESOL,</p><p class="s63" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">FreeSolv, Lipophilicity and QM8.</p><p class="s63" style="text-indent: 0pt;text-align: left;">processing)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="text-indent: 0pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-left: 13pt;text-indent: 0pt;line-height: 8pt;text-align: left;">COLLAB, IMDB-B,</p><p class="s63" style="padding-top: 8pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">Yes</p><p class="s63" style="text-indent: 0pt;text-align: left;">(quadratic)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;text-align: left;"><a href="#bookmark151" class="s126">UGformer </a>[93]       No     No     No     <span class="s119">Yes</span></p><p class="s63" style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Graph classification and text classification</p><p class="s63" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">IMDB-M, DD, PROTEINS, MUTAG, PTC, MR, R8, R52,</p><p class="s63" style="text-indent: 0pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-left: 9pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Ohsumed</p><p class="s63" style="padding-top: 7pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Yes</p><table style="border-collapse:collapse;margin-left:3pt" cellspacing="0"><tr style="height:17pt"><td style="width:76pt"><p style="padding-top: 4pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark167" class="s121">DTI-GTN [109]</a></p></td><td style="width:44pt"><p class="s120" style="padding-top: 4pt;padding-left: 2pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:38pt"><p class="s120" style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: center;">Yes</p></td><td style="width:44pt"><p class="s120" style="padding-left: 14pt;padding-right: 2pt;text-indent: -3pt;line-height: 8pt;text-align: left;">Yes (inter- leaving)</p></td></tr><tr style="height:8pt"><td style="width:76pt"><p class="s120" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Dual-</p></td><td style="width:44pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:38pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:44pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:8pt"><td style="width:76pt"><p class="s120" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: left;">GCN+Transformer</p></td><td style="width:44pt"><p class="s120" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">No</p></td><td style="width:38pt"><p class="s120" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: center;">No</p></td><td style="width:44pt"><p class="s120" style="padding-left: 21pt;text-indent: 0pt;line-height: 7pt;text-align: left;">No</p></td></tr><tr style="height:9pt"><td style="width:76pt"><p style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: left;"><a href="#bookmark169" class="s121">[111]</a></p></td><td style="width:44pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:38pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:44pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-left: 223pt;text-indent: 10pt;text-align: left;">Yes (quadratic)</p><p class="s63" style="padding-left: 55pt;text-indent: 0pt;text-align: left;">Drug-target interaction prediction</p><p class="s63" style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><a href="#bookmark168" class="s126">Peng et al. </a>[110] dataset   No</p><p class="s63" style="padding-top: 7pt;padding-left: 26pt;text-indent: 0pt;line-height: 4pt;text-align: left;">MS COCO and Visual</p><p class="s63" style="padding-left: 222pt;text-indent: 0pt;text-align: left;">Yes (linear)    No   Image Captioning</p><p class="s63" style="padding-left: 40pt;text-indent: 0pt;text-align: left;">Genome         <span class="s119">Yes</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:7.964pt" cellspacing="0"><tr style="height:26pt"><td style="width:82pt"><p style="padding-top: 7pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark170" class="s121">SGTransformer [112]</a></p></td><td style="width:41pt"><p class="s120" style="padding-top: 7pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">Yes</p></td><td style="width:43pt"><p class="s120" style="padding-top: 7pt;text-indent: 0pt;text-align: center;">Yes</p></td><td style="width:346pt"><p class="s120" style="padding-top: 1pt;padding-left: 49pt;padding-right: 4pt;text-indent: -33pt;line-height: 33%;text-align: left;"><span class="s125">No     </span>Yes     <span class="s123">Yes   </span>Scene graph to image    <span class="s123">Visual Genome,      </span><span class="s125">Yes, (quadratic)   </span>(message  <span class="s125">generation         </span>COCO-Stuff and</p><p class="s120" style="padding-left: 98pt;text-indent: 0pt;line-height: 5pt;text-align: left;">passing)                CLEVR-Dialog</p></td></tr><tr style="height:17pt"><td style="width:82pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark171" class="s121">TPT [113]</a></p></td><td style="width:41pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s120" style="padding-top: 1pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">Yes</p></td><td style="width:43pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s120" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:346pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s125" style="padding-left: 16pt;text-indent: 0pt;line-height: 63%;text-align: left;">No     Yes      No    <span class="s120">Video scene graph      VidHOI and Action     </span>No</p><p class="s120" style="padding-left: 137pt;text-indent: 0pt;line-height: 6pt;text-align: left;">generation         Genome</p></td></tr></table><p style="padding-top: 4pt;text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">capacity of global attention.</p><p style="padding-top: 2pt;padding-left: 7pt;text-indent: 9pt;text-align: left;">Several scalable graph transformer models have been pro- posed to enhance the scalability and efficiency of graph</p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark98" class="a">transformers. For instance, Rampa´sˇek et al. </a>[39] introduced GPS, use low-rank matrix approximations to reduce com- putational complexity, and achieve state-of-the-art results on</p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark174" class="a">diverse benchmarks. GPS decouples local real-edge aggre- gation from a fully-connected transformer and incorporates different positional and structural encodings to capture graph topology. It also offers a modular framework that supports multiple encoding types and mechanisms for local and global attention. Cong et al. </a>[116] developed DyFormer, a dynamic graph transformer that utilizes substructure tokens and local attention to enhance the focus and diversity of global attention. DyFormer employs a temporal union graph structure and a subgraph-based node sampling strategy for efficient and scalable training.</p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark48" class="a">Scalable graph transformers are an innovative and efficient category of graph transformers that excel in handling large- scale graphs while minimizing computational cost and mem- ory usage. However, scalable graph transformers face certain limitations, including the trade-off between scalability and expressiveness, the challenge of selecting optimal hyperpa- rameters and encodings, and the absence of theoretical anal- ysis regarding their convergence and stability. Consequently, further investigation is required to explore optimal designs and evaluations of scalable graph transformers for various applications. For a comprehensive overview of scalable graph transformer methods, please refer to Table </a><a href="#bookmark48">IV.</a></p><p style="padding-top: 7pt;text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="D."><p class="s21" style="padding-left: 22pt;text-indent: -14pt;text-align: justify;"><a name="bookmark24">&zwnj;</a>Pre-trained Graph Transformers</p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark193" class="a">Pre-trained graph transformers utilize large-scale unlabeled graphs to acquire transferable node embeddings </a><a href="#bookmark194" class="a">[135]. These embeddings can be fine-tuned for downstream tasks with scarce labeled data that address the challenges of data scarcity and domain adaptation in graph learning tasks </a><a href="#bookmark195" class="a">[136], </a><a href="#bookmark196" class="a">[137]. These transformers are similar to pre-trained large language models (LLMs) and are trained on graph datasets using self- supervised learning objectives, such as masked node prediction </a><a href="#bookmark197" class="a">[138], edge reconstruction </a><a href="#bookmark198" class="a">[139], and graph contrastive learn- ing </a><a href="#bookmark199" class="a">[140]. These objectives aim to encapsulate the inherent properties of graph data independently of external labels or supervision </a><a href="#bookmark200" class="a">[141]. The pre-trained model can be fine-tuned on a specific downstream task with a smaller or domain- specific graph dataset by incorporating a task-specific layer or loss function and optimizing it on labeled data [142]. This allows the pre-trained model to transfer the knowledge acquired from the large-scale graph dataset to the subsequent task, giving better performance compared to the training from scratch </a><a href="#bookmark200">[142].</a></p><p style="padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark201" class="a">Pre-trained graph transformers face some challenges, such as the selection of appropriate pre-training tasks, domain knowledge incorporation, heterogeneous information integra- tion, and pre-training quality evaluation </a><a href="#bookmark202" class="a">[143]. To address these issues, KPGT </a><a href="#bookmark203" class="a">[144] and KGTransformer </a><a href="#bookmark49" class="a">[145] have been proposed. KPGT leverages additional domain knowledge for pre-training, while KGTransformer serves as a uniform Knowledge Representation and Fusion (KRF) module in di- verse tasks. Despite their power and flexibility, pre-trained graph transformers encounter issues related to graph data heterogeneity and sparsity, domain adaptation, model gener- alization and performance interpretation. A summary of pre- trained graph transformer methods is provided in Table </a><a href="#bookmark49">V.</a></p></li><li data-list-text="E."><p class="s21" style="padding-top: 4pt;padding-left: 21pt;text-indent: -13pt;text-align: justify;"><a name="bookmark25">&zwnj;</a>Design Guide for Effective Graph Transformers</p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;">Developing effective graph transformers requires meticulous attention to detail and careful consideration. This guide pro- vides general principles and tips for designing graph trans- formers for various scenarios and tasks.</p><ul id="l8"><li data-list-text="•"><p class="s21" style="padding-left: 27pt;text-indent: -10pt;text-align: justify;">Choose the appropriate type of graph transformers based on the nature and complexity of your graph data and tasks. <span class="p">For simple and small graph data, a shallow graph transformer with a few layers may suffice. For complex and large graph data, a deep graph transformer with many layers can learn more expressive representations. For dynamic or streaming graph data, a scalable graph transformer is more efficient. Pre-trained graph trans- formers are more suitable for sparse or noisy graph data.</span></p></li><li data-list-text="•"><p class="s21" style="padding-left: 27pt;text-indent: -10pt;text-align: justify;">Design suitable structural and positional encodings for your graph data. <span class="p">These encodings capture the structure of graphs and are added to input node or edge features before feeding them to transformer layers. The choice of encodings depends on the characteristics of the graph data, such as directionality, weight, and homogeneity. The careful design of these encodings ensures their informa- tiveness.</span></p></li><li data-list-text="•"><p class="s21" style="padding-left: 27pt;text-indent: -10pt;text-align: justify;">Optimize the self-attention mechanism for your graph data. <span class="p">Self-attention mechanisms compute attention scores among all pairs of nodes or edges in the graph, captur- ing long-range dependencies and interactions. However, it introduces challenges like computational complexity, memory consumption, overfitting, over-smoothing, and over-squashing. Techniques like sampling, sparsification, partitioning, hashing, masking, regularization, and nor- malization can be employed to address these challenges and improve the quality and efficiency of the self- attention mechanism.</span></p></li><li data-list-text="•"><p class="s21" style="padding-left: 27pt;text-indent: -10pt;text-align: justify;">Utilize pre-training techniques to enhance the perfor- mance of graph transformers. <span class="p">Pre-training techniques leverage pre-trained models or data from other domains or tasks transferring knowledge or parameters to a specific graph learning task. Methods like fine-tuning, distillation, and adaptation can be used to adapt pre-trained models or data. Utilizing pre-training techniques is particularly beneficial when a large amount of pre-training data or resources are available.</span></p></li></ul></li></ol></li><li data-list-text="V."><p style="padding-top: 6pt;padding-left: 98pt;text-indent: -57pt;text-align: left;"><a name="bookmark29">&zwnj;</a>Application Perspectives of Graph Transformers<a name="bookmark47">&zwnj;</a></p><p style="padding-top: 2pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;">Graph transformers are finding applications in various do- mains that involve interconnected data. This section delves into their applications for graph-related tasks, categorized by the level of analysis: node-level, edge-level and graph- level. Beyond these core tasks, graph transformers are also making strides in applications that handle text, images, and videos, where data can be effectively represented as graphs for analysis.</p><ol id="l9"><li data-list-text="A."><p class="s21" style="padding-top: 8pt;padding-left: 21pt;text-indent: -13pt;text-align: justify;"><a name="bookmark30">&zwnj;</a>Node-level Tasks</p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark215" class="a">Node-level tasks involve the acquisition of node represen- tations or the prediction of node attributes using the graph structure and node features </a><a href="#bookmark215">[153].</a></p><p class="s7" style="padding-top: 6pt;padding-left: 71pt;text-indent: 0pt;line-height: 9pt;text-align: center;"><a name="bookmark48">&zwnj;</a>TABLE IV</p><p class="s7" style="padding-left: 71pt;text-indent: 0pt;line-height: 9pt;text-align: center;">Scalable graph transformers</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 12pt;text-indent: 0pt;line-height: 1pt;text-align: left;"/><p style="padding-top: 5pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Model</p><p class="s13" style="padding-top: 1pt;padding-bottom: 2pt;padding-left: 30pt;text-indent: -17pt;text-align: left;">Graph inductive bias</p><p style="text-indent: 0pt;line-height: 1pt;text-align: left;"/><p class="s13" style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Graph attention</p><p style="text-indent: 0pt;text-align: left;"/><p class="s127" style="padding-left: 18pt;text-indent: 0pt;line-height: 11pt;text-align: left;">mechanisms      <span class="s13">Application            Datasets      Code</span></p><p class="s13" style="padding-left: 85pt;text-indent: 0pt;text-align: left;">Node Positional encoding</p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-top: 4pt;padding-left: 87pt;text-indent: 0pt;text-align: left;">Yes (local</p><p class="s13" style="text-indent: 0pt;text-align: right;"><a href="#bookmark98" class="s128">GPS </a>[39]        and</p><p class="s13" style="text-indent: 0pt;text-align: right;">global)</p><p class="s13" style="padding-left: 10pt;text-indent: 0pt;text-align: left;">Edge Structural Encoding</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 19pt;text-indent: -9pt;text-align: left;">Yes (local and relative)</p><p class="s13" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">Message- Passing Bias</p><p class="s13" style="padding-top: 4pt;padding-left: 84pt;text-indent: 0pt;text-align: center;">Yes (inter- leaving)</p><p class="s13" style="padding-top: 3pt;padding-left: 84pt;text-indent: 0pt;line-height: 4pt;text-align: center;">Yes (rela-</p><p class="s13" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Global attention</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 85pt;text-indent: 0pt;text-align: center;">Yes (quadratic and linear)</p><p class="s13" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">Local Attention</p><p style="padding-top: 4pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 23pt;text-indent: -7pt;text-align: left;">Yes (message passing)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">Graph representation learning</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">ZINC, PCQM4Mv2,</p><p class="s13" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">CIFAR10, MalNet-Tiny, OGB benchmarks</p><p class="s13" style="padding-left: 11pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Citation networks,</p><p class="s13" style="padding-left: 10pt;text-indent: 0pt;text-align: left;">availability</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 10pt;text-indent: 0pt;text-align: left;">Yes</p><p class="s13" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;text-align: left;"><a href="#bookmark112" class="s128">NodeFormer </a>[53]     No      No</p><p class="s13" style="padding-top: 4pt;padding-left: 14pt;text-indent: -1pt;text-align: left;">tional bias)</p><p class="s13" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Yes (linear)     No     Node classification</p><p style="padding-top: 3pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 118pt;text-indent: 0pt;text-align: left;">Node classification, image/text</p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">OGB-Proteins, Amazon2M, Mini-ImageNet,</p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;line-height: 7pt;text-align: left;">20News-Groups</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Cora, Citeseer, Pubmed,</p><p class="s13" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Yes</p><p class="s13" style="text-indent: 0pt;text-align: left;">Yes (global)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;"><a href="#bookmark173" class="s128">DIFFORMER </a>[115]    No      No      No    Yes (linear)   Yes (spectral)</p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">classification, spatial-temporal dynamics prediction</p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">ogbn-Proteins, Pokec, CIFAR, STL, 20News</p><p class="s13" style="padding-left: 10pt;text-indent: 0pt;text-align: left;">Yes</p><p class="s13" style="text-indent: 0pt;text-align: left;">(global)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;text-align: left;"><a href="#bookmark174" class="s128">DyFormer </a>[116]     <span class="s127">Yes</span></p><p class="s13" style="padding-top: 3pt;padding-left: 13pt;text-indent: 1pt;text-align: left;">Yes (pre- processing)</p><p class="s13" style="padding-top: 3pt;padding-left: 5pt;text-indent: 10pt;text-align: left;">Yes (quadratic)</p><p class="s13" style="padding-left: 53pt;text-indent: 0pt;text-align: left;">Dynamic graph</p><p class="s13" style="padding-left: 53pt;text-indent: -40pt;text-align: left;">No     representation learning</p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">RDS, UCI, Yelp, ML-10M,</p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">SNAP-Wikipedia, SNAP-Reddit</p><table style="border-collapse:collapse;margin-left:3pt" cellspacing="0"><tr style="height:27pt"><td style="width:75pt"><p style="padding-top: 4pt;padding-left: 1pt;padding-right: 9pt;text-indent: 0pt;text-align: left;"><a href="#bookmark175" class="s130">Graph ViT/MLP- Mixer [117]</a></p></td><td style="width:39pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s129" style="padding-left: 10pt;text-indent: 0pt;text-align: left;">Yes</p></td><td style="width:47pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s129" style="text-indent: 0pt;text-align: center;">No</p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s129" style="padding-left: 2pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:40pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s129" style="padding-left: 1pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:62pt"><p class="s129" style="padding-top: 4pt;padding-left: 21pt;text-indent: -7pt;text-align: left;">Yes (message passing)</p></td><td style="width:68pt"><p class="s129" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Graph representation learning</p></td></tr><tr style="height:16pt"><td style="width:75pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 1pt;text-indent: 0pt;line-height: 7pt;text-align: left;"><a href="#bookmark176" class="s130">DiGress [118]</a></p></td><td style="width:39pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s129" style="padding-left: 11pt;text-indent: 0pt;line-height: 7pt;text-align: left;">No</p></td><td style="width:47pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s129" style="text-indent: 0pt;line-height: 7pt;text-align: center;">Yes</p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s129" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">No</p></td><td style="width:40pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s129" style="padding-left: 1pt;text-indent: 0pt;line-height: 7pt;text-align: center;">Yes</p></td><td style="width:62pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s129" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: center;">Yes</p></td><td style="width:68pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s129" style="padding-left: 8pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Graph generation</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">ZINC, MNIST, CIFAR10, MolTOX21, MolHIV,</p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">Peptide-func, Peptide-struct, CSL, EXP, SR25, LRGB,</p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">TreeNeighbourMatch QM9, MOSES, GuacaMol, SBM, planar graphs CIFAR10, MalNet-Tiny,</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">Yes</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">Yes</p><p class="s13" style="padding-left: 87pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Yes (local</p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;"><a href="#bookmark177" class="s128">EXPHORMER </a>[119]   and</p><p class="s13" style="padding-left: 91pt;text-indent: 0pt;text-align: left;">global)</p><p class="s13" style="padding-top: 3pt;padding-left: 21pt;text-indent: -10pt;text-align: left;">Yes (local and global)</p><p class="s13" style="padding-left: 87pt;text-indent: 0pt;text-align: center;">Yes (inter- leaving)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">Yes (linear)  Yes (spectral)</p><p class="s13" style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Graph learning and representation tasks</p><p style="padding-top: 3pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Node and edge</p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">MNIST, CLUSTER, PATTERN, PascalVOC-SP, COCO-SP, PCQM-Contact,</p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;line-height: 7pt;text-align: left;">ogbn-arxiv,</p><p class="s13" style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;line-height: 4pt;text-align: left;">PATTERN, CLUSTER, TSP,</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">Yes</p><p style="text-indent: 0pt;text-align: left;"><a href="#bookmark178" class="s128">EGT [120]</a></p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-top: 4pt;padding-left: 90pt;text-indent: 5pt;text-align: left;">Yes (global)</p><p class="s13" style="padding-top: 3pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">Yes (global)    No    <span class="s127">Yes</span></p><p class="s13" style="padding-left: 82pt;text-indent: 0pt;text-align: left;">classification, graph</p><p class="s13" style="text-indent: 0pt;text-align: left;">(quadratic)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-left: 82pt;text-indent: -40pt;text-align: left;">No     classification and regression, transfer learning.</p><p class="s13" style="padding-top: 4pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">MNIST, CIFAR10, ZINC, PCQM4M, PCQM4Mv2,</p><p class="s13" style="padding-left: 14pt;text-indent: 0pt;line-height: 7pt;text-align: left;">MolPCBA and MolHIV.</p><p class="s13" style="padding-top: 3pt;padding-left: 14pt;text-indent: 0pt;line-height: 7pt;text-align: left;">ZINC, MNIST, CIFAR10,</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 70pt;text-indent: 0pt;text-align: center;">Yes</p><p class="s13" style="text-indent: 0pt;text-align: left;">(quadratic)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;"><a href="#bookmark179" class="s128">GRIT </a>[121]        Yes      No      No     <span class="s127">Yes</span></p><p style="padding-top: 4pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="text-indent: 0pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-left: 134pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Yes (local,</p><p class="s13" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">Graph regression and classification</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="text-indent: 0pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-left: 13pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Yes (message</p><p class="s13" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">PATTERN, CLUSTER,</p><p class="s13" style="padding-left: 9pt;text-indent: 0pt;text-align: left;">Peptides-func, Peptides-struct, ZINC-full, PCQM4Mv2</p><p class="s13" style="text-indent: 0pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-left: 9pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Cora, CiteSeer, PubMed,</p><p class="s13" style="padding-top: 4pt;padding-left: 10pt;text-indent: 0pt;text-align: left;">Yes</p><p class="s13" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;text-align: left;"><a href="#bookmark180" class="s128">HSGT </a>[122]       No</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;line-height: 0pt;text-align: left;">Higher-Order Trans-</p><p class="s13" style="padding-left: 86pt;text-indent: 0pt;text-align: center;">based on shortest path distance)</p><p class="s13" style="padding-left: 88pt;text-indent: 0pt;text-align: center;">Yes (quadratic)</p><p style="padding-top: 3pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 88pt;text-indent: 0pt;line-height: 4pt;text-align: center;">Yes</p><p class="s13" style="padding-left: 85pt;text-indent: 0pt;text-align: center;">passing, based on neighborhood sampling)</p><p class="s13" style="padding-top: 7pt;padding-left: 85pt;text-indent: 0pt;line-height: 0pt;text-align: center;">Yes (message</p><p class="s13" style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: left;">Node classification</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 9pt;text-indent: 0pt;line-height: 0pt;text-align: left;">Graph and</p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">Amazon-Photo, ogbn-arxiv, ogbn-proteins, ogbn-products, Reddit, Flickr, Yelp</p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Synthetic chains,</p><p class="s13" style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;text-align: left;"><a href="#bookmark181" class="s128">formers </a>[123]       <span class="s127">No      No      No</span></p><p style="padding-top: 4pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="text-indent: 0pt;line-height: 4pt;text-align: right;">Yes</p><p class="s13" style="padding-top: 4pt;padding-left: 13pt;text-indent: 1pt;text-align: left;">(quadratic and linear)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">passing)</p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;line-height: 15pt;text-align: left;">hypergraph modeling Graph representation</p><p class="s13" style="padding-left: 10pt;text-indent: 0pt;text-align: left;">PCQM4M-LSC, Jets,</p><p class="s13" style="padding-left: 10pt;text-indent: 0pt;text-align: left;">Delaunay, Hyperedge prediction</p><p class="s13" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Yes</p><p class="s13" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;text-align: left;"><a href="#bookmark132" class="s128">EGAT </a>[73]        No      No</p><p class="s13" style="padding-top: 4pt;padding-left: 13pt;text-indent: 2pt;text-align: left;">(inter- leaving)</p><p class="s13" style="padding-top: 4pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Yes (linear)    No</p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">learning and molecular property prediction</p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">AMLSim, Cora, Citeseer, PubMed</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="text-indent: 0pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-left: 13pt;text-indent: 0pt;line-height: 7pt;text-align: left;">CSBM, Amazon, Facebook,</p><p class="s13" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;text-align: left;"><a href="#bookmark148" class="s128">L-CAT </a>[90]        No      No      No    Yes (linear)     No     Node classification</p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">GitHub, TwitchEN, Coauthor Physics, OGB (arxiv, products, mag, proteins)</p><p class="s13" style="padding-top: 4pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">Yes</p><p class="s13" style="text-indent: 0pt;text-align: left;">No     No     No</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 13pt;text-indent: 0pt;text-align: left;"><a href="#bookmark146" class="s128">Point  Transformer [88]</a></p><p class="s13" style="padding-left: 11pt;text-indent: 7pt;text-align: left;">Yes (relative)</p><p class="s13" style="padding-left: 20pt;text-indent: -7pt;text-align: left;">Yes (message passing)</p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">3D point cloud processing</p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">S3DIS, ModelNet40, ShapeNetPart</p><p class="s13" style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Yes</p><p class="s13" style="text-indent: 0pt;text-align: left;">learning</p><p style="text-indent: 0pt;text-align: left;"/><table style="border-collapse:collapse;margin-left:3pt" cellspacing="0"><tr style="height:45pt"><td style="width:67pt"><p style="padding-left: 1pt;text-indent: 0pt;text-align: left;"><a href="#bookmark150" class="s130">Specformer [92]</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 1pt;text-indent: 0pt;text-align: left;"><a href="#bookmark182" class="s130">HierGAT [124]</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 1pt;text-indent: 0pt;text-align: left;"><a href="#bookmark183" class="s130">Geo-ER [125]</a></p></td><td style="width:53pt"><p class="s129" style="padding-left: 7pt;padding-right: 13pt;text-indent: 11pt;line-height: 204%;text-align: left;">No Yes (local)</p><p class="s129" style="padding-right: 6pt;text-indent: 0pt;line-height: 7pt;text-align: center;">No</p></td><td style="width:41pt"><p class="s129" style="padding-left: 11pt;padding-right: 16pt;text-indent: 0pt;line-height: 204%;text-align: center;">No No</p><p class="s129" style="padding-left: 11pt;padding-right: 16pt;text-indent: 0pt;line-height: 7pt;text-align: center;">No</p></td><td style="width:38pt"><p class="s129" style="padding-left: 18pt;padding-right: 10pt;text-indent: 0pt;line-height: 204%;text-align: left;">No No</p><p class="s129" style="padding-left: 18pt;text-indent: 0pt;line-height: 7pt;text-align: left;">No</p></td><td style="width:45pt"><p class="s129" style="padding-left: 10pt;text-indent: 0pt;line-height: 204%;text-align: left;">Yes (linear) Yes (linear)</p><p class="s129" style="padding-left: 10pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Yes (linear)</p></td></tr><tr style="height:19pt"><td style="width:67pt"><p style="padding-top: 5pt;padding-left: 1pt;text-indent: 0pt;text-align: left;"><a href="#bookmark184" class="s130">GTA [126]</a></p></td><td style="width:53pt"><p class="s129" style="padding-top: 5pt;padding-right: 6pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:41pt"><p class="s129" style="padding-top: 5pt;padding-left: 11pt;padding-right: 16pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:38pt"><p class="s129" style="padding-top: 5pt;padding-right: 10pt;text-indent: 0pt;text-align: right;">No</p></td><td style="width:45pt"><p class="s129" style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;text-align: center;">Yes (linear)</p></td></tr><tr style="height:16pt"><td style="width:67pt"><p style="padding-top: 5pt;padding-left: 1pt;text-indent: 0pt;text-align: left;"><a href="#bookmark185" class="s130">GraTransDRP [127]</a></p></td><td style="width:53pt"><p class="s129" style="padding-top: 5pt;padding-right: 6pt;text-indent: 0pt;text-align: center;">Yes</p></td><td style="width:41pt"><p class="s129" style="padding-top: 5pt;padding-left: 11pt;padding-right: 16pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:38pt"><p class="s129" style="padding-top: 5pt;padding-right: 10pt;text-indent: 0pt;text-align: right;">No</p></td><td style="width:45pt"><p class="s129" style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;text-align: center;">Yes (linear)</p></td></tr><tr style="height:18pt"><td style="width:67pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 1pt;text-indent: 0pt;line-height: 7pt;text-align: left;"><a href="#bookmark186" class="s130">LGT [128]</a></p></td><td style="width:53pt"><p class="s129" style="padding-top: 1pt;padding-left: 18pt;padding-right: 13pt;text-indent: -9pt;line-height: 8pt;text-align: left;">Yes (local and</p></td><td style="width:41pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s129" style="padding-left: 11pt;padding-right: 16pt;text-indent: 0pt;line-height: 7pt;text-align: center;">No</p></td><td style="width:38pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s129" style="padding-right: 10pt;text-indent: 0pt;line-height: 7pt;text-align: right;">No</p></td><td style="width:45pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s129" style="padding-left: 8pt;text-indent: 0pt;line-height: 7pt;text-align: center;">Yes (linear)</p></td></tr><tr style="height:24pt"><td style="width:67pt"><p style="padding-top: 3pt;text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 1pt;text-indent: 0pt;text-align: left;"><a href="#bookmark187" class="s130">GTAGC [129]</a></p></td><td style="width:53pt"><p class="s129" style="padding-right: 6pt;text-indent: 0pt;line-height: 7pt;text-align: center;">global)</p><p class="s129" style="padding-left: 9pt;padding-right: 15pt;text-indent: 0pt;line-height: 8pt;text-align: center;">Yes (global)</p></td><td style="width:41pt"><p style="padding-top: 3pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s129" style="padding-left: 11pt;padding-right: 16pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:38pt"><p style="padding-top: 3pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s129" style="padding-right: 10pt;text-indent: 0pt;text-align: right;">No</p></td><td style="width:45pt"><p style="padding-top: 3pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s129" style="padding-left: 8pt;text-indent: 0pt;text-align: center;">Yes (linear)</p></td></tr><tr style="height:15pt"><td style="width:67pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 1pt;text-indent: 0pt;line-height: 7pt;text-align: left;"><a href="#bookmark188" class="s130">ANS-GT [130]</a></p></td><td style="width:53pt"><p class="s129" style="padding-left: 18pt;padding-right: 13pt;text-indent: -9pt;line-height: 8pt;text-align: left;">Yes (local and</p></td><td style="width:41pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s129" style="padding-left: 11pt;padding-right: 16pt;text-indent: 0pt;line-height: 7pt;text-align: center;">No</p></td><td style="width:38pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s129" style="padding-right: 10pt;text-indent: 0pt;line-height: 7pt;text-align: right;">No</p></td><td style="width:45pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s129" style="padding-left: 8pt;text-indent: 0pt;line-height: 7pt;text-align: center;">Yes (linear)</p></td></tr><tr style="height:10pt"><td style="width:67pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:53pt"><p class="s129" style="padding-right: 6pt;text-indent: 0pt;line-height: 7pt;text-align: center;">global)</p></td><td style="width:41pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:38pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:45pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:10pt"><td style="width:67pt"><p style="padding-top: 1pt;padding-left: 1pt;text-indent: 0pt;line-height: 7pt;text-align: left;"><a href="#bookmark189" class="s130">RelTR [131]</a></p></td><td style="width:53pt"><p class="s129" style="padding-top: 1pt;padding-right: 6pt;text-indent: 0pt;line-height: 7pt;text-align: center;">No</p></td><td style="width:41pt"><p class="s129" style="padding-top: 1pt;padding-left: 11pt;padding-right: 16pt;text-indent: 0pt;line-height: 7pt;text-align: center;">No</p></td><td style="width:38pt"><p class="s129" style="padding-top: 1pt;padding-right: 10pt;text-indent: 0pt;line-height: 7pt;text-align: right;">No</p></td><td style="width:45pt"><p class="s129" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 7pt;text-align: center;">Yes (linear)</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"/><p class="s131" style="text-indent: 0pt;text-align: right;">Yes (spectral)  <span class="s13">Graph representation</span></p><p class="s13" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">Synthetic, node-level and graph-level datasets</p><p class="s13" style="padding-top: 3pt;padding-left: 73pt;text-indent: 0pt;text-align: center;">Yes</p><p class="s13" style="text-indent: 0pt;text-align: left;">matching datasets</p><p style="text-indent: 0pt;text-align: left;"/><p class="s131" style="padding-left: 285pt;text-indent: 0pt;text-align: left;">No     Entity Resolution    <span class="s13">Magellan and WDC product</span></p><p class="s13" style="padding-top: 3pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Yes</p><p class="s13" style="padding-top: 3pt;padding-left: 277pt;text-indent: -7pt;text-align: left;">Yes (message passing)</p><p class="s13" style="padding-top: 3pt;padding-left: 277pt;text-indent: -7pt;text-align: left;">Yes (message passing)</p><p class="s13" style="padding-top: 3pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Geospatial Entity Resolution Multivariate time series anomaly detection in IoT</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 21pt;text-indent: 0pt;text-align: left;">OSM, FSQ, Yelp       Yes</p><p style="padding-top: 4pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 21pt;text-indent: 0pt;text-align: left;">SWaT, WADI, SMAP, MSL  Yes</p><p class="s13" style="text-indent: 0pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-left: 325pt;text-indent: 0pt;text-align: left;">Drug response prediction</p><p class="s13" style="padding-top: 4pt;padding-left: 29pt;text-indent: 0pt;text-align: left;">GDSC            Yes</p><p class="s13" style="padding-top: 3pt;padding-left: 277pt;text-indent: -7pt;text-align: left;">Yes (message passing)</p><p class="s127" style="padding-top: 3pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">Molecular properties  <span class="s13">ZINC, QM9, ESOL, FreeSolv  No,</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="text-indent: 0pt;text-align: left;">prediction</p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-left: 269pt;text-indent: 0pt;text-align: left;">Yes (spectral)   Graph clustering    Citeseer, Cora, Pubmed     No</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="text-indent: 0pt;text-align: left;">passing)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s127" style="text-indent: 0pt;text-align: right;">Yes (message  <span class="s13">Node classification</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="text-indent: 0pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-left: 325pt;text-indent: 0pt;text-align: left;">Scene graph generation</p><p class="s13" style="padding-top: 3pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">Cora, Citeseer, Pubmed, Chameleon, Actor, Squirrel, Cornell, Texas, Wisconsin Visual Genome, Open Images V6, VRD</p><p class="s13" style="padding-left: 10pt;text-indent: 0pt;line-height: 19pt;text-align: left;">No Yes</p><p class="s13" style="text-indent: 0pt;text-align: left;">global)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="text-indent: 0pt;text-align: left;">Yes</p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-left: 87pt;text-indent: 0pt;text-align: left;">Yes (local</p><p class="s13" style="padding-left: 91pt;text-indent: -78pt;text-align: left;"><a href="#bookmark190" class="s128">BSTG-Trans </a>[132]    and global)</p><p class="s127" style="padding-top: 3pt;padding-left: 10pt;text-indent: 0pt;text-align: left;">Yes (local and   <span class="s13">No   Yes (linear)</span></p><p class="s13" style="padding-top: 4pt;padding-left: 20pt;text-indent: -7pt;text-align: left;">Yes (message Passing)</p><p class="s13" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Long-term Pose Forecasting</p><p class="s13" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Human3.6M, HumanEva-I, Human360K</p><p class="s13" style="padding-left: 87pt;text-indent: 0pt;text-align: left;">Yes (local</p><p class="s13" style="text-indent: 0pt;text-align: left;">No    No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="text-indent: 0pt;text-align: left;">Yes (linear)   No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s13" style="padding-left: 91pt;text-indent: -78pt;text-align: left;"><a href="#bookmark191" class="s128">GraphSum </a>[133]     and global)</p><p class="s13" style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">No      No   Yes (linear)   <span class="s127">Yes (message</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">Abstractive MDS    WikiSum and MultiNews    Yes</p><p class="s13" style="text-indent: 0pt;text-align: left;">passing)</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;text-align: left;"><a href="#bookmark192" class="s128">KG-Transformer [134]</a></p><p class="s13" style="padding-top: 3pt;padding-left: 13pt;text-indent: 1pt;text-align: left;">Yes (pre- processing)</p><p class="s13" style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">Text generation from knowledge graphs</p><p class="s13" style="padding-left: 10pt;text-indent: 0pt;text-align: left;">UMLS, PubMed, 2010</p><p class="s13" style="padding-left: 10pt;text-indent: 0pt;text-align: left;">i2b2/VA, JNLPBA, BC5CDR,  No GAD, EU-ADR</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 1pt;text-align: left;"/><p class="s7" style="padding-top: 6pt;padding-left: 71pt;text-indent: 0pt;line-height: 9pt;text-align: center;"><a name="bookmark49">&zwnj;</a>TABLE V</p><p class="s7" style="padding-left: 71pt;text-indent: 0pt;line-height: 9pt;text-align: center;">Pre-trained graph transformers</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 1pt;text-align: left;"/><p style="padding-top: 4pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="padding-left: 13pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Model</p><p class="s63" style="padding-top: 1pt;padding-left: 30pt;text-indent: -17pt;text-align: left;">Graph inductive bias</p><p class="s63" style="padding-left: 13pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Graph attention</p><p style="text-indent: 0pt;text-align: left;"/><p class="s119" style="padding-left: 18pt;text-indent: 0pt;line-height: 12pt;text-align: left;">mechanisms    <span class="s63">Application           Datasets      Code</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-left: 96pt;text-indent: 0pt;text-align: left;">Node Positional encoding</p><p class="s63" style="padding-top: 4pt;padding-left: 14pt;text-indent: 0pt;text-align: left;">Edge Structural Encoding</p><p class="s63" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Message- Passing Bias</p><p class="s63" style="padding-top: 4pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">Global at- tention</p><p class="s63" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Local Attention</p><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: right;">availability</p><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: left;">processing)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;text-align: left;"><a href="#bookmark194" class="s126">GRAPHIX-T5 </a>[136]      No      Yes     <span class="s119">Yes (pre-</span></p><p class="s63" style="padding-top: 4pt;padding-left: 13pt;text-indent: 5pt;text-align: left;">Yes (linear)</p><p class="s63" style="padding-top: 4pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">SPIDER, SYN, DK, REALISTIC, SPIDER-SSP</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">Yes</p><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: left;">No  Text-to-SQL</p><p style="text-indent: 0pt;text-align: left;"/><table style="border-collapse:collapse;margin-left:3pt" cellspacing="0"><tr style="height:24pt"><td style="width:179pt" colspan="3"><p class="s120" style="padding-top: 7pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark195" class="s121">GROVER </a>[137]       No      No</p></td><td style="width:37pt"><p class="s120" style="padding-top: 3pt;padding-left: 12pt;text-indent: -3pt;text-align: left;">Yes (inter leaving)</p></td></tr><tr style="height:16pt"><td style="width:72pt"><p style="padding-top: 3pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark193" class="s121">G-BERT [135]</a></p></td><td style="width:57pt"><p class="s120" style="padding-top: 3pt;padding-left: 4pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:50pt"><p class="s120" style="padding-top: 3pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:37pt"><p class="s120" style="padding-top: 3pt;padding-right: 9pt;text-indent: 0pt;text-align: right;">No</p></td></tr><tr style="height:18pt"><td style="width:72pt"><p style="padding-top: 3pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark196" class="s121">ChemBERTa [138]</a></p></td><td style="width:57pt"><p class="s120" style="padding-top: 3pt;padding-left: 4pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:50pt"><p class="s120" style="padding-top: 3pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:37pt"><p class="s120" style="padding-top: 3pt;padding-right: 9pt;text-indent: 0pt;text-align: right;">No</p></td></tr><tr style="height:22pt"><td style="width:72pt"><p style="padding-top: 5pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark197" class="s121">HEAT [139]</a></p></td><td style="width:57pt"><p class="s120" style="padding-top: 5pt;padding-left: 4pt;text-indent: 0pt;text-align: center;">Yes (local)</p></td><td style="width:50pt"><p class="s120" style="padding-top: 5pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:37pt"><p class="s120" style="padding-top: 5pt;padding-right: 9pt;text-indent: 0pt;text-align: right;">No</p></td></tr><tr style="height:22pt"><td style="width:72pt"><p style="padding-top: 7pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark198" class="s121">FormNetV2 [140]</a></p></td><td style="width:57pt"><p class="s120" style="padding-top: 7pt;padding-left: 4pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:50pt"><p class="s120" style="padding-top: 7pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:37pt"><p class="s120" style="padding-top: 7pt;padding-right: 9pt;text-indent: 0pt;text-align: right;">No</p></td></tr><tr style="height:16pt"><td style="width:72pt"><p style="padding-top: 5pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark201" class="s121">MPG [143]</a></p></td><td style="width:57pt"><p class="s120" style="padding-top: 5pt;padding-left: 4pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:50pt"><p class="s120" style="padding-top: 5pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:37pt"><p class="s120" style="padding-top: 5pt;padding-right: 9pt;text-indent: 0pt;text-align: right;">No</p></td></tr><tr style="height:46pt"><td style="width:72pt"><p style="padding-top: 5pt;text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark202" class="s121">LiGhT [144]</a></p><p style="padding-top: 7pt;text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a href="#bookmark204" class="s121">LightGT [146]</a></p></td><td style="width:57pt"><p class="s120" style="padding-top: 1pt;padding-left: 22pt;padding-right: 18pt;text-indent: 0pt;line-height: 20pt;text-align: center;">No Yes</p></td><td style="width:50pt"><p class="s120" style="padding-top: 1pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">Yes (path encoding and distance encoding)</p><p class="s120" style="padding-top: 3pt;padding-right: 1pt;text-indent: 0pt;line-height: 8pt;text-align: center;">No</p></td><td style="width:37pt"><p class="s120" style="padding-left: 19pt;padding-right: 9pt;text-indent: 0pt;line-height: 24pt;text-align: left;">No No</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-top: 3pt;padding-left: 225pt;text-indent: 0pt;line-height: 8pt;text-align: left;">-  Yes</p><p class="s63" style="padding-left: 242pt;text-indent: 0pt;line-height: 8pt;text-align: left;">(linear)</p><p class="s63" style="padding-left: 91pt;text-indent: 0pt;text-align: center;">Yes (message passing)</p><p class="s63" style="padding-left: 12pt;text-indent: 0pt;text-align: left;">Molecular representation learning</p><p class="s63" style="padding-top: 7pt;padding-left: 31pt;text-indent: 0pt;text-align: left;">MoleculeNet         No</p><p class="s63" style="padding-left: 311pt;text-indent: 0pt;text-align: center;">Yes (linear)</p><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-left: 238pt;text-indent: -2pt;text-align: center;">Yes (quadratic)</p><p class="s63" style="padding-left: 51pt;text-indent: 0pt;text-align: left;">Medication recommendation Molecular property prediction</p><p class="s63" style="padding-top: 3pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">MIMIC-III          Yes</p><p class="s63" style="padding-top: 7pt;padding-left: 15pt;text-indent: 0pt;text-align: left;">PubChem 77M, MoleculeNet  Yes</p><p class="s63" style="padding-top: 3pt;padding-left: 242pt;text-indent: 5pt;text-align: left;">Yes (linear)</p><p class="s63" style="padding-left: 91pt;text-indent: 0pt;text-align: center;">Yes (message passing)</p><p class="s63" style="padding-top: 3pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">Structured reconstruction</p><p class="s63" style="padding-top: 3pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">SpaceNet Challenge and Structured3D</p><p class="s63" style="padding-top: 7pt;padding-left: 79pt;text-indent: 0pt;text-align: center;">Yes</p><p class="s63" style="padding-top: 3pt;padding-left: 242pt;text-indent: 5pt;text-align: left;">Yes (linear)</p><p class="s63" style="padding-left: 91pt;text-indent: 0pt;text-align: center;">Yes (message passing)</p><p class="s63" style="padding-left: 12pt;text-indent: 0pt;text-align: left;">Form document information extraction</p><p class="s63" style="padding-top: 7pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">FUNSD, CORD, SROIE    No</p><p class="s63" style="padding-left: 242pt;text-indent: 5pt;text-align: left;">Yes (linear)</p><p class="s63" style="padding-left: 16pt;text-indent: 8pt;text-align: left;">Yes (spectral)</p><p class="s63" style="padding-top: 3pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">Drug discovery    MoleculeNet, DDI, DTI     Yes</p><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-top: 3pt;padding-left: 85pt;text-indent: 0pt;line-height: 8pt;text-align: left;">ChEMBL29, BACE, BBBP,</p><p class="s63" style="padding-left: 242pt;text-indent: 5pt;text-align: left;">Yes (linear)</p><p class="s63" style="padding-left: 57pt;text-indent: 0pt;text-align: left;">Molecular property prediction</p><p class="s63" style="padding-left: 15pt;text-indent: 0pt;text-align: left;">ClinTox, SIDER, Estrogen, MetStab, Tox21, ToxCast, FreeSolv, Lipophilicity</p><p class="s63" style="padding-top: 3pt;padding-left: 71pt;text-indent: 0pt;text-align: center;">Yes</p><table style="border-collapse:collapse;margin-left:3pt" cellspacing="0"><tr style="height:32pt"><td style="width:78pt"><p style="padding-top: 7pt;text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark203" class="s121">KGTransformer [145]</a></p></td><td style="width:48pt"><p class="s120" style="padding-left: 1pt;text-indent: 0pt;line-height: 8pt;text-align: center;">(global)</p><p class="s120" style="padding-top: 7pt;padding-left: 1pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:57pt"><p style="padding-top: 7pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s120" style="padding-right: 2pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:26pt"><p style="padding-top: 7pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s120" style="padding-right: 2pt;text-indent: 0pt;text-align: right;">No</p></td></tr><tr style="height:26pt"><td style="width:78pt"><p style="padding-top: 7pt;padding-left: 2pt;text-indent: 0pt;text-align: left;"><a href="#bookmark205" class="s121">CoVGT [147]</a></p></td><td style="width:48pt"><p class="s120" style="padding-top: 7pt;padding-right: 18pt;text-indent: 0pt;text-align: right;">No</p></td><td style="width:57pt"><p class="s120" style="padding-top: 7pt;padding-right: 2pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:26pt"><p class="s120" style="padding-top: 7pt;padding-right: 2pt;text-indent: 0pt;text-align: right;">No</p></td></tr><tr style="height:18pt"><td style="width:78pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a href="#bookmark206" class="s121">G-Adapter [148]</a></p></td><td style="width:48pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s120" style="padding-right: 18pt;text-indent: 0pt;line-height: 8pt;text-align: right;">No</p></td><td style="width:57pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s120" style="padding-right: 2pt;text-indent: 0pt;line-height: 8pt;text-align: center;">Yes (local)</p></td><td style="width:26pt"><p style="padding-top: 1pt;text-indent: 0pt;text-align: left;"><br/></p><p class="s120" style="padding-right: 2pt;text-indent: 0pt;line-height: 8pt;text-align: right;">No</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-left: 242pt;text-indent: 5pt;text-align: left;">Yes (linear)</p><p class="s63" style="padding-left: 57pt;text-indent: 0pt;text-align: left;">Multimedia recommendation</p><p class="s63" style="padding-top: 3pt;padding-left: 23pt;text-indent: 0pt;text-align: left;">Movielens, Tiktok, Kwai    Yes</p><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-top: 3pt;padding-left: 242pt;text-indent: 5pt;text-align: left;">Yes (linear)</p><p class="s63" style="padding-top: 7pt;padding-left: 242pt;text-indent: 5pt;text-align: left;">Yes (linear)</p><p class="s63" style="padding-left: 90pt;text-indent: 0pt;text-align: center;">Yes (message passing) Yes (message- passing)</p><p class="s63" style="padding-top: 3pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Knowledge graph transfer</p><p class="s63" style="padding-top: 7pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Video Question Answering</p><p class="s63" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;line-height: 8pt;text-align: left;">WFC, WN18RR, AwA-KG,</p><p class="s63" style="padding-left: 20pt;text-indent: 0pt;line-height: 8pt;text-align: left;">CommonsenQA</p><p class="s63" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">NExT-QA, TGIF-QA, TGIF-QA-R, STAR-QA,</p><p class="s63" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">Causal-VidQA, MSRVTT-QA MolHIV, MolPCBA, FreeSolv,</p><p class="s63" style="padding-top: 7pt;padding-left: 12pt;text-indent: 0pt;line-height: 293%;text-align: left;">Yes Yes</p><p class="s63" style="padding-left: 238pt;text-indent: 9pt;text-align: left;">Yes (quadratic)</p><p class="s63" style="padding-left: 51pt;text-indent: 0pt;text-align: left;">Molecular graph tasks</p><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: left;">No</p><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-top: 7pt;padding-left: 51pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Knowledge graph</p><p class="s63" style="padding-left: 20pt;text-indent: 0pt;line-height: 87%;text-align: left;">ESOL, BBBP, Estrogen-<span class="s35">α</span>, Estrogen-<span class="s35">β</span>, MetStablow, MetStabhigh</p><p class="s63" style="padding-top: 3pt;padding-left: 73pt;text-indent: 0pt;text-align: center;">Yes</p><p class="s63" style="text-indent: 0pt;line-height: 8pt;text-align: left;">leaving)</p><p style="text-indent: 0pt;text-align: left;"/><table style="border-collapse:collapse;margin-left:3pt" cellspacing="0"><tr style="height:60pt"><td style="width:136pt"><p class="s120" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark207" class="s121">Li et al. </a>[149]     <span class="s123">Yes (local</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s120" style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark209" class="s121">Pellegrini et al. </a>[150]    No</p></td><td style="width:44pt"><p class="s120" style="padding-top: 7pt;padding-right: 4pt;text-indent: 0pt;text-align: center;">No</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s120" style="padding-right: 4pt;text-indent: 0pt;text-align: center;">Yes</p></td><td style="width:332pt"><p class="s120" style="padding-left: 20pt;text-indent: 0pt;line-height: 12pt;text-align: left;">No     <span class="s123">Yes     </span><span class="s132">Yes    Movement synchrony  </span>PT13 and Human3.6MS     No</p><p class="s125" style="padding-left: 54pt;text-indent: 0pt;line-height: 38%;text-align: left;">(linear)  <span class="s120">(message  estimation in autism</span></p><p class="s120" style="padding-right: 42pt;text-indent: 0pt;line-height: 7pt;text-align: center;">passing)  therapy interventions</p><p class="s120" style="padding-left: 134pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Disease prediction</p><p class="s120" style="padding-left: 54pt;padding-right: 9pt;text-indent: -34pt;line-height: 49%;text-align: left;">No    <span class="s123">Yes     </span>No   <span class="s123">using graph     </span>TADPOLE and MIMIC-III    Yes (linear)       convolutional</p><p class="s120" style="padding-left: 134pt;text-indent: 0pt;text-align: left;">networks</p></td></tr><tr style="height:22pt"><td style="width:136pt"><p class="s120" style="padding-left: 5pt;text-indent: 0pt;line-height: 94%;text-align: left;">Video Graph Transformer  <span class="s125">No</span></p></td><td style="width:44pt"><p class="s120" style="padding-top: 3pt;padding-right: 4pt;text-indent: 0pt;text-align: center;">No</p></td><td style="width:332pt"><p class="s120" style="padding-left: 20pt;text-indent: 0pt;line-height: 26%;text-align: left;"><span class="s125">No     </span>Yes     <span class="s123">Yes    Video Question    </span>NExT-QA, TGIF-QA,      <span class="s125">Yes</span></p><p class="s125" style="padding-left: 54pt;text-indent: 0pt;line-height: 38%;text-align: left;">(linear)   <span class="s120">(message  Answering      </span>MSRVTT-QA</p><p class="s120" style="padding-left: 94pt;text-indent: 0pt;line-height: 7pt;text-align: left;">passing)  (VideoQA)</p></td></tr><tr style="height:13pt"><td style="width:136pt"><p class="s120" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;"><a href="#bookmark213" class="s121">Tan et al. </a>[152]      No</p></td><td style="width:44pt"><p class="s120" style="padding-right: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">No</p></td><td style="width:332pt"><p class="s125" style="padding-left: 20pt;text-indent: 0pt;line-height: 21%;text-align: left;">No     <span class="s120">Yes     </span>No    <span class="s120">Few-shot node     CoraFull, ogbn-arxiv, CiteSeer,  </span>No</p><p class="s120" style="padding-left: 54pt;text-indent: 0pt;text-align: left;">(linear)         classification     Cora</p></td></tr><tr style="height:23pt"><td style="width:136pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s120" style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark119" class="s121">GPT-GNN </a>[60]      No</p></td><td style="width:44pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s120" style="padding-top: 4pt;padding-right: 4pt;text-indent: 0pt;text-align: center;">Yes</p></td><td style="width:332pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s123" style="padding-left: 134pt;text-indent: 0pt;line-height: 7pt;text-align: left;">Graph representation  <span class="s120">OAG, Amazon, Reddit, OAG</span></p><p class="s120" style="padding-left: 19pt;text-indent: 0pt;line-height: 61%;text-align: left;">Yes     No     Yes    learning and graph   <span class="s125">(citation)           </span>Yes</p><p class="s120" style="padding-left: 134pt;text-indent: 0pt;line-height: 6pt;text-align: left;">mining</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"/><p class="s63" style="padding-left: 13pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a href="#bookmark207" class="s126">kgTransformer </a>[149]     No      No     <span class="s119">Yes (inter-</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s63" style="padding-left: 74pt;text-indent: 0pt;text-align: center;">and global)</p><p class="s63" style="padding-left: 9pt;text-indent: 9pt;text-align: left;">Yes (quadratic)</p><p class="s63" style="padding-left: 13pt;text-indent: 0pt;text-align: left;">reasoning for complex logical queries</p><p class="s63" style="padding-top: 3pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">FB15k-237, NELL995     Yes</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 13pt;text-indent: 0pt;text-align: left;"><a href="#bookmark211" class="s126">(VGT) [151]</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l10"><li data-list-text="1)"><p class="s21" style="padding-top: 4pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a name="bookmark31">&zwnj;</a>Protein Structure Prediction: <a href="#bookmark217" class="a">In the field of bioin- formatics, graph transformers have demonstrated substantial potential in Protein Structure Prediction (PSP) </a><a href="#bookmark219" class="a">[154]. Gu et al. </a><a href="#bookmark220" class="a">[155] introduced HEAL, which employs hierarchical graph transformers on super-nodes that imitate functional motifs to interact with nodes in the protein graph, effectively capturing structural semantics. Pepe et al. </a><a href="#bookmark223" class="a">[156] used Geometric Algebra (GA) modelling to introduce a new metric based on the relative orientations of amino acid residues, serving as an additional input feature to a graph transformer, assisting in the prediction of the 3D coordinates of a protein. Chen et al. </a><span class="p">[157] proposed gated-graph transformers integrating node and edge gates within a graph transformer framework to regulate information flow during graph message passing, proving beneficial in pre- dicting the quality of 3D protein complex structures. Despite</span></p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark224" class="a">the encouraging outcomes, various challenges persist, such as the complexity of protein structures, scarcity of high-quality training data, and substantial computational resource </a>[158]. Further investigation is necessary to address these challenges and enhance the precision of these models.</p></li><li data-list-text="2)"><p class="s21" style="padding-top: 1pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a name="bookmark32">&zwnj;</a>Entity Resolution: <a href="#bookmark226" class="a">Entity Resolution (ER) is a crucial task in data management that aims to identify and link disparate representations of real-world entities from diverse sources </a><a href="#bookmark228" class="a">[159]. Recent research has highlighted the efficacy of graph transformers in ER. For example, Yao et al. </a><a href="#bookmark230" class="a">[160] proposed Hierarchical Graph Attention Networks (HierGAT) integrating the self-attention mechanism and graph attention network mechanism to capture and leverage the relationships between different ER decisions, leading to substantial en- hancements over conventional approaches. Ying et al. </a><a href="#bookmark230">[161]</a></p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark232" class="a" name="bookmark33">extended the standard transformer architecture and introduced several straightforward yet powerful structural encoding tech- niques to enhance the modeling of graph-structured data. Despite facing challenges related to data complexity and struc- tural information encoding, these techniques have exhibited promising outcomes in terms of enhanced performance, scal- ability, and accuracy. Furthermore, Dou et al. </a>[162] proposed the Hybrid Matching Knowledge for Entity Matching (GTA) method improves the transformer for representing relational data by integrating additional hybrid matching knowledge acquired through graph contrastive learning on a specially designed hybrid matching graph. This approach has also demonstrated promising results by effectively boosting the transformer for representing relational data and surpassing existing entity matching frameworks.</p></li><li data-list-text="3)"><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Anomaly Detection: <a href="#bookmark124" class="a">Graph transformers are valuable tools for anomaly detection, especially in dynamic graphs and time series data </a><a href="#bookmark235" class="a">[65], </a><a href="#bookmark237" class="a">[163]. They tackle key challenges like encoding information for unattributed nodes and extract- ing discriminative knowledge from spatial-temporal dynamic graphs. Liu et al </a><a href="#bookmark239" class="a">[164] proposed TADDY, a transformer- based Anomaly Detection framework, enhancing node encod- ing to represent each node’s structural and temporal roles in evolving graph streams. Similarly, Xu et al. </a><a href="#bookmark241" class="a">[165] proposed the Anomaly Transformer which uses an Anomaly-Attention mechanism to measure association discrepancy and employs a minimax strategy to enhance normal-abnormal differentiation. Chen et al. </a><a href="#bookmark243" class="a">[166] proposed the GTA framework for multivariate time series anomaly detection incorporates graph structure learning, graph convolution, and temporal dependency mod- eling with a transformer-based architecture. Tuli et al. </a><span class="p">[167] developed TranAD, a deep transformer network for anomaly detection in multivariate time series data, showing efficient anomaly detection and diagnosis in modern industrial appli- cations. Despite their effectiveness, further research is needed to enhance their performance and applicability across different domains.</span></p><p style="padding-top: 7pt;text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li data-list-text="B."><p class="s21" style="padding-left: 21pt;text-indent: -13pt;text-align: justify;"><a name="bookmark34">&zwnj;</a>Edge-level Tasks</p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark245" class="a" name="bookmark35">Edge-level tasks aim to learn edge representations or predict edge attributes based on graph structure and node features </a><a href="#bookmark246" class="a">[168], </a><a href="#bookmark246">[169].</a></p><ol id="l11"><li data-list-text="1)"><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Drug-Drug Interaction Prediction: <a href="#bookmark248" class="a">Graph transformers have been increasingly employed in the prediction of Drug- Drug Interactions (DDIs) owing to their capability to adeptly model the intricate relationships between drugs and targets </a><a href="#bookmark167" class="a">[170]. Wang et al. </a><a href="#bookmark250" class="a">[109] proposed a method which uses a line graph with drug-protein pairs as vertices and a graph transformer network (DTI-GTN) for the purpose of forecast- ing drug-target interactions. Djeddi et al. </a><a href="#bookmark208" class="a">[171] proposed a novel approach named DTIOG for the prediction of DTIs, leveraging a Knowledge Graph Embedding (KGE) strategy and integrating contextual information derived from protein se- quences. Despite the encouraging outcomes, these approaches encounter challenges such as overlooking certain facets of the intermolecular information and identifying potential interac- tions for newly discovered drugs </a><span class="p">[172]. Nevertheless, findings</span></p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark210" class="a" name="bookmark36">from multiple studies indicate that graph transformers can proficiently anticipate DDIs and surpass the performance of existing algorithms </a><a href="#bookmark210">[173].</a></p></li><li data-list-text="2)"><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Knowledge Graph Completion: <a href="#bookmark212" class="a">In the domain of Knowl- edge Graph (KG) completion, the utilization of graph trans- formers has been extensively investigated </a><span class="p">[174]. Chen et al.</span></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark216" class="a" name="bookmark37">[175] proposed a novel inductive KG representation model, known as iHT, for KG completion through large-scale pre- training. This model comprises an entity encoder and a neighbor-aware relational scoring function, both parameterized by transformers. The application of this approach has led to remarkable advancements in performance, with a relative enhancement of more than 25% in mean reciprocal rank compared to previous models. Liu et al. </a><a href="#bookmark218" class="a">[176] introduced a generative transformer with knowledge-guided decoding for academic KG completion, which incorporates pertinent knowledge from the training corpus to provide guidance. Chen et al. </a>[177] developed a hybrid transformer with multi-level fusion to tackle challenges in multimodal KG completion tasks. This model integrates visual and textual representations through coarse-grained prefix-guided interaction and fine- grained correlation-aware fusion modules.</p></li><li data-list-text="3)"><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Recommender Systems: <a href="#bookmark221" class="a">Graph transformers have been effectively utilized in recommender systems by combining generative self-supervised learning with a graph transformer architecture [</a><a href="#bookmark222" class="a">178]. Xia et al. </a><span class="p">[179] used the generative self- supervised learning method to extract representations from the data in an unsupervised manner and utilized graph trans- former architecture to capture intricate relationships between users and items in the recommendation system. Li et al.</span></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: justify;">[180] introduced a new method for recommender systems that leverages graph transformers (GFormer). Their approach automates the self-supervision augmentation process through a technique called rationale-aware generative self-supervised learning. This technique identifies informative patterns in user- item interactions. The proposed recommender system utilizes a special type of collaborative rationale discovery to selectively augment the self-supervision while preserving the overall relationships between users and items. The rationale-aware self-supervised learning in the graph transformer enables graph collaborative filtering. While challenges remain in areas like graph construction, network design, model optimization, computation efficiency, and handling diverse user behaviors, experiments show that the approach consistently outperforms baseline models on various datasets.</p><p style="padding-top: 6pt;text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li data-list-text="C."><p class="s21" style="padding-left: 22pt;text-indent: -14pt;text-align: justify;"><a name="bookmark38">&zwnj;</a>Graph-level Tasks</p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a name="bookmark39">&zwnj;</a>Graph-level tasks aim to learn graph representations or predict graph attributes based on graph structure and node features.</p><ol id="l12"><li data-list-text="1)"><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Molecular Property Prediction: <a href="#bookmark227" class="a">Graph transformers are powerful tools for molecular property prediction, utilizing the graph structure of molecules to capture essential structural and semantic information </a><a href="#bookmark229" class="a">[181]. Chen et al. </a><span class="p">[182] proposed Algebraic Graph-Assisted Bidirectional Transformer (AGBT) framework which integrates complementary 3D molecular information into graph invariants, rectifying the oversight</span></p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark202" class="a" name="bookmark40">of three-dimensional stereochemical information in certain machine learning models. Li et al </a><a href="#bookmark231" class="a">[144] utilized Knowledge- Guided Pre-training of Graph Transformer (KPGT) in a self- supervised learning framework which emphasizes the impor- tance of chemical bonds and models the structural information of molecular graphs. Buterez et al. </a>[183] proposed transfer learning with graph transformer to enhance molecular property prediction on sparse and costly high-fidelity data.</p></li><li data-list-text="2)"><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a name="bookmark41">&zwnj;</a>Graph Clustering: <a href="#bookmark233" class="a">Graph transformers have been in- creasingly utilized in the field of Graph Clustering, offering innovative methodologies and overcoming significant chal- lenges </a><a href="#bookmark234" class="a">[184]. Yun et al. </a><a href="#bookmark236" class="a">[185] proposed a graph transformer network to generate new graph structures for identifying useful connections between unconnected nodes on the original graph while learning effective node representation on the new graphs in an end-to-end fashion. Gao et al. </a><span class="p">[186] proposed a patch graph transformer (PatchGT) that segments a graph into patches based on spectral clustering without any trainable parameters and allows the model to first use GNN layers to learn patch-level representations and then use transformer to obtain graph-level representations. These methodologies have addressed issues such as the limitations of the local attention mechanism and difficulties in learning high-level information, leading to enhanced graph representation, improved model performance, and effective node representation.</span></p></li><li data-list-text="3)"><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Graph Synthesis: <a href="#bookmark75" class="a">Graph transformers have been applied in the field of graph synthesis to improve graph data mining and representation learning. Existing graph transformers with Positional Encodings have limitations in node classification tasks on complex graphs, as they do not fully capture the local node properties </a><a href="#bookmark238" class="a">[16]. To address this, Ma et al. </a><a href="#bookmark240" class="a">[187] introduced the Adaptive Graph Transformer (AGT). This model tackles the challenge of extracting structural patterns from graphs in a way that is both effective and efficient. AGT achieves this by learning from two different graph perspectives: centrality and subgraph views. This approach has been shown to achieve state-of-the-art performance on real-world web graphs and synthetic graphs characterized by heterophily and noise. Jiang et al. </a><a href="#bookmark180" class="a">[188] proposed an Anchor Graph Transformer (AGFormer) that leverages an anchor graph model to perform more efficient and robust node-to- node message passing for overcoming the computational cost and sensitivity to graph noises of regular graph transformers. Zhu et al. </a><span class="p">[122] developed a Hierarchical Scalable Graph Transformer (HSGT) which scales the transformer architecture to node representation learning tasks on large-scale graphs by utilizing graph hierarchies and sampling-based training methods.</span></p><p style="padding-top: 6pt;text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li data-list-text="D."><p class="s21" style="padding-left: 22pt;text-indent: -14pt;text-align: justify;"><a name="bookmark42">&zwnj;</a>Other Application Scenarios</p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark242" class="a" name="bookmark43">Graph transformers have a wide range of applications beyond graph-structured data. They can also be utilized in scenarios involving text, images, or videos. </a><a href="#bookmark242">[189].</a></p><ol id="l13"><li data-list-text="1)"><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Text Summarization: <a href="#bookmark244" class="a">Text summarization is a crucial aspect of NLP which has been significantly advanced with the introduction of Graph transformers </a><span class="p">[190]. These models utilize extractive, abstractive, and hybrid methods for summarization.</span></p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark247" class="a" name="bookmark44">Extractive summarization selects and extracts key sentences or phrases from the original text to create a summary </a><a href="#bookmark249" class="a">[191]. In contrast, abstractive summarization interprets the core con- cepts in the text and produces a concise summary. Hybrid summarization combines the advantages of both approaches </a><a href="#bookmark251" class="a">[192]. Despite the progress, challenges remain in text com- prehension, main idea identification, and coherent summary generation. Nevertheless, the application of graph transformers in text summarization has demonstrated promising outcomes in terms of summary quality and efficiency </a><a href="#bookmark251">[193].</a></p></li><li data-list-text="2)"><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a name="bookmark45">&zwnj;</a>Image Captioning: <a href="#bookmark252" class="a">Graph transformers have emerged as a potent tool within the domain of image captioning, offering a structured representation of images and efficiently processing them to produce descriptive captions </a><a href="#bookmark253" class="a">[194]. Techniques such as Transforming Scene Graphs (TSG) leverage multi-head attention to architect graph neural networks for embedding scene graphs, which encapsulate a myriad of specific knowl- edge to facilitate the generation of words across various parts of speech </a><a href="#bookmark254" class="a">[195]. Despite encountering challenges, such as training complexity, absence of contextual information, and lack of fine-grained details in the extracted features, graph transformers have exhibited promising outcomes. They have enhanced the quality of generated sentences and attained state- of-the-art performance in image captioning endeavors </a><a href="#bookmark254">[196].</a></p></li><li data-list-text="3)"><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a name="bookmark46">&zwnj;</a>Image Generation: <a href="#bookmark255" class="a">Graph transformers have been ef- fectively utilized in image generation, as demonstrated by various research studies </a><a href="#bookmark92" class="a">[197]. Sortino et al. </a><a href="#bookmark256" class="a">[33] proposed a transformer-based method conditioned by scene graphs for image generation, employing a decoder to sequentially com- pose images. Zhang et al. </a><a href="#bookmark257" class="a">[198] proposed StyleSwin which uses transformers in constructing a generative adversarial network for high-resolution image creation. Despite challenges like redundant interactions and the requirement for intricate architectures, these studies have exhibited promising outcomes in terms of image quality and variety </a><a href="#bookmark258" class="a">[199], </a><a href="#bookmark258">[200].</a></p></li><li data-list-text="4)"><p class="s21" style="padding-left: 7pt;text-indent: 9pt;text-align: justify;">Video Generation: <a href="#bookmark259" class="a">Graph transformers have been ex- tensively applied in the field of Video Generation. Xiao et al. </a><a href="#bookmark260" class="a">[201] proposed a Video Graph Transformer (VGT) model, which utilizes a dynamic graph transformer module to encode videos, capturing visual objects, their relationships, and dy- namics. It incorporates disentangled video and text transform- ers for comparing relevance between the video and text. Wu et al. </a><a href="#bookmark261" class="a">[202] proposed The Object-Centric Video Transformer (OCVT) which adopts an object-centric strategy to break down scenes into tokens suitable for a generative video transformer and understanding the intricate spatiotemporal dynamics of multiple interacting objects within a scene. Yan et al. </a><a href="#bookmark262" class="a">[203] developed VideoGPT, which learns downsampled discrete latent representations of a raw video through 3D convolutions and axial self-attention. A GPT-like architecture is then used to model the discrete latent in a spatiotemporal manner us- ing position encodings. Tulyakov et al. </a><span class="p">[204] proposed the MoCoGAN model which creates a video by mapping a series of random vectors to a sequence of video frames. Despite the challenges in capturing complex spatio-temporal dynam- ics in videos, these methodologies have exhibited promising outcomes across various facets of video generation, ranging from question answering to video summarization and beyond.</span></p></li></ol></li></ol></li><li data-list-text="VI."><p style="padding-top: 4pt;padding-left: 56pt;text-indent: -18pt;text-align: left;"><a name="bookmark50">&zwnj;</a>Open Issues and Future Directions<a name="bookmark56">&zwnj;</a></p><p style="padding-top: 6pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;">Despite their immense potential for learning from graph- structured data, graph transformers still face open issues and challenges that require further exploration. Here we highlight some of these open challenges.</p><p style="padding-top: 7pt;text-indent: 0pt;text-align: left;"><br/></p><ol id="l14"><li data-list-text="A."><p class="s21" style="padding-left: 21pt;text-indent: -13pt;text-align: justify;"><a name="bookmark51">&zwnj;</a>Scalability and Efficiency</p><p style="padding-top: 5pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark172" class="a">The scalability and efficiency of graph transformers pose considerable challenges due to their substantial memory and computational requirements especially when employing global attention mechanisms to deal with large-scale graphs </a><a href="#bookmark263" class="a">[114], </a>[205]. These challenges are further amplified in deep architec- tures which are susceptible to overfitting and over-smoothing. To address these issues, several potential strategies can be proposed:</p><ol id="l15"><li data-list-text="1)"><p style="padding-top: 2pt;padding-left: 32pt;text-indent: -14pt;text-align: justify;">Developing efficient attention mechanisms, such as lin- ear, sparse and low-rank attention, to reduce the com- plexity and memory usage of graph transformers.</p></li><li data-list-text="2)"><p style="padding-left: 32pt;text-indent: -14pt;text-align: justify;">Applying graph sparsification or coarsening techniques to decrease the size and density of graphs while main- taining their crucial structural and semantic information.</p></li><li data-list-text="3)"><p style="padding-left: 32pt;text-indent: -14pt;text-align: justify;">Using graph partitioning or sampling methods to divide large graphs into smaller subgraphs or batches for par- allel or sequential processing.</p></li><li data-list-text="4)"><p style="padding-left: 32pt;text-indent: -14pt;text-align: justify;">Exploring graph distillation or compression methods to create compact and effective graph transformer models, which are suitable for deployment on resource-limited devices.</p></li><li data-list-text="5)"><p style="padding-left: 32pt;text-indent: -14pt;text-align: justify;">Investigating regularization or normalization techniques, such as dropout, graph diffusion, convolution, and graph spectral normalization, to prevent overfitting and over- smoothing in graph transformer models.</p><p style="padding-top: 7pt;text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li data-list-text="B."><p class="s21" style="padding-left: 21pt;text-indent: -13pt;text-align: justify;"><a name="bookmark52">&zwnj;</a>Generalization and Robustness</p><p style="padding-top: 5pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;">Graph transformers often face challenges when it comes to generalizing to graphs that they have not encountered before or that fall outside of their usual distribution. This is especially true for graphs that have different sizes, structures, features, and domains. Additionally, graph transformers can be vulnerable to adversarial attacks and noisy inputs, which can result in a decline in performance and the production of misleading results. In order to improve the generalization and robustness of graph transformers, the following strategies could be taken into account:</p><ol id="l16"><li data-list-text="1)"><p style="padding-top: 2pt;padding-left: 32pt;text-indent: -14pt;text-align: justify;">Developing adaptive and flexible attention mechanisms, such as dynamic attention, span-adaptive attention, and multi-head attention with different scales, to accommo- date varying graphs and tasks.</p></li><li data-list-text="2)"><p style="padding-left: 32pt;text-indent: -14pt;text-align: justify;">Applying domain adaptation or transfer learning tech- niques to facilitate learning from multiple source do- mains and transfer the knowledge from source domains to target domains.</p></li><li data-list-text="3)"><p style="padding-left: 32pt;text-indent: -14pt;text-align: justify;">Exploring meta-learning or few-shot learning techniques to enable learning from limited data and rapid adaptation to new tasks.</p></li><li data-list-text="4)"><p style="padding-top: 4pt;padding-left: 32pt;text-indent: -14pt;text-align: justify;">Designing robust and secure attention mechanisms, such as adversarial attention regularization, attention mask- ing, and attention perturbation, to resist adversarial at- tacks and noisy inputs.</p></li><li data-list-text="5)"><p style="padding-left: 32pt;text-indent: -14pt;text-align: justify;">Evaluating the uncertainty and reliability of graph trans- former models using probabilistic or Bayesian methods, such as variational inference, Monte Carlo dropout, and deep ensembles.</p><p style="padding-top: 6pt;text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li data-list-text="C."><p class="s21" style="padding-left: 22pt;text-indent: -14pt;text-align: justify;"><a name="bookmark53">&zwnj;</a>Interpretability and Explainability</p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;">Graph transformers commonly regarded as black box mod- els present significant challenges in terms of interpretability and explainability. The lack of sufficient justification and evidence for their decisions can undermine their credibility and transparency. To address this issue, several approaches can be considered:</p><ol id="l17"><li data-list-text="1)"><p style="padding-top: 2pt;padding-left: 32pt;text-indent: -14pt;text-align: justify;">Developing transparent and interpretable attention mech- anisms, such as attention visualization, attention attribu- tion, and attention pruning, to highlight the importance and relevance of different nodes and edges in graphs.</p></li><li data-list-text="2)"><p style="padding-left: 32pt;text-indent: -14pt;text-align: justify;">Applying explainable artificial intelligence (XAI) tech- niques, such as saliency maps, influence functions, and counterfactual explanations, to analyze and understand the behaviour and logic of graph transformer models.</p></li><li data-list-text="3)"><p style="padding-left: 32pt;text-indent: -14pt;text-align: justify;">Exploring natural language generation techniques, such as template-based generation, neural text generation, and question-answering generation, to produce natural language explanations for outputs or actions of graph transformer models.</p></li><li data-list-text="4)"><p style="padding-left: 32pt;text-indent: -14pt;text-align: justify;"><a href="#bookmark90" class="a">Investigating human-in-the-loop methods, such as active learning, interactive learning, and user studies, to in- corporate human guidance in the learning or evaluation process of graph transformer models </a><a href="#bookmark90">[31].</a></p><p style="padding-top: 5pt;text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li data-list-text="D."><p class="s21" style="padding-left: 22pt;text-indent: -14pt;text-align: justify;"><a name="bookmark54">&zwnj;</a>Learning on Dynamic Graphs</p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;"><a href="#bookmark264" class="a">Graphs, which are frequently characterized by their dynamic and intricate nature, possess the ability to transform over time as a result of the addition or removal of nodes and edges, as well as the modification of node and edge attributes </a>[206]. Moreover, these graphs may exhibit diverse types and modalities of nodes and edges. In order to empower graph transformers to effectively manage such dynamic graphs, it is advisable to explore the following strategies:</p><ol id="l18"><li data-list-text="1)"><p style="padding-top: 2pt;padding-left: 32pt;text-indent: -14pt;text-align: justify;">Developing temporal and causal attention mechanisms, such as recurrent, temporal, and causal attention, to capture the temporal and causal evolution of graphs.</p></li><li data-list-text="2)"><p style="padding-left: 32pt;text-indent: -14pt;text-align: justify;">Applying continual learning techniques on dynamic graphs to void forgetting previous knowledge and re- training.</p></li><li data-list-text="3)"><p style="padding-left: 32pt;text-indent: -14pt;text-align: justify;">Exploring multimodal attention mechanisms, such as image-text, audio-visual, and heterogeneous attention, to integrate multimodal nodes and edges.</p></li><li data-list-text="4)"><p style="padding-left: 32pt;text-indent: -14pt;text-align: justify;">Leveraging multi-level and multi-layer attention mecha- nisms, such as node-edge, graph-graph, and hypergraph attention, to aggregate information from different levels and layers of graphs.</p></li></ol></li><li data-list-text="E."><p class="s21" style="padding-top: 4pt;padding-left: 21pt;text-indent: -13pt;text-align: justify;"><a name="bookmark55">&zwnj;</a>Data Quality and Diversity</p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;">Graph transformers require a significant quantity of diverse and high-quality data to achieve effective learning. Never- theless, in the real world, data is frequently limited, noisy, incomplete, imbalanced, and biased. This has a detrimental effect on the performance and fairness of graph transformer models. To mitigate these challenges related to data quality and diversity, several potential strategies can be considered:</p><ol id="l19"><li data-list-text="1)"><p style="padding-top: 2pt;padding-left: 32pt;text-indent: -14pt;text-align: justify;">Developing noise-tolerant attention mechanisms, such as denoising attention, error correction attention, and self- supervised attention, to filter out data noise and errors.</p></li><li data-list-text="2)"><p style="padding-left: 32pt;text-indent: -14pt;text-align: justify;"><a href="#bookmark265" class="a">Applying data augmentation or generation techniques, such as graph augmentation </a>[207], graph completion, and graph generation, to enhance the quantity and vari- ety of data.</p></li><li data-list-text="3)"><p style="padding-left: 32pt;text-indent: -14pt;text-align: justify;">Exploring data imbalance and bias mitigation tech- niques, such as data re-sampling, data re-weighting, and data debiasing, to improve data fairness and equity.</p></li><li data-list-text="4)"><p style="padding-left: 32pt;text-indent: -14pt;text-align: justify;">Evaluating data quality and diversity using data quality assessment or data diversity analysis methods, such as data quality metrics, data diversity measures, and data quality visualization.</p></li></ol></li></ol><p style="padding-top: 2pt;text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="VII."><p style="padding-left: 115pt;text-indent: -22pt;text-align: left;"><a name="bookmark57">&zwnj;</a>Conclusion<a name="bookmark58">&zwnj;</a></p></li></ol><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;">Graph transformers are a novel and powerful class of neural network models, which can effectively encode and process graph-structured data. This survey provides a com- prehensive overview of graph transformers in terms of design perspectives, taxonomy, applications, and open issues. We first discuss how graph transformers incorporate graph inductive bias, including node positional encodings, edge structural encodings, message-passing bias and attention bias, to encode the structural information of graphs. Then, we introduce the design of graph attention mechanisms, including global and local attention mechanisms. Afterwards, a taxonomy of graph transformers is presented. This survey also includes a design guide for effective graph transformers, including the best practices and recommendations for selecting appropriate components and hyperparameters. Moreover, the application scenarios of graph transformers are reviewed based on various graph-related tasks (e.g., node-level, edge-level, and graph- level tasks), as well as tasks in other domains. Lastly, current challenges and future directions of graph transformers are identified. This survey aims to serve as a valuable reference for researchers and practitioners interested in graph transformers and their applications.</p><p style="padding-top: 2pt;text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 81pt;text-indent: 0pt;text-align: center;">Acknowledgment</p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 9pt;text-align: justify;">The authors would like to thank Estrid He (RMIT Univer- sity) for her helpful comments.</p><p style="padding-top: 2pt;text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 81pt;text-indent: 0pt;text-align: center;"><a name="bookmark59">&zwnj;</a>References</p><ol id="l20"><li data-list-text="[1]"><p class="s7" style="padding-top: 5pt;padding-left: 28pt;text-indent: -14pt;line-height: 9pt;text-align: center;"><a name="bookmark60">&zwnj;</a>P. Velicˇkovic´, “Everything is connected: Graph neural networks,”</p><p class="s133" style="padding-left: 87pt;text-indent: 0pt;line-height: 9pt;text-align: center;"><a name="bookmark61">&zwnj;</a>Current Opinion in Structural Biology<span class="s7">, vol. 79, p. 102538, 2023.</span></p></li><li data-list-text="[2]"><p class="s7" style="padding-left: 30pt;text-indent: -14pt;text-align: justify;">S. Wu, F. Sun, W. Zhang, X. Xie, and B. Cui, “Graph Neural Networks in Recommender Systems: A Survey,” <i>ACM Computing Surveys</i>, vol. 55, no. 5, pp. 1–37, May 2023.</p></li><li data-list-text="[3]"><p class="s7" style="padding-top: 7pt;padding-left: 26pt;text-indent: -14pt;text-align: justify;"><a name="bookmark62">&zwnj;</a>A. A. Yusuf, C. Feng, X. Mao, R. Ally Duma, M. S. Abood, and A. H. A. Chukkol, “Graph neural networks for visual question answering: A systematic review,” <i>Multimedia Tools and Applications</i>, Nov. 2023.<a name="bookmark63">&zwnj;</a></p></li><li data-list-text="[4]"><p class="s7" style="padding-left: 26pt;text-indent: -14pt;text-align: justify;"><a name="bookmark64">&zwnj;</a>X. Ma, J. Wu, S. Xue, J. Yang, C. Zhou, Q. Z. Sheng, H. Xiong, and L. Akoglu, “A comprehensive survey on graph anomaly detection with deep learning,” <i>IEEE Transactions on Knowledge and Data Engineering</i>, 2021.</p></li><li data-list-text="[5]"><p class="s7" style="padding-left: 26pt;text-indent: -14pt;text-align: justify;"><a name="bookmark65">&zwnj;</a>N. Das, B. Sadhukhan, R. Chatterjee, and S. Chakrabarti, “Integrating sentiment analysis with graph neural networks for enhanced stock prediction: A comprehensive survey,” <i>Decision Analytics Journal</i>, p. 100417, 2024.</p></li><li data-list-text="[6]"><p class="s7" style="padding-left: 26pt;text-indent: -14pt;text-align: justify;"><a name="bookmark66">&zwnj;</a>Y. Lin, T. Ruan, J. Liu, and H. Wang, “A survey on neural data-to-text generation,” <i>IEEE Transactions on Knowledge and Data Engineering</i>, 2023.</p></li><li data-list-text="[7]"><p class="s7" style="padding-left: 26pt;text-indent: -14pt;text-align: justify;"><a name="bookmark67">&zwnj;</a>K. S. Yow, N. Liao, S. Luo, and R. Cheng, “Machine Learning for Sub- graph Extraction: Methods, Applications and Challenges,” <i>Proceedings of the VLDB Endowment</i>, vol. 16, no. 12, pp. 3864–3867, Aug. 2023.</p></li><li data-list-text="[8]"><p class="s7" style="padding-left: 26pt;text-indent: -14pt;line-height: 9pt;text-align: justify;">F. Liu, S. Xue, J. Wu, C. Zhou, W. Hu, C. Paris, S. Nepal, J. Yang, and</p><p class="s7" style="padding-left: 26pt;text-indent: 0pt;text-align: justify;"><a name="bookmark68">&zwnj;</a>P. S. Yu, “Deep learning for community detection: Progress, challenges and opportunities,” in <i>Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020</i>, C. Bessiere, Ed. ijcai.org, 2020, pp. 4981–4987.</p></li><li data-list-text="[9]"><p class="s7" style="padding-left: 26pt;text-indent: -14pt;text-align: justify;"><a name="bookmark69">&zwnj;</a>Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, “A comprehensive survey on graph neural networks,” <i>IEEE Transactions on Neural Networks and Learning Systems</i>, vol. 32, no. 1, pp. 4–24, 2020.</p></li><li data-list-text="[10]"><p class="s7" style="padding-left: 26pt;text-indent: -18pt;text-align: justify;"><a name="bookmark70">&zwnj;</a>Z. Ye, Y. J. Kumar, G. O. Sing, F. Song, and J. Wang, “A comprehensive survey of graph neural networks for knowledge graphs,” <i>IEEE Access</i>, vol. 10, pp. 75 729–75 741, 2022.</p></li><li data-list-text="[11]"><p class="s7" style="padding-left: 26pt;text-indent: -18pt;text-align: justify;"><a name="bookmark71">&zwnj;</a>C. Ying, T. Cai, S. Luo, S. Zheng, G. Ke, D. He, Y. Shen, and T.-Y. Liu, “Do transformers really perform badly for graph representation?” <i>Advances in Neural Information Processing Systems</i>, vol. 34, pp. 28 877–28 888, 2021.</p></li><li data-list-text="[12]"><p class="s7" style="padding-left: 26pt;text-indent: -18pt;text-align: justify;"><a name="bookmark72">&zwnj;</a>L. Mu¨ller, M. Galkin, C. Morris, and L. Rampa´sˇek, “Attending to graph transformers,” <i>Transactions on Machine Learning Research</i>, 2024.</p></li><li data-list-text="[13]"><p class="s7" style="padding-left: 26pt;text-indent: -18pt;text-align: justify;"><a name="bookmark73">&zwnj;</a>A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” <i>Advances in Neural Information Processing Systems</i>, vol. 30, 2017.</p></li><li data-list-text="[14]"><p class="s7" style="padding-left: 26pt;text-indent: -18pt;text-align: justify;"><a name="bookmark74">&zwnj;</a>T. Lin, Y. Wang, X. Liu, and X. Qiu, “A survey of transformers,” <i>AI Open</i>, 2022.</p></li><li data-list-text="[15]"><p class="s7" style="padding-left: 26pt;text-indent: -18pt;text-align: justify;"><a name="bookmark75">&zwnj;</a>J. Kim, D. Nguyen, S. Min, S. Cho, M. Lee, H. Lee, and S. Hong, “Pure transformers are powerful graph learners,” <i>Advances in Neural Information Processing Systems</i>, vol. 35, pp. 14 582–14 595, 2022.</p></li><li data-list-text="[16]"><p class="s7" style="padding-left: 26pt;text-indent: -18pt;text-align: justify;"><a name="bookmark76">&zwnj;</a>V. P. Dwivedi and X. Bresson, “A generalization of transformer networks to graphs,” <i>AAAI Workshop on Deep Learning on Graphs: Methods and Applications</i>, 2021.</p></li><li data-list-text="[17]"><p class="s7" style="padding-left: 26pt;text-indent: -18pt;text-align: justify;"><a name="bookmark77">&zwnj;</a>F. Xia, K. Sun, S. Yu, A. Aziz, L. Wan, S. Pan, and H. Liu, “Graph learning: A survey,” <i>IEEE Transactions on Artificial Intelligence</i>, vol. 2, no. 2, pp. 109–127, 2021.</p></li><li data-list-text="[18]"><p class="s7" style="padding-left: 26pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">C. Chen, Y. Wu, Q. Dai, H.-Y. Zhou, M. Xu, S. Yang, X. Han, and</p><p class="s7" style="padding-left: 26pt;text-indent: 0pt;text-align: justify;"><a name="bookmark78">&zwnj;</a>Y. Yu, “A survey on graph neural networks and graph transform- ers in computer vision: A task-oriented perspective,” <i>arXiv preprint arXiv:2209.13232</i>, 2022.</p></li><li data-list-text="[19]"><p class="s7" style="padding-left: 26pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">E. Min, R. Chen, Y. Bian, T. Xu, K. Zhao, W. Huang, P. Zhao, J. Huang,</p><p class="s7" style="padding-left: 26pt;text-indent: 0pt;text-align: justify;"><a name="bookmark79">&zwnj;</a>S. Ananiadou, and Y. Rong, “Transformer for graphs: An overview from architecture perspective,” <i>arXiv preprint arXiv:2202.08455</i>, 2022.</p></li><li data-list-text="[20]"><p class="s7" style="padding-left: 26pt;text-indent: -18pt;text-align: justify;"><a name="bookmark80">&zwnj;</a>F. Chen, Y.-C. Wang, B. Wang, and C.-C. J. Kuo, “Graph representation learning: A survey,” <i>APSIPA Transactions on Signal and Information Processing</i>, vol. 9, p. e15, 2020.</p></li><li data-list-text="[21]"><p class="s7" style="padding-left: 26pt;text-indent: -18pt;text-align: justify;"><a name="bookmark81">&zwnj;</a>K. Xu, W. Hu, J. Leskovec, and S. Jegelka, “How powerful are graph neural networks?” in <i>7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019</i>. OpenReview.net, 2019.</p></li><li data-list-text="[22]"><p class="s7" style="padding-left: 26pt;text-indent: -18pt;text-align: justify;"><a name="bookmark82">&zwnj;</a>X. Wang and M. Zhang, “How powerful are spectral graph neural networks,” in <i>International Conference on Machine Learning</i>. PMLR, 2022, pp. 23 341–23 362.</p></li><li data-list-text="[23]"><p class="s7" style="padding-left: 26pt;text-indent: -18pt;text-align: justify;"><a name="bookmark83">&zwnj;</a>D. Bo, X. Wang, Y. Liu, Y. Fang, Y. Li, and C. Shi, “A Survey on Spectral Graph Neural Networks,” <i>arXiv preprint arXiv:2302.05631</i>, 2023.</p></li><li data-list-text="[24]"><p class="s7" style="padding-left: 26pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">Z. Zhang, P. Cui, and W. Zhu, “Deep learning on graphs: A survey,”</p><p class="s133" style="padding-left: 26pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">IEEE Transactions on Knowledge and Data Engineering<span class="s7">, vol. 34, no. 1,</span></p><p class="s7" style="padding-left: 26pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark84">&zwnj;</a>pp. 249–270, 2020.</p></li><li data-list-text="[25]"><p class="s7" style="padding-left: 26pt;text-indent: -18pt;text-align: justify;">D. Bacciu, F. Errica, A. Micheli, and M. Podda, “A gentle introduction to deep learning for graphs,” <i>Neural Networks</i>, vol. 129, pp. 203–221, 2020.</p></li><li data-list-text="[26]"><p class="s7" style="padding-top: 7pt;padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark85">&zwnj;</a>P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative position representations,” in <i>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), New Orleans, Louisiana, USA, June 1-6</i>, 2018,</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">pp. 464–468.</p></li><li data-list-text="[27]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark86">&zwnj;</a>W. Nam and B. Jang, “A survey on multimodal bidirectional machine learning translation of image and natural language processing,” <i>Expert Systems with Applications</i>, p. 121168, 2023.</p></li><li data-list-text="[28]"><p class="s7" style="padding-top: 1pt;padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark87">&zwnj;</a>A. Elangovan, J. He, and K. Verspoor, “Memorization vs. general- ization: quantifying data leakage in nlp performance evaluation,” in <i>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics (ECL)</i>, 2021, pp. 1325– 1335.<a name="bookmark88">&zwnj;</a></p></li><li data-list-text="[29]"><p class="s7" style="padding-top: 1pt;padding-left: 30pt;text-indent: -18pt;text-align: justify;">J. He, Y. Li, Z. Zhai, B. Fang, C. Thorne, C. Druckenbrodt, S. Akhondi, and K. Verspoor, “Focused contrastive loss for classification with pre- trained language models,” <i>IEEE Transactions on Knowledge and Data Engineering (TKDE)</i>, 2023.</p></li><li data-list-text="[30]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark89">&zwnj;</a>J. He, B. Fang, H. Yoshikawa, Y. Li, S. A. Akhondi, C. Druckenbrodt,</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;text-align: justify;">C. Thorne, Z. Afzal, Z. Zhai, L. Cavedon <i>et al.</i>, “Chemu 2021: reaction reference resolution and anaphora resolution in chemical patents,” in <i>Advances in Information Retrieval: Proceedings of the 43rd European Conference on IR Research (ECIR)</i>. Springer International Publishing, 2021, pp. 608–615.</p></li><li data-list-text="[31]"><p class="s7" style="padding-top: 1pt;padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark90">&zwnj;</a>A. Elangovan, E. He, Y. Li, and K. Verspoor, “Effects of human adversarial and affable samples on bert generalization,” in <i>Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)</i>, 2023.</p></li><li data-list-text="[32]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark91">&zwnj;</a>N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, and</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;text-align: justify;">D. Tran, “Image transformer,” in <i>International Conference on Machine Learning</i>. PMLR, 2018, pp. 4055–4064.</p></li><li data-list-text="[33]"><p class="s7" style="padding-top: 1pt;padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark92">&zwnj;</a>R. Sortino, S. Palazzo, and C. Spampinato, “Transforming Image Generation from Scene Graphs,” in <i>2022 26th International Conference on Pattern Recognition (ICPR)</i>. IEEE, 2022, pp. 4118–4124.</p></li><li data-list-text="[34]"><p class="s7" style="padding-top: 1pt;padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark93">&zwnj;</a>J. He, J. Qi, and K. Ramamohanarao, “Timesan: A time-modulated self-attentive network for next point-of-interest recommendation,” in <i>Proceedings of the 2020 International Joint Conference on Neural Networks (IJCNN)</i>. IEEE, 2020, pp. 1–8.</p></li><li data-list-text="[35]"><p class="s7" style="padding-top: 1pt;padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark94">&zwnj;</a>S. Deldari, H. Xue, A. Saeed, J. He, D. V. Smith, and F. D. Salim, “Be- yond just vision: A review on self-supervised representation learning on multimodal and temporal data,” <i>arXiv preprint arXiv:2206.02353</i>, 2022.<a name="bookmark95">&zwnj;</a></p></li><li data-list-text="[36]"><p class="s7" style="padding-top: 1pt;padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark96">&zwnj;</a>S. Yun, M. Jeong, R. Kim, J. Kang, and H. J. Kim, “Graph trans- former networks,” <i>Advances in Neural Information Processing Systems</i>, vol. 32, 2019.</p></li><li data-list-text="[37]"><p class="s7" style="padding-top: 1pt;padding-left: 30pt;text-indent: -18pt;text-align: justify;">F. Xia, S. Yu, C. Liu, J. Li, and I. Lee, “Chief: Clustering with higher- order motifs in big networks,” <i>IEEE Transactions on Network Science and Engineering</i>, vol. 9, no. 3, pp. 990–1005, 2021.</p></li><li data-list-text="[38]"><p class="s7" style="padding-top: 1pt;padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark97">&zwnj;</a>V. Shiv and C. Quirk, “Novel positional encodings to enable tree-based transformers,” <i>Advances in Neural Information Processing Systems</i>, vol. 32, 2019.<a name="bookmark98">&zwnj;</a></p></li><li data-list-text="[39]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">L. Rampa´sˇek, M. Galkin, V. P. Dwivedi, A. T. Luu, G. Wolf, and</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;text-align: justify;"><a name="bookmark99">&zwnj;</a>D. Beaini, “Recipe for a general, powerful, scalable graph transformer,” <i>Advances in Neural Information Processing Systems</i>, vol. 35, pp. 14 501–14 515, 2022.</p></li><li data-list-text="[40]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">P. Chen, H. Tsai, S. Bhojanapalli, H. W. Chung, Y. Chang, and</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;text-align: justify;">C. Ferng, “A simple and effective positional encoding for transformers,” in <i>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), Virtual Event / Punta Cana, Dominican Republic, 7-11 November</i>, 2021, pp. 2974–2988.</p></li><li data-list-text="[41]"><p class="s7" style="padding-top: 1pt;padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark100">&zwnj;</a>W. Park, W.-G. Chang, D. Lee, J. Kim, and seung-won hwang, “GRPE: Relative positional encoding for graph transformer,” in <i>ICLR2022 Machine Learning for Drug Discovery</i>, 2022.</p></li><li data-list-text="[42]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark101">&zwnj;</a>P. Velicˇkovic´, G. Cucurull, A. Casanova, A. Romero, P. Lio, and</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;text-align: justify;">Y. Bengio, “Graph attention networks,” in <i>International Conference on Learning Representations</i>, 2018.</p></li><li data-list-text="[43]"><p class="s7" style="padding-top: 1pt;padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark102">&zwnj;</a>L. Ma, R. Rabbany, and A. Romero-Soriano, “Graph attention networks with positional embeddings,” in <i>Pacific-Asia Conference on Knowledge Discovery and Data Mining</i>. Springer, 2021, pp. 514–527.</p></li><li data-list-text="[44]"><p class="s7" style="padding-top: 1pt;padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark103">&zwnj;</a>J. Liu, F. Xia, L. Wang, B. Xu, X. Kong, H. Tong, and I. King, “Shifu2: A network representation learning based model for advisor- advisee relationship mining,” <i>IEEE Transactions on Knowledge and Data Engineering</i>, vol. 33, no. 4, pp. 1763–1777, 2019.</p></li><li data-list-text="[45]"><p class="s7" style="padding-top: 1pt;padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark104">&zwnj;</a>J. Park, S. Yun, H. Park, J. Kang, J. Jeong, K.-M. Kim, J.-w. Ha, and H. J. Kim, “Deformable Graph Transformer,” <i>arXiv preprint arXiv:2206.14337</i>, 2022.</p></li><li data-list-text="[46]"><p class="s7" style="padding-top: 7pt;padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark105">&zwnj;</a>G. Mialon, D. Chen, M. Selosse, and J. Mairal, “Graphit: Encoding graph structure in transformers,” <i>arXiv preprint arXiv:2106.05667</i>, 2021.<a name="bookmark106">&zwnj;</a></p></li><li data-list-text="[47]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark107">&zwnj;</a>A. Haviv, O. Ram, O. Press, P. Izsak, and O. Levy, “Transformer language models without positional encodings still learn positional information,” in <i>Findings of the Association for Computational Lin- guistics: EMNLP 2022</i>, 2022, pp. 1382–1390.</p></li><li data-list-text="[48]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark108">&zwnj;</a>H. Peng, G. Li, Y. Zhao, and Z. Jin, “Rethinking Positional Encoding in Tree Transformer for Code Representation,” in <i>Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</i>, 2022, pp. 3204–3214.</p></li><li data-list-text="[49]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark109">&zwnj;</a>S. Geisler, Y. Li, D. J. Mankowitz, A. T. Cemgil, S. Gu¨nnemann, and C. Paduraru, “Transformers meet directed graphs,” in <i>International Conference on Machine Learning</i>. PMLR, 2023, pp. 11 144–11 172.</p></li><li data-list-text="[50]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark110">&zwnj;</a>F. Xia, J. Liu, H. Nie, Y. Fu, L. Wan, and X. Kong, “Random walks: A review of algorithms and applications,” <i>IEEE Transactions on Emerging Topics in Computational Intelligence</i>, vol. 4, no. 2, pp. 95–107, 2019.</p></li><li data-list-text="[51]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;line-height: 9pt;text-align: justify;"><a name="bookmark111">&zwnj;</a>A<span class="s134">´ </span>. Huertas-Garc´ıa, A. Mart´ın, J. Huertas-Tato, and D. Camacho, “Ex- ploring Dimensionality Reduction Techniques in Multilingual Trans- formers,” <i>Cognitive Computation</i>, vol. 15, no. 2, pp. 590–612, 2023.</p></li><li data-list-text="[52]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark112">&zwnj;</a>D. Chen, L. O’Bray, and K. Borgwardt, “Structure-aware transformer for graph representation learning,” in <i>International Conference on Machine Learning</i>. PMLR, 2022, pp. 3469–3489.</p></li><li data-list-text="[53]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;">Q. Wu, W. Zhao, Z. Li, D. P. Wipf, and J. Yan, “Nodeformer: A scalable graph structure learning transformer for node classification,” <i>Advances in Neural Information Processing Systems</i>, vol. 35, pp. 27 387–27 401,</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark113">&zwnj;</a>2022.</p></li><li data-list-text="[54]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark114">&zwnj;</a>M. Pu, Y. Huang, Y. Liu, Q. Guan, and H. Ling, “Edter: Edge detection with transformer,” in <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 2022, pp. 1402–1412.</p></li><li data-list-text="[55]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark115">&zwnj;</a>N. Dhingra, F. Ritter, and A. Kunz, “Bgt-net: Bidirectional gru trans- former network for scene graph generation,” in <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 2021, pp. 2150–2159.</p></li><li data-list-text="[56]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark116">&zwnj;</a>K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y. Wang, “Transformer in transformer,” <i>Advances in Neural Information Processing Systems</i>, vol. 34, pp. 15 908–15 919, 2021.</p></li><li data-list-text="[57]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">T. Tambe, C. Hooper, L. Pentecost, T. Jia, E.-Y. Yang, M. Donato,</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;text-align: justify;"><a name="bookmark117">&zwnj;</a>V. Sanh, P. Whatmough, A. M. Rush, and D. Brooks, “Edgebert: Sentence-level energy optimizations for latency-aware multi-task nlp inference,” in <i>MICRO-54: 54th Annual IEEE/ACM International Sym- posium on Microarchitecture</i>, 2021, pp. 830–844.</p></li><li data-list-text="[58]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark118">&zwnj;</a>M. S. Hussain, M. J. Zaki, and D. Subramanian, “Global self-attention as a replacement for graph convolution,” in <i>Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</i>, 2022, pp. 655–665.</p></li><li data-list-text="[59]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark119">&zwnj;</a>P. Zhang, X. Dai, J. Yang, B. Xiao, L. Yuan, L. Zhang, and J. Gao, “Multi-scale vision longformer: A new vision transformer for high- resolution image encoding,” in <i>Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision</i>, 2021, pp. 2998–3008.</p></li><li data-list-text="[60]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark120">&zwnj;</a>Z. Hu, Y. Dong, K. Wang, K.-W. Chang, and Y. Sun, “Gpt-gnn: Generative pre-training of graph neural networks,” in <i>Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</i>, 2020, pp. 1857–1867.</p></li><li data-list-text="[61]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark121">&zwnj;</a>J. Yin, J. Shen, C. Guan, D. Zhou, and R. Yang, “Lidar-based online 3d video object detection with graph-based message passing and spatiotemporal transformer attention,” in <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 2020, pp. 11 495–11 504.</p></li><li data-list-text="[62]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark122">&zwnj;</a>D. Cai and W. Lam, “Graph transformer for graph-to-sequence learn- ing,” in <i>Proceedings of the AAAI Conference on Artificial Intelligence</i>, vol. 34, 2020, pp. 7464–7471.</p></li><li data-list-text="[63]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark123">&zwnj;</a>W. U. Ahmad, N. Peng, and K.-W. Chang, “GATE: Graph attention transformer encoder for cross-lingual relation and event extraction,” in <i>Proceedings of the AAAI Conference on Artificial Intelligence</i>, vol. 35, 2021, pp. 12 462–12 470.</p></li><li data-list-text="[64]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">L. Ma, C. Lin, D. Lim, A. Romero-Soriano, P. K. Dokania, M. Coates,</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;text-align: justify;"><a name="bookmark124">&zwnj;</a>P. Torr, and S.-N. Lim, “Graph Inductive Biases in Transformers without Message Passing,” <i>arXiv preprint arXiv:2305.17589</i>, 2023.</p></li><li data-list-text="[65]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark125">&zwnj;</a>F. Xia, X. Chen, S. Yu, M. Hou, M. Liu, and L. You, “Coupled attention networks for multivariate time series anomaly detection,” <i>IEEE Transactions on Emerging Topics in Computing</i>, 2023.</p></li><li data-list-text="[66]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;">D. Kreuzer, D. Beaini, W. Hamilton, V. Le´tourneau, and P. Tossou, “Rethinking graph transformers with spectral attention,” <i>Advances in Neural Information Processing Systems</i>, vol. 34, pp. 21 618–21 629,</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;line-height: 9pt;text-align: left;">2021.</p></li><li data-list-text="[67]"><p class="s7" style="padding-top: 7pt;padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark126">&zwnj;</a>T. Nguyen, X. Nguyen, S. R. Joty, and X. Li, “Differentiable window for dynamic local attention,” in <i>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020</i>, D. Jurafsky, J. Chai, N. Schluter, and J. R. Tetreault, Eds. Association for Computational Linguistics, 2020, pp. 6589–6599.<a name="bookmark127">&zwnj;</a></p></li><li data-list-text="[68]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;text-align: justify;"><a name="bookmark128">&zwnj;</a>C. Shen, “Twins: Revisiting the design of spatial attention in vision transformers,” <i>Advances in Neural Information Processing Systems</i>, vol. 34, pp. 9355–9366, 2021.</p></li><li data-list-text="[69]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark129">&zwnj;</a>M. S. Hussain, M. J. Zaki, and D. Subramanian, “Global self-attention as a replacement for graph convolution,” in <i>Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</i>, 2022, pp. 655–665.</p></li><li data-list-text="[70]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark130">&zwnj;</a>J. Lee, I. Lee, and J. Kang, “Self-attention graph pooling,” in <i>Inter- national Conference on Machine Learning</i>. PMLR, 2019, pp. 3734– 3743.</p></li><li data-list-text="[71]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;">D. Soydaner, “Attention mechanism in neural networks: Where it comes and where it goes,” <i>Neural Computing and Applications</i>, vol. 34, no. 16,</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark131">&zwnj;</a>pp. 13 371–13 385, 2022.</p></li><li data-list-text="[72]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark132">&zwnj;</a>I. Schwartz, S. Yu, T. Hazan, and A. G. Schwing, “Factor graph attention,” in <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 2019, pp. 2039–2048.</p></li><li data-list-text="[73]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;">Z. Wang, J. Chen, and H. Chen, “EGAT: Edge-featured graph attention network,” in <i>Artificial Neural Networks and Machine Learning–ICANN 2021: 30th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 14–17, 2021, Proceedings, Part I 30</i>.</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark133">&zwnj;</a>Springer, 2021, pp. 253–264.</p></li><li data-list-text="[74]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark134">&zwnj;</a>W. Zhang, Z. Yin, Z. Sheng, Y. Li, W. Ouyang, X. Li, Y. Tao, Z. Yang, and B. Cui, “Graph attention multi-layer perceptron,” in <i>Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</i>, 2022, pp. 4560–4570.</p></li><li data-list-text="[75]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark135">&zwnj;</a>Z. Niu, G. Zhong, and H. Yu, “A review on the attention mechanism of deep learning,” <i>Neurocomputing</i>, vol. 452, pp. 48–62, 2021.</p></li><li data-list-text="[76]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;line-height: 9pt;text-align: justify;">K. Choromanski, H. Lin, H. Chen, T. Zhang, A. Sehanobish,</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;text-align: justify;"><a name="bookmark136">&zwnj;</a>V. Likhosherstov, J. Parker-Holder, T. Sarlos, A. Weller, and T. Wein- garten, “From block-toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked transformers,” in <i>International Conference on Machine Learning</i>. PMLR, 2022, pp. 3962–3983.</p></li><li data-list-text="[77]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark137">&zwnj;</a>Y. Tay, M. Dehghani, D. Bahri, and D. Metzler, “Efficient transformers: A survey,” <i>ACM Computing Surveys</i>, vol. 55, no. 6, pp. 1–28, 2022.</p></li><li data-list-text="[78]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark138">&zwnj;</a>J. Liu, Z. Pan, H. He, J. Cai, and B. Zhuang, “Ecoformer: Energy- saving attention with linear complexity,” <i>Advances in Neural Informa- tion Processing Systems</i>, vol. 35, pp. 10 295–10 308, 2022.</p></li><li data-list-text="[79]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark139">&zwnj;</a>A. Moreno, Z. Wu, S. Nagesh, W. Dempsey, and J. M. Rehg, “Kernel Multimodal Continuous Attention,” <i>Advances in Neural Information Processing Systems</i>, vol. 35, pp. 18 046–18 059, 2022.</p></li><li data-list-text="[80]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark140">&zwnj;</a>A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret, “Transformers are rnns: Fast autoregressive transformers with linear attention,” in <i>International Conference on Machine Learning</i>.  PMLR, 2020, pp. 5156–5165.</p></li><li data-list-text="[81]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;">H. Lee, H. Choi, K. Sohn, and D. Min, “KNN local attention for image restoration,” in <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 2022, pp. 2139–2149.</p></li><li data-list-text="[82]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark141">&zwnj;</a>Y. Mei, Y. Fan, and Y. Zhou, “Image super-resolution with non-local sparse attention,” in <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 2021, pp. 3517–3526.</p></li><li data-list-text="[83]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark142">&zwnj;</a>G. Daras, N. Kitaev, A. Odena, and A. G. Dimakis, “Smyrf-efficient attention using asymmetric clustering,” <i>Advances in Neural Information Processing Systems</i>, vol. 33, pp. 6476–6489, 2020.</p></li><li data-list-text="[84]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;">N. Kitaev, L. Kaiser, and A. Levskaya, “Reformer: The efficient transformer,” in <i>8th International Conference on Learning Repre- sentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</i>.</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark143">&zwnj;</a>OpenReview.net, 2020.</p></li><li data-list-text="[85]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;">D. Soydaner, “Attention mechanism in neural networks: Where it comes and where it goes,” <i>Neural Computing and Applications</i>, vol. 34, no. 16,</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark144">&zwnj;</a>pp. 13 371–13 385, 2022.</p></li><li data-list-text="[86]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark145">&zwnj;</a>J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl, “Message passing neural networks,” <i>Machine learning meets quantum physics</i>, pp. 199–214, 2020.</p></li><li data-list-text="[87]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;"><a name="bookmark146">&zwnj;</a>C. Liu, Y. Sun, R. Davis, S. T. Cardona, and P. Hu, “ABT-MPNN: An atom-bond transformer-based message-passing neural network for molecular property prediction,” <i>Journal of Cheminformatics</i>, vol. 15, no. 1, p. 29, 2023.</p></li><li data-list-text="[88]"><p class="s7" style="padding-left: 30pt;text-indent: -18pt;text-align: justify;">H. Zhao, L. Jiang, J. Jia, P. H. Torr, and V. Koltun, “Point transformer,” in <i>Proceedings of the IEEE/CVF International Conference on Com- puter Vision</i>, 2021, pp. 16 259–16 268.</p></li><li data-list-text="[89]"><p class="s7" style="padding-top: 7pt;padding-left: 32pt;text-indent: -18pt;text-align: justify;"><a name="bookmark147">&zwnj;</a>W. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning on large graphs,” <i>Advances in Neural Information Processing Systems</i>, vol. 30, 2017.<a name="bookmark148">&zwnj;</a></p></li><li data-list-text="[90]"><p class="s7" style="padding-left: 32pt;text-indent: -18pt;text-align: justify;"><a name="bookmark149">&zwnj;</a>A. Javaloy, P. Sa´nchez-Mart´ın, A. Levi, and I. Valera, “Learnable graph convolutional attention networks,” in <i>The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</i>. OpenReview.net, 2023.</p></li><li data-list-text="[91]"><p class="s7" style="padding-left: 32pt;text-indent: -18pt;text-align: justify;"><a name="bookmark150">&zwnj;</a>L. Mou and X. X. Zhu, “Learning to pay attention on spectral domain: A spectral attention module-based convolutional network for hyperspectral image classification,” <i>IEEE Transactions on Geoscience and Remote Sensing</i>, vol. 58, no. 1, pp. 110–122, 2019.</p></li><li data-list-text="[92]"><p class="s7" style="padding-left: 32pt;text-indent: -18pt;text-align: justify;"><a name="bookmark151">&zwnj;</a>D. Bo, C. Shi, L. Wang, and R. Liao, “Specformer: Spectral graph neural networks meet transformers,” in <i>The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</i>. OpenReview.net, 2023.</p></li><li data-list-text="[93]"><p class="s7" style="padding-left: 32pt;text-indent: -18pt;text-align: justify;"><a name="bookmark152">&zwnj;</a>D. Q. Nguyen, T. D. Nguyen, and D. Phung, “Universal graph transformer self-attention networks,” in <i>Companion Proceedings of the Web Conference 2022</i>, 2022, pp. 193–196.</p></li><li data-list-text="[94]"><p class="s7" style="padding-left: 32pt;text-indent: -18pt;text-align: justify;"><a name="bookmark153">&zwnj;</a>H. Zhao, S. Ma, D. Zhang, Z. Deng, and F. Wei, “Are more layers beneficial to graph transformers?” in <i>The Eleventh International Con- ference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</i>. OpenReview.net, 2023.</p></li><li data-list-text="[95]"><p class="s7" style="padding-left: 32pt;text-indent: -18pt;text-align: justify;">S. Brody, U. Alon, and E. Yahav, “How attentive are graph attention networks?” in <i>The Tenth International Conference on Learning Rep- resentations, ICLR 2022, Virtual Event, April 25-29, 2022</i>. OpenRe-</p><p class="s7" style="padding-left: 32pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark154">&zwnj;</a>view.net, 2022.</p></li><li data-list-text="[96]"><p class="s7" style="padding-left: 32pt;text-indent: -18pt;text-align: justify;"><a name="bookmark155">&zwnj;</a>B. Heo, S. Yun, D. Han, S. Chun, J. Choe, and S. J. Oh, “Rethinking spatial dimensions of vision transformers,” in <i>Proceedings of the IEEE/CVF International Conference on Computer Vision</i>, 2021, pp. 11 936–11 945.</p></li><li data-list-text="[97]"><p class="s7" style="padding-left: 32pt;text-indent: -18pt;text-align: justify;"><a name="bookmark156">&zwnj;</a>F. Yu, K. Huang, M. Wang, Y. Cheng, W. Chu, and L. Cui, “Width &amp; depth pruning for vision transformers,” in <i>Proceedings of the AAAI Conference on Artificial Intelligence</i>, vol. 36, 2022, pp. 3143–3151.</p></li><li data-list-text="[98]"><p class="s7" style="padding-left: 32pt;text-indent: -18pt;text-align: justify;">H. Li, M. Wang, S. Liu, and P. Chen, “A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity,” in <i>The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</i>. Open-</p><p class="s7" style="padding-left: 32pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark157">&zwnj;</a>Review.net, 2023.</p></li><li data-list-text="[99]"><p class="s7" style="padding-left: 32pt;text-indent: -18pt;text-align: justify;"><a name="bookmark158">&zwnj;</a>R. Gubelmann and S. Handschuh, “Uncovering More Shallow Heuristics: Probing the Natural Language Inference Capacities of Transformer-Based Pre-Trained Language Models Using Syllogistic Patterns,” <i>arXiv preprint arXiv:2201.07614</i>, 2022.</p></li><li data-list-text="[100]"><p class="s7" style="padding-left: 32pt;text-indent: -22pt;text-align: justify;"><a name="bookmark159">&zwnj;</a>Y. Liu, S. Pan, Y. G. Wang, F. Xiong, L. Wang, Q. Chen, and V. C. Lee, “Anomaly Detection in Dynamic Graphs via Transformer,” <i>IEEE Transactions on Knowledge and Data Engineering</i>, pp. 1–1, 2021.</p></li><li data-list-text="[101]"><p class="s7" style="padding-left: 32pt;text-indent: -22pt;text-align: justify;"><a name="bookmark160">&zwnj;</a>J. Gao, J. Gao, X. Ying, M. Lu, and J. Wang, “Higher-order interaction goes neural: A substructure assembling graph attention network for graph classification,” <i>IEEE Transactions on Knowledge and Data Engineering</i>, 2021.</p></li><li data-list-text="[102]"><p class="s7" style="padding-left: 32pt;text-indent: -22pt;text-align: justify;"><a name="bookmark161">&zwnj;</a>J. Gong, Z. Teng, Q. Teng, H. Zhang, L. Du, S. Chen, M. Z. A. Bhuiyan, J. Li, M. Liu, and H. Ma, “Hierarchical graph transformer- based deep learning model for large-scale multi-label text classifica- tion,” <i>IEEE Access</i>, vol. 8, pp. 30 885–30 896, 2020.</p></li><li data-list-text="[103]"><p class="s7" style="padding-left: 32pt;text-indent: -22pt;text-align: justify;"><a name="bookmark162">&zwnj;</a>Y. Yang and D. P. Wipf, “Transformers from an optimization perspec- tive,” <i>Advances in Neural Information Processing Systems</i>, vol. 35, pp. 36 958–36 971, 2022.</p></li><li data-list-text="[104]"><p class="s7" style="padding-left: 32pt;text-indent: -22pt;text-align: justify;"><a name="bookmark163">&zwnj;</a>G. Li, C. Xiong, A. Thabet, and B. Ghanem, “Deepergcn: All you need to train deeper gcns,” <i>arXiv preprint arXiv:2006.07739</i>, 2020.</p></li><li data-list-text="[105]"><p class="s7" style="padding-left: 32pt;text-indent: -22pt;text-align: justify;">R. Koncel-Kedziorski, D. Bekal, Y. Luan, M. Lapata, and H. Hajishirzi, “Text generation from knowledge graphs with graph transformers,” in <i>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)</i>, J. Burstein, C. Doran, and</p><p class="s7" style="padding-left: 32pt;text-indent: 0pt;text-align: justify;"><a name="bookmark164">&zwnj;</a>T. Solorio, Eds. Association for Computational Linguistics, 2019, pp. 2284–2293.</p></li><li data-list-text="[106]"><p class="s7" style="padding-left: 32pt;text-indent: -22pt;text-align: justify;"><a name="bookmark165">&zwnj;</a>M. Xu, H. Wang, B. Ni, H. Guo, and J. Tang, “Self-supervised graph-level representation learning with local and global structure,” in <i>International Conference on Machine Learning</i>. PMLR, 2021, pp. 11 548–11 558.</p></li><li data-list-text="[107]"><p class="s7" style="padding-left: 32pt;text-indent: -22pt;text-align: justify;"><a name="bookmark166">&zwnj;</a>L. Bergen, T. O’Donnell, and D. Bahdanau, “Systematic generalization with edge transformers,” <i>Advances in Neural Information Processing Systems</i>, vol. 34, pp. 1390–1402, 2021.</p></li><li data-list-text="[108]"><p class="s7" style="padding-left: 32pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">E. O. Johnson, E. LaVerriere, E. Office, M. Stanley, E. Meyer,</p><p class="s7" style="padding-left: 32pt;text-indent: 0pt;text-align: justify;">T. Kawate, J. E. Gomez, R. E. Audette, N. Bandyopadhyay, N. Betan- court <i>et al.</i>, “Large-scale chemical–genetics yields new m. tuberculosis inhibitor classes,” <i>Nature</i>, vol. 571, no. 7763, pp. 72–78, 2019.</p></li><li data-list-text="[109]"><p class="s7" style="padding-top: 7pt;padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark167">&zwnj;</a>H. Wang, F. Guo, M. Du, G. Wang, and C. Cao, “A novel method for drug-target interaction prediction based on graph transformers model,” <i>BMC Bioinformatics</i>, vol. 23, no. 1, p. 459, Nov. 2022.<a name="bookmark168">&zwnj;</a></p></li><li data-list-text="[110]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark169">&zwnj;</a>J. Peng, J. Li, and X. Shang, “A learning-based method for drug-target interaction prediction based on feature representation learning and deep neural network,” <i>BMC Bioinformatics</i>, vol. 21, no. Suppl 13, p. 394, 2020.</p></li><li data-list-text="[111]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark170">&zwnj;</a>X. Dong, C. Long, W. Xu, and C. Xiao, “Dual Graph Convolutional Networks with Transformer and Curriculum Learning for Image Cap- tioning,” in <i>Proceedings of the 29th ACM International Conference on Multimedia</i>, ser. MM ’21.  New York, NY, USA: Association for Computing Machinery, Oct. 2021, pp. 2615–2624.</p></li><li data-list-text="[112]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark171">&zwnj;</a>R. Sortino, S. Palazzo, F. Rundo, and C. Spampinato, “Transformer- based image generation from scene graphs,” <i>Computer Vision and Image Understanding</i>, vol. 233, p. 103721, 2023.</p></li><li data-list-text="[113]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark172">&zwnj;</a>Y. Zhang, Y. Pan, T. Yao, R. Huang, T. Mei, and C.-W. Chen, “End- to-End Video Scene Graph Generation with Temporal Propagation Transformer,” <i>IEEE Transactions on Multimedia</i>, pp. 1–13, 2023.</p></li><li data-list-text="[114]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark173">&zwnj;</a>F. Xia, L. Wang, T. Tang, X. Chen, X. Kong, G. Oatley, and I. King, “Cengcn: Centralized convolutional networks with vertex imbalance for scale-free graphs,” <i>IEEE Transactions on Knowledge and Data Engineering</i>, vol. 35, no. 5, pp. 4555–4569, 2022.</p></li><li data-list-text="[115]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark174">&zwnj;</a>Q. Wu, C. Yang, W. Zhao, Y. He, D. Wipf, and J. Yan, “Difformer: Scalable (graph) transformers induced by energy constrained diffusion,” in <i>The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</i>. OpenReview.net, 2023.</p></li><li data-list-text="[116]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">W. Cong, Y. Wu, Y. Tian, M. Gu, Y. Xia, C.-c. J. Chen, and</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;text-align: justify;"><a name="bookmark175">&zwnj;</a>M. Mahdavi, “DyFormer: A Scalable Dynamic Graph Transformer with Provable Benefits on Generalization Ability,” in <i>Proceedings of the 2023 SIAM International Conference on Data Mining (SDM)</i>. SIAM, 2023, pp. 442–450.</p></li><li data-list-text="[117]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark176">&zwnj;</a>X. He, B. Hooi, T. Laurent, A. Perold, Y. LeCun, and X. Bresson, “A generalization of vit/mlp-mixer to graphs,” in <i>International Conference on Machine Learning</i>. PMLR, 2023, pp. 12 724–12 745.</p></li><li data-list-text="[118]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">C. Vignac, I. Krawczuk, A. Siraudin, B. Wang, V. Cevher, and</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;text-align: justify;"><a name="bookmark177">&zwnj;</a>P. Frossard, “Digress: Discrete denoising diffusion for graph gener- ation,” in <i>The Eleventh International Conference on Learning Repre- sentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</i>, 2023.</p></li><li data-list-text="[119]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">H. Shirzad, A. Velingker, B. Venkatachalam, D. J. Sutherland, and</p><ol id="l21"><li data-list-text="A."><p class="s7" style="padding-left: 30pt;text-indent: 0pt;text-align: justify;"><a name="bookmark178">&zwnj;</a>K. Sinop, “Exphormer: Sparse transformers for graphs,” in <i>Interna- tional Conference on Machine Learning</i>. PMLR, 2023, pp. 31 613– 31 632.</p></li></ol></li><li data-list-text="[120]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark179">&zwnj;</a>M. S. Hussain, M. J. Zaki, and D. Subramanian, “Global self-attention as a replacement for graph convolution,” in <i>Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</i>, 2022, pp. 655–665.</p></li><li data-list-text="[121]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">L. Ma, C. Lin, D. Lim, A. Romero-Soriano, P. K. Dokania, M. Coates,</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;text-align: justify;"><a name="bookmark180">&zwnj;</a>P. Torr, and S.-N. Lim, “Graph inductive biases in transformers without message passing,” in <i>International Conference on Machine Learning</i>, 2023, pp. 23 321–23 337.</p></li><li data-list-text="[122]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;">W. Zhu, T. Wen, G. Song, X. Ma, and L. Wang, “Hierarchical transformer for scalable graph learning,” in <i>Proceedings of the Thirty- Second International Joint Conference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao, SAR, China</i>, 2023, pp. 4702–</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark181">&zwnj;</a>4710.</p></li><li data-list-text="[123]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark182">&zwnj;</a>J. Kim, S. Oh, and S. Hong, “Transformers generalize deepsets and can be extended to graphs &amp; hypergraphs,” <i>Advances in Neural Information Processing Systems</i>, vol. 34, pp. 28 016–28 028, 2021.</p></li><li data-list-text="[124]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark183">&zwnj;</a>D. Yao, Y. Gu, G. Cong, H. Jin, and X. Lv, “Entity resolution with hierarchical graph attention networks,” in <i>Proceedings of the 2022 International Conference on Management of Data</i>, 2022, pp. 429–442.</p></li><li data-list-text="[125]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark184">&zwnj;</a>P. Balsebre, D. Yao, G. Cong, and Z. Hai, “Geospatial entity reso- lution,” in <i>Proceedings of the ACM Web Conference 2022</i>, 2022, pp. 3061–3070.</p></li><li data-list-text="[126]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;">Z. Chen, D. Chen, X. Zhang, Z. Yuan, and X. Cheng, “Learning Graph Structures with Transformer for Multivariate Time Series Anomaly Detection in IoT,” <i>IEEE Internet of Things Journal</i>, vol. 9, no. 12,</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark185">&zwnj;</a>pp. 9179–9189, Jun. 2022.</p></li><li data-list-text="[127]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark186">&zwnj;</a>T. Chu, T. T. Nguyen, B. D. Hai, Q. H. Nguyen, and T. Nguyen, “Graph Transformer for Drug Response Prediction,” <i>IEEE/ACM Transactions on Computational Biology and Bioinformatics</i>, vol. 20, no. 2, pp. 1065– 1072, Mar. 2023.</p></li><li data-list-text="[128]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;">Y. Lu, K. Zeng, Q. Zhang, J. Zhang, L. Cai, and J. Tian, “A simple and efficient graph Transformer architecture for molecular properties prediction,” <i>Chemical Engineering Science</i>, vol. 280, p. 119057, Oct. 2023.</p></li><li data-list-text="[129]"><p class="s7" style="padding-top: 7pt;padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark187">&zwnj;</a>S. Han, J. Liu, J. Wu, Y. Chen, and L. Tao, “Transforming Graphs for Enhanced Attribute-Based Clustering: An Innovative Graph Trans- former Method,” <i>arXiv preprint arXiv:2306.11307</i>, 2023.<a name="bookmark188">&zwnj;</a></p></li><li data-list-text="[130]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark189">&zwnj;</a>Z. Zhang, Q. Liu, Q. Hu, and C.-K. Lee, “Hierarchical graph trans- former with adaptive node sampling,” <i>Advances in Neural Information Processing Systems</i>, vol. 35, pp. 21 171–21 183, 2022.</p></li><li data-list-text="[131]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark190">&zwnj;</a>Y. Cong, M. Y. Yang, and B. Rosenhahn, “Reltr: Relation transformer for scene graph generation,” <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 2023.</p></li><li data-list-text="[132]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark191">&zwnj;</a>S. Mo and M. Xin, “BSTG-Trans: A Bayesian Spatial-Temporal Graph Transformer for Long-term Pose Forecasting,” <i>IEEE Transactions on Multimedia</i>, pp. 1–13, 2023.</p></li><li data-list-text="[133]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark192">&zwnj;</a>W. Li, X. Xiao, J. Liu, H. Wu, H. Wang, and J. Du, “Leveraging graph to improve abstractive multi-document summarization,” in <i>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</i>, 2020, pp. 6232–6243.</p></li><li data-list-text="[134]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark193">&zwnj;</a>B. He, D. Zhou, J. Xiao, X. Jiang, Q. Liu, N. J. Yuan, and T. Xu, “Bert-mk: Integrating graph contextualized knowledge into pre-trained language models,” in <i>Findings of the Association for Computational Linguistics: EMNLP</i>, 2020, pp. 2281–2290.</p></li><li data-list-text="[135]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark194">&zwnj;</a>J. Shang, T. Ma, C. Xiao, and J. Sun, “Pre-training of graph augmented transformers for medication recommendation,” in <i>28th International Joint Conference on Artificial Intelligence (IJCAI)</i>, 2019, pp. 5953– 5959.</p></li><li data-list-text="[136]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">J. Li, B. Hui, R. Cheng, B. Qin, C. Ma, N. Huo, F. Huang, W. Du,</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;text-align: justify;">L. Si, and Y. Li, “Graphix-t5: Mixing pre-trained transformers with graph-aware layers for text-to-sql parsing,” in <i>Proceedings of the AAAI Conference on Artificial Intelligence</i>, vol. 37, no. 11, 2023, pp. 13 076–</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark195">&zwnj;</a>13 084.</p></li><li data-list-text="[137]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark196">&zwnj;</a>Y. Rong, Y. Bian, T. Xu, W. Xie, Y. Wei, W. Huang, and J. Huang, “Self-supervised graph transformer on large-scale molecular data,” <i>Advances in Neural Information Processing Systems</i>, vol. 33, pp. 12 559–12 571, 2020.</p></li><li data-list-text="[138]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark197">&zwnj;</a>S. Chithrananda, G. Grand, and B. Ramsundar, “ChemBERTa: Large- scale self-supervised pretraining for molecular property prediction,” <i>arXiv preprint arXiv:2010.09885</i>, 2020.</p></li><li data-list-text="[139]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark198">&zwnj;</a>J. Chen, Y. Qian, and Y. Furukawa, “HEAT: Holistic Edge Attention Transformer for Structured Reconstruction,” in <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 2022, pp. 3866–3875.</p></li><li data-list-text="[140]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">C.-Y. Lee, C.-L. Li, H. Zhang, T. Dozat, V. Perot, G. Su, X. Zhang,</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;text-align: justify;">K. Sohn, N. Glushnev, R. Wang <i>et al.</i>, “Formnetv2: Multimodal graph contrastive learning for form document information extraction,” in <i>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</i>, 2023, pp. 9011–</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark199">&zwnj;</a>9026.</p></li><li data-list-text="[141]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark200">&zwnj;</a>L. Wan, Z. Fu, L. Sun, X. Wang, G. Xu, X. Yan, and F. Xia, “Self- supervised teaching and learning of representations on graphs,” in <i>Proceedings of the ACM Web Conference 2023</i>, 2023, pp. 489–498.</p></li><li data-list-text="[142]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark201">&zwnj;</a>S. Casola, I. Lauriola, and A. Lavelli, “Pre-trained transformers: An empirical comparison,” <i>Machine Learning with Applications</i>, vol. 9, p. 100334, 2022.</p></li><li data-list-text="[143]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark202">&zwnj;</a>J. Xia, J. Zheng, C. Tan, G. Wang, and S. Z. Li, “Towards effective and generalizable fine-tuning for pre-trained molecular graph models,” <i>bioRxiv</i>, pp. 2022–02, 2022.</p></li><li data-list-text="[144]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark203">&zwnj;</a>H. Li, D. Zhao, and J. Zeng, “KPGT: Knowledge-guided pre-training of graph transformer for molecular property prediction,” in <i>Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</i>, 2022, pp. 857–867.</p></li><li data-list-text="[145]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">W. Zhang, Y. Zhu, M. Chen, Y. Geng, Y. Huang, Y. Xu, W. Song, and</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;text-align: justify;"><a name="bookmark204">&zwnj;</a>H. Chen, “Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer,” in <i>Proceedings of the ACM Web Conference 2023</i>, 2023, pp. 2581–2590.</p></li><li data-list-text="[146]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark205">&zwnj;</a>Y. Wei, W. Liu, F. Liu, X. Wang, L. Nie, and T.-S. Chua, “LightGT: A Light Graph Transformer for Multimedia Recommendation,” in <i>Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</i>, 2023, pp. 1508– 1517.</p></li><li data-list-text="[147]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark206">&zwnj;</a>J. Xiao, P. Zhou, A. Yao, Y. Li, R. Hong, S. Yan, and T.-S. Chua, “Contrastive video question answering via video graph transformer,” <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 2023.</p></li><li data-list-text="[148]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark207">&zwnj;</a>A. Gui, J. Ye, and H. Xiao, “G-adapter: Towards structure-aware parameter-efficient transfer learning for graph transformer networks,” <i>arXiv preprint arXiv:2305.10329</i>, 2023.</p></li><li data-list-text="[149]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">X. Liu, S. Zhao, K. Su, Y. Cen, J. Qiu, M. Zhang, W. Wu, Y. Dong, and</p></li></ol><p class="s7" style="padding-left: 30pt;text-indent: 0pt;text-align: justify;">J. Tang, “Mask and reason: Pre-training knowledge graph transformers for complex logical queries,” in <i>Proceedings of the 28th ACM SIGKDD</i></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:5.464pt" cellspacing="0"><tr style="height:10pt"><td style="width:260pt"><p class="s135" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">Conference on Knowledge Discovery and Data Mining<span class="s136">, 2022, pp.</span></p></td><td style="width:25pt"><p class="s136" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;"><a name="bookmark208">&zwnj;</a>[172]</p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">S. Liu, Y. Wang, Y. Deng, L. He, B. Shao, J. Yin, N. Zheng, T.-Y.</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark209">&zwnj;</a>1120–1130.</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Liu, and T. Wang, “Improved drug–target interaction prediction with</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[150] C. Pellegrini, N. Navab, and A. Kazi, “Unsupervised pre-training</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">intermolecular graph transformer,” <i>Briefings in Bioinformatics</i>, vol. 23,</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">of graph transformers on patient population graphs,” <i>Medical Image</i></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark210">&zwnj;</a>no. 5, p. bbac162, 2022.</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s135" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark211">&zwnj;</a>Analysis<span class="s136">, vol. 89, p. 102895, 2023.</span></p></td><td style="width:25pt"><p class="s136" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">[173]</p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">M. H. Al-Rabeah and A. Lakizadeh, “Prediction of drug-drug inter-</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[151] J. Xiao, P. Zhou, T.-S. Chua, and S. Yan, “Video graph transformer</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">action events using graph neural networks based feature extraction,”</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">for video question answering,” in <i>European Conference on Computer</i></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s135" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark212">&zwnj;</a>Scientific Reports<span class="s136">, vol. 12, no. 1, p. 15590, 2022.</span></p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s135" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark213">&zwnj;</a>Vision<span class="s136">. Springer, 2022, pp. 39–58.</span></p></td><td style="width:25pt"><p class="s136" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">[174]</p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">K. Sun, S. Yu, C. Peng, X. Li, M. Naseriparsa, and F. Xia, “Abnormal</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[152] Z. Tan, R. Guo, K. Ding, and H. Liu, “Virtual node tuning for few-</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">entity-aware knowledge graph completion,” in <i>2022 IEEE International</i></p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">shot node classification,” in <i>Proceedings of the 29th ACM SIGKDD</i></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s135" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Conference on Data Mining Workshops (ICDMW)<span class="s136">. IEEE, 2022, pp.</span></p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s135" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">Conference on Knowledge Discovery and Data Mining<span class="s136">, 2023, pp.</span></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark214">&zwnj;</a>891–900.</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark215">&zwnj;</a>2177–2188.</p></td><td style="width:25pt"><p class="s136" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">[175]</p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">S. Chen, H. Cheng, X. Liu, J. Jiao, Y. Ji, and J. Gao, “Pre-</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[153] L. Waikhom and R. Patgiri, “A survey of graph neural networks in</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">training transformers for knowledge graph completion,” <i>arXiv preprint</i></p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">various learning paradigms: Methods, applications, and challenges,”</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s135" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark216">&zwnj;</a>arXiv:2303.15682<span class="s136">, 2023.</span></p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s135" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark217">&zwnj;</a>Artificial Intelligence Review<span class="s136">, vol. 56, no. 7, pp. 6295–6364, 2023.</span></p></td><td style="width:25pt"><p class="s136" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">[176]</p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">X. Liu, S. Mao, X. Wang, and J. Bu, “Generative Transformer with</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[154] K. Tunyasuvunakool, “The prospects and opportunities of protein</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Knowledge-Guided Decoding for Academic Knowledge Graph Com-</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">structure prediction with AI,” <i>Nature Reviews Molecular Cell Biology</i>,</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark218">&zwnj;</a>pletion,” <i>Mathematics</i>, vol. 11, no. 5, p. 1073, 2023.</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark219">&zwnj;</a>vol. 23, no. 7, pp. 445–446, 2022.</p></td><td style="width:25pt"><p class="s136" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">[177]</p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">X. Chen, N. Zhang, L. Li, S. Deng, C. Tan, C. Xu, F. Huang,</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[155] Z. Gu, X. Luo, J. Chen, M. Deng, and L. Lai, “Hierarchical graph</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">L. Si, and H. Chen, “Hybrid Transformer with Multi-level Fusion for</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">transformer with contrastive learning for protein function prediction,”</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Multimodal Knowledge Graph Completion,” in <i>Proceedings of the 45th</i></p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s135" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark220">&zwnj;</a>Bioinformatics<span class="s136">, vol. 39, no. 7, p. btad410, 2023.</span></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s135" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">International ACM SIGIR Conference on Research and Development in</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[156] A. Pepe, J. Lasenby, and P. Chaco´n, “Using a Graph Transformer</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s135" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark221">&zwnj;</a>Information Retrieval<span class="s136">. Madrid Spain: ACM, Jul. 2022, pp. 904–915.</span></p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">Network to Predict 3D Coordinates of Proteins via Geometric Algebra</p></td><td style="width:25pt"><p class="s136" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">[178]</p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">J. Yu, H. Yin, X. Xia, T. Chen, J. Li, and Z. Huang, “Self-supervised</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">Modelling,” in <i>Empowering Novel Geometric Algebra for Graphics and</i></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">learning for recommender systems: A survey,” <i>IEEE Transactions on</i></p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s135" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">Engineering<span class="s136">, E. Hitzer, G. Papagiannakis, and P. Vasik, Eds. Cham:</span></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s135" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark222">&zwnj;</a>Knowledge and Data Engineering<span class="s136">, 2023.</span></p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark223">&zwnj;</a>Springer Nature Switzerland, 2023, vol. 13862, pp. 83–95.</p></td><td style="width:25pt"><p class="s136" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">[179]</p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">L. Xia, C. Huang, and C. Zhang, “Self-Supervised Hypergraph Trans-</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[157] X. Chen, A. Morehead, J. Liu, and J. Cheng, “Dproq: a gated-graph</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">former for Recommender Systems,” in <i>Proceedings of the 28th ACM</i></p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">transformer for protein complex structure assessment,” <i>arXiv preprint</i></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s135" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">SIGKDD Conference on Knowledge Discovery and Data Mining<span class="s136">.</span></p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s135" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark224">&zwnj;</a>arXiv:2205.10627<span class="s136">, 2022.</span></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark225">&zwnj;</a>Washington DC USA: ACM, Aug. 2022, pp. 2100–2109.</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[158] ——, “A gated graph transformer for protein complex structure quality</p></td><td style="width:25pt"><p class="s136" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">[180]</p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">C. Li, L. Xia, X. Ren, Y. Ye, Y. Xu, and C. Huang, “Graph Trans-</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">assessment and its performance in CASP15,” <i>Bioinformatics</i>, vol. 39,</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">former for Recommendation,” in <i>Proceedings of the 46th International</i></p></td></tr><tr style="height:9pt"><td style="width:260pt"><p style="text-indent: 0pt;text-align: left;"/><p class="s136" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark226">&zwnj;</a>no. Supplement 1, pp. i308–i317, 2023.</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s135" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">ACM SIGIR Conference on Research and Development in Information</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[159] Y. Lin, H. Wang, J. Li, and H. Gao, “Efficient Entity Resolution on</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s135" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark227">&zwnj;</a>Retrieval<span class="s136">, Jul. 2023, pp. 1680–1689.</span></p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">Heterogeneous Records,” <i>IEEE Transactions on Knowledge and Data</i></p></td><td style="width:25pt"><p class="s136" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">[181]</p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">J. Shen and C. A. Nicolaou, “Molecular property prediction: Recent</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s135" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark228">&zwnj;</a>Engineering<span class="s136">, vol. 32, no. 5, pp. 912–926, May 2020.</span></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">trends in the era of artificial intelligence,” <i>Drug Discovery Today:</i></p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[160] D. Yao, Y. Gu, G. Cong, H. Jin, and X. Lv, “Entity Resolution with</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s135" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark229">&zwnj;</a>Technologies<span class="s136">, vol. 32, pp. 29–36, 2019.</span></p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">Hierarchical Graph Attention Networks,” in <i>Proceedings of the 2022</i></p></td><td style="width:25pt"><p class="s136" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">[182]</p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">D. Chen, K. Gao, D. D. Nguyen, X. Chen, Y. Jiang, G.-W. Wei,</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s135" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">International Conference on Management of Data<span class="s136">. Philadelphia PA</span></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">and F. Pan, “Algebraic graph-assisted bidirectional transformers for</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark230">&zwnj;</a>USA: ACM, Jun. 2022, pp. 429–442.</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">molecular property prediction,” <i>Nature communications</i>, vol. 12, no. 1,</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[161] C. Ying, T. Cai, S. Luo, S. Zheng, G. Ke, D. He, Y. Shen, and T.-Y.</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark231">&zwnj;</a>p. 3521, 2021.</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">Liu, “Do transformers really perform badly for graph representation?”</p></td><td style="width:25pt"><p class="s136" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">[183]</p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">D. Buterez, J. P. Janet, S. J. Kiddle, D. Oglic, and P. Lio´, “Transfer</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s135" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">Advances in Neural Information Processing Systems<span class="s136">, vol. 34, pp.</span></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">learning with graph neural networks for improved molecular prop-</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark232">&zwnj;</a>28 877–28 888, 2021.</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">erty prediction in the multi-fidelity setting,” <i>Nature Communications</i>,</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[162] W. Dou, D. Shen, T. Nie, Y. Kou, C. Sun, H. Cui, and G. Yu,</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark233">&zwnj;</a>vol. 15, no. 1, p. 1517, 2024.</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">“Empowering transformer with hybrid matching knowledge for entity</p></td><td style="width:25pt"><p class="s136" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">[184]</p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">S. E. Schaeffer, “Graph clustering,” <i>Computer science review</i>, vol. 1,</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">matching,” in <i>International Conference on Database Systems for Ad-</i></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark234">&zwnj;</a>no. 1, pp. 27–64, 2007.</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s135" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark235">&zwnj;</a>vanced Applications<span class="s136">, 2022, pp. 52–67.</span></p></td><td style="width:25pt"><p class="s136" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">[185]</p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">S. Yun, M. Jeong, R. Kim, J. Kang, and H. J. Kim, “Graph transformer</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[163] M. Jin, H. Y. Koh, Q. Wen, D. Zambon, C. Alippi, G. I. Webb,</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">networks,” <i>Advances in neural information processing systems</i>, vol. 32,</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">I. King, and S. Pan, “A survey on graph neural networks for time</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark236">&zwnj;</a>2019.</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">series: Forecasting, classification, imputation, and anomaly detection,”</p></td><td style="width:25pt"><p class="s136" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">[186]</p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">H. Gao, X. Han, J. Huang, J.-X. Wang, and L. Liu, “PatchGT:</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s135" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark237">&zwnj;</a>arXiv preprint arXiv:2307.03759<span class="s136">, 2023.</span></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Transformer over Non-trainable Clusters for Learning Graph Repre-</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[164] Y. Liu, S. Pan, Y. G. Wang, F. Xiong, L. Wang, Q. Chen, and V. C.</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">sentations,” in <i>Learning on Graphs Conference</i>. PMLR, 2022, pp.</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">Lee, “Anomaly detection in dynamic graphs via transformer,” <i>IEEE</i></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark238">&zwnj;</a>27–1.</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s135" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark239">&zwnj;</a>Transactions on Knowledge and Data Engineering<span class="s136">, 2021.</span></p></td><td style="width:25pt"><p class="s136" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">[187]</p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">X. Ma, Q. Chen, Y. Wu, G. Song, L. Wang, and B. Zheng, “Rethinking</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[165] J. Xu, H. Wu, J. Wang, and M. Long, “Anomaly transformer: Time</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Structural Encodings: Adaptive Graph Transformer for Node Classifi-</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">series anomaly detection with association discrepancy,” in <i>The Tenth</i></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">cation Task,” in <i>Proceedings of the ACM Web Conference 2023</i>. Austin</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s135" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">International Conference on Learning Representations, ICLR 2022,</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark240">&zwnj;</a>TX USA: ACM, Apr. 2023, pp. 533–544.</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s135" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark241">&zwnj;</a>Virtual Event, April 25-29, 2022<span class="s136">. OpenReview.net, 2022.</span></p></td><td style="width:25pt"><p class="s136" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">[188]</p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">B. Jiang, F. Xu, Z. Zhang, J. Tang, and F. Nie, “Agformer: Efficient</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[166] Z. Chen, D. Chen, X. Zhang, Z. Yuan, and X. Cheng, “Learning</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">graph representation with anchor-graph transformer,” <i>arXiv preprint</i></p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">graph structures with transformer for multivariate time-series anomaly</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s135" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark242">&zwnj;</a>arXiv:2305.07521<span class="s136">, 2023.</span></p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">detection in IoT,” <i>IEEE Internet of Things Journal</i>, vol. 9, no. 12, pp.</p></td><td style="width:25pt"><p class="s136" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">[189]</p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">C. Chen, Y. Wu, Q. Dai, H.-Y. Zhou, M. Xu, S. Yang, X. Han, and</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark243">&zwnj;</a>9179–9189, 2021.</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Y. Yu, “A survey on graph neural networks and graph transform-</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[167] S. Tuli, G. Casale, and N. R. Jennings, “Tranad: Deep transformer</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">ers in computer vision: A task-oriented perspective,” <i>arXiv preprint</i></p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">networks for anomaly detection in multivariate time series data,” <i>Proc.</i></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s135" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark244">&zwnj;</a>arXiv:2209.13232<span class="s136">, 2022.</span></p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s135" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark245">&zwnj;</a>VLDB Endow.<span class="s136">, vol. 15, no. 6, pp. 1201–1214, 2022.</span></p></td><td style="width:25pt"><p class="s136" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">[190]</p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">W. S. El-Kassas, C. R. Salama, A. A. Rafea, and H. K. Mohamed,</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[168] P. Joshi, M. Hasanuzzaman, C. Thapa, H. Afli, and T. Scully, “Enabling</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">“Automatic text summarization: A comprehensive survey,” <i>Expert</i></p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark246">&zwnj;</a>all in-edge deep learning: A literature review,” <i>IEEE Access</i>, 2023.</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s135" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark247">&zwnj;</a>systems with applications<span class="s136">, vol. 165, p. 113679, 2021.</span></p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[169] B. Shi, Y. Wang, F. Guo, J. Shao, H. Shen, and X. Cheng, “Opengda:</p></td><td style="width:25pt"><p class="s136" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">[191]</p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">A. Gupta, D. Chugh, Anjum, and R. Katarya, “Automated News Sum-</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">Graph domain adaptation benchmark for cross-network learning,” in</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">marization Using Transformers,” in <i>Sustainable Advanced Computing</i>,</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s135" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">Proceedings of the 32nd ACM International Conference on Information</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">S. Aurelia, S. S. Hiremath, K. Subramanian, and S. K. Biswas, Eds.</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s135" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark248">&zwnj;</a>and Knowledge Management<span class="s136">, 2023, pp. 5396–5400.</span></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark249">&zwnj;</a>Singapore: Springer Singapore, 2022, vol. 840, pp. 249–259.</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[170] M. Hauben, “Artificial Intelligence and Data Mining for the Pharma-</p></td><td style="width:25pt"><p class="s136" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">[192]</p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">B. Yang, X. Luo, K. Sun, and M. Y. Luo, “Recent Progress on Text</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark250">&zwnj;</a>covigilance of Drug–Drug Interactions,” <i>Clinical Therapeutics</i>, 2023.</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Summarisation Based on BERT and GPT,” in <i>Knowledge Science,</i></p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[171] W. E. Djeddi, K. Hermi, S. Ben Yahia, and G. Diallo, “Advancing drug–</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s135" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Engineering and Management<span class="s136">. Cham: Springer Nature Switzerland,</span></p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">target interaction prediction: A comprehensive graph-based approach</p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark251">&zwnj;</a>2023, vol. 14120, pp. 225–241.</p></td></tr><tr style="height:9pt"><td style="width:260pt"><p class="s136" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">integrating knowledge graph embedding and ProtBert pretraining,”</p></td><td style="width:25pt"><p class="s136" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">[193]</p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">S. Kumar and A. Solanki, “An abstractive text summarization tech-</p></td></tr><tr style="height:10pt"><td style="width:260pt"><p class="s135" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;">BMC Bioinformatics<span class="s136">, vol. 24, no. 1, p. 488, Dec. 2023.</span></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:234pt"><p class="s136" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">nique using transformer model with self-attention mechanism,” <i>Neural</i></p></td></tr></table><p class="s133" style="padding-top: 6pt;padding-left: 30pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Computing and Applications<span class="s7">, vol. 35, no. 25, pp. 18 603–18 622, Sep.</span></p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark252">&zwnj;</a>2023.</p><ol id="l22"><li data-list-text="[194]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark253">&zwnj;</a>M. Z. Hossain, F. Sohel, M. F. Shiratuddin, and H. Laga, “A compre- hensive survey of deep learning for image captioning,” <i>ACM Computing Surveys (CsUR)</i>, vol. 51, no. 6, pp. 1–36, 2019.</p></li><li data-list-text="[195]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">X. Yang, J. Peng, Z. Wang, H. Xu, Q. Ye, C. Li, S. Huang, F. Huang,</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;text-align: justify;">Z. Li, and Y. Zhang, “Transforming Visual Scene Graphs to Image Captions,” in <i>Proceedings of the 61st Annual Meeting of the Associ- ation for Computational Linguistics (Volume 1: Long Papers)</i>, 2023,</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark254">&zwnj;</a>pp. 12 427–12 440.</p></li><li data-list-text="[196]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark255">&zwnj;</a>Y. Wang, J. Xu, and Y. Sun, “End-to-end transformer based model for image captioning,” in <i>Proceedings of the AAAI Conference on Artificial Intelligence</i>, vol. 36, 2022, pp. 2585–2594.</p></li><li data-list-text="[197]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><a name="bookmark256">&zwnj;</a>M. Z<span class="s134">˙ </span>elaszczyk and J. Man´dziuk, “Text-to-image cross-modal genera- tion: A systematic review,” <i>arXiv preprint arXiv:2401.11631</i>, 2024.</p></li><li data-list-text="[198]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;line-height: 9pt;text-align: justify;">B. Zhang, S. Gu, B. Zhang, J. Bao, D. Chen, F. Wen, Y. Wang, and</p><ol id="l23"><li data-list-text="B."><p class="s7" style="padding-left: 30pt;text-indent: 0pt;text-align: justify;"><a name="bookmark257">&zwnj;</a>Guo, “Styleswin: Transformer-based gan for high-resolution image generation,” in <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 2022, pp. 11 304–11 314.</p></li></ol></li><li data-list-text="[199]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark258">&zwnj;</a>Y. Cong, M. Y. Yang, and B. Rosenhahn, “Reltr: Relation transformer for scene graph generation,” <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 2023.</p></li><li data-list-text="[200]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark259">&zwnj;</a>R. Li, S. Zhang, and X. He, “Sgtr: End-to-end scene graph generation with transformer,” in <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 2022, pp. 19 486–19 496.</p></li><li data-list-text="[201]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;">J. Xiao, P. Zhou, T.-S. Chua, and S. Yan, “Video Graph Transformer for Video Question Answering,” in <i>Computer Vision – ECCV 2022</i>, ser. Lecture Notes in Computer Science, S. Avidan, G. Brostow, M. Cisse´,</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;text-align: justify;"><a name="bookmark260">&zwnj;</a>G. M. Farinella, and T. Hassner, Eds.  Cham: Springer Nature Switzerland, 2022, pp. 39–58.</p></li><li data-list-text="[202]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark261">&zwnj;</a>Y.-F. Wu, J. Yoon, and S. Ahn, “Generative video transformer: Can ob- jects be the words?” in <i>International Conference on Machine Learning</i>. PMLR, 2021, pp. 11 307–11 318.</p></li><li data-list-text="[203]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark262">&zwnj;</a>W. Yan, Y. Zhang, P. Abbeel, and A. Srinivas, “Videogpt: Video gener- ation using vq-vae and transformers,” <i>arXiv preprint arXiv:2104.10157</i>, 2021.</p></li><li data-list-text="[204]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;">S. Tulyakov, M.-Y. Liu, X. Yang, and J. Kautz, “Mocogan: Decompos- ing motion and content for video generation,” in <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, 2018,</p><p class="s7" style="padding-left: 30pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a name="bookmark263">&zwnj;</a>pp. 1526–1535.</p></li><li data-list-text="[205]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark264">&zwnj;</a>K. Sun, F. Xia, J. Liu, B. Xu, V. Saikrishna, and C. C. Aggarwal, “At- tributed graph force learning,” <i>IEEE Transactions on Neural Networks and Learning Systems</i>, 2022.</p></li><li data-list-text="[206]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;"><a name="bookmark265">&zwnj;</a>F. G. Febrinanto, F. Xia, K. Moore, C. Thapa, and C. Aggarwal, “Graph lifelong learning: A survey,” <i>IEEE Computational Intelligence Magazine</i>, vol. 18, no. 1, pp. 32–51, 2023.</p></li><li data-list-text="[207]"><p class="s7" style="padding-left: 30pt;text-indent: -22pt;text-align: justify;">S. Yu, H. Huang, M. N. Dao, and F. Xia, “Graph augmentation learning,” in <i>Companion Proceedings of the Web Conference 2022</i>, 2022, pp. 1063–1072.</p></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="91" height="120" src="data:image/jpg;base64,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"/></td></tr></table></span></p><h4 style="padding-left: 90pt;text-indent: 0pt;text-align: justify;"><a name="bookmark266">&zwnj;</a>Ahsan Shehzad <span class="s7">received the B.S. degree in com- puter science from BZU, Multan, Pakistan, in 2015, and the M.S. degree in computer science from Air University, Islamabad, Pakistan, in 2018. He is currently pursuing the Ph.D. degree in software engineering at Dalian University of Technology, China. His current research interests include artificial intelligence, graph learning, health informatics, and brain science.</span><a name="bookmark267">&zwnj;</a></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="96" height="128" src="data:image/jpg;base64,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"/></td></tr></table></span></p><h4 style="padding-left: 90pt;text-indent: 0pt;text-align: justify;"><a name="bookmark268">&zwnj;</a>Feng Xia <span class="s7">(Senior Member, IEEE) received the BSc and PhD degrees from Zhejiang University, Hangzhou, China. He is a Professor in School of Computing Technologies, RMIT University, Aus- tralia. Dr. Xia has published over 300 scientific papers in journals and conferences (such as IEEE TAI, TKDE, TNNLS, ACM TKDD, TIST, TOMM;</span></h4><p class="s7" style="padding-left: 90pt;text-indent: 0pt;text-align: justify;">IJCAI, AAAI, ICLR, and WWW). His research in- terests include artificial intelligence, graph learning, brain science, digital health, and robotics. He is a Senior Member of IEEE and ACM, and an ACM</p><h4 style="padding-top: 7pt;padding-left: 90pt;text-indent: 0pt;text-align: justify;"><a name="bookmark269">&zwnj;</a>Shagufta Abid <span class="s7">received the B.S. degree in computer science from IUB, Bahawalpur, Pakistan, in 2015, and the M.S. degree in computer science from Air University, Islamabad, Pakistan, in 2019. She is currently pursuing the Ph.D. degree in software en- gineering at Dalian University of Technology, China. Her current research interests include artificial intel- ligence, graph learning, and health informatics.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="91" height="120" src="data:image/jpg;base64,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"/></td></tr></table></span></p><p style="text-indent: 0pt;text-align: left;"><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="91" height="120" src="data:image/jpg;base64,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"/></td></tr></table></span></p><h4 style="padding-left: 90pt;text-indent: 0pt;text-align: justify;"><a name="bookmark270">&zwnj;</a>Ciyuan Peng <span class="s7">is a Ph.D. student at the Institute of Innovation, Science and Sustainability, Federation University Australia. She received the B.Sc. degree from Chongqing Normal University, China, in 2018, and the M.Sc. degree from Chung-Ang University, Korea, in 2020. Her research interests include data science, graph learning, brain science and knowledge graphs.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="96" height="105" src="data:image/jpg;base64,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"/></td></tr></table></span></p><h4 style="padding-left: 90pt;text-indent: 0pt;text-align: justify;"><a name="bookmark271">&zwnj;</a>Shuo Yu <span class="s7">(Member, IEEE) received B.Sc. and M.Sc. degrees from the School of Science, Shenyang Uni- versity of Technology, China. She received a Ph.D. degree from the School of Software, Dalian Univer- sity of Technology, China. Dr. Shuo Yu is currently an Associate Professor at the School of Computer Science and Technology, Dalian University of Tech- nology. She has published over 50 papers and re- ceived several academic awards, including the IEEE DataCom 2017 Best Paper Award, IEEE CSDE 2020 Best Paper Award, and ACM/IEEE JCDL 2020 The</span></h4><p class="s7" style="padding-left: 7pt;text-indent: 0pt;text-align: left;">Vannevar Bush Best Paper Honorable Mention. Her research interests include data science, graph learning, and knowledge science.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="86" height="117" src="data:image/jpg;base64,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"/></td></tr></table></span></p><h4 style="padding-left: 90pt;text-indent: 0pt;text-align: justify;"><a name="bookmark272">&zwnj;</a>Dongyu Zhang <span class="s7">received the MA degree in applied linguistics from Leicester University, UK; and the Ph.D. degree in computer application technology at Dalian University of Technology, Dalian, China. She is currently a full professor in the School of Software at Dalian University of Technology, Dalian, China. Her research interests include natural language pro- cessing, sentiment analysis, and social computing. She is a member of the Association for Compu- tational Linguistics, and the China Association of Artificial Intelligence.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="84" height="120" src="data:image/jpg;base64,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"/></td></tr></table></span></p><h4 style="padding-left: 90pt;text-indent: 0pt;text-align: justify;"><a name="bookmark273">&zwnj;</a>Karin Verspoor <span class="s7">received the BA degree in computer science and cognitive sciences from Rice Univer- sity (Houston, TX, USA) and the MSc and PhD degrees from the University of Edinburgh (UK). She is currently a Professor and Dean of the School of Computing Technologies, RMIT University in Melbourne, Australia. She has published over 300 papers, with a focus on extraction of information from clinical texts and the biomedical literature with natural language processing and machine learning- based modelling.</span></h4><p class="s7" style="padding-left: 7pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Distinguished Speaker.</p></body></html>
