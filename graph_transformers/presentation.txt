# Graph Transformers: Обзор современных подходов
## Презентация

### Слайд 1: Abstract
Graph Transformers (GTs) представляют собой мощную альтернативу Message Passing Neural Networks (MPNNs) в обучении на графах. Они решают проблемы:
- Дальние зависимости
- Over-squashing
- Ограничения MPNNs

[Текст для выступления]
Добрый день! Сегодня мы поговорим о Graph Transformers - современном подходе к работе с графовыми данными. В отличие от традиционных Message Passing Neural Networks, Graph Transformers предлагают более эффективное решение для работы с графами. Почему это важно? Потому что в реальных графах, таких как социальные сети или молекулярные структуры, важные связи часто находятся далеко друг от друга. Традиционные методы, основанные на передаче сообщений между соседними узлами, не могут эффективно обрабатывать такие дальние зависимости. Кроме того, при передаче информации через узкие места графа происходит эффект over-squashing, когда информация теряется или искажается. Graph Transformers решают эти проблемы, позволяя напрямую моделировать взаимодействия между любой парой узлов, независимо от их расстояния в графе.

### Слайд 2: Introduction
Графы - повсеместные структуры данных, представляющие отношения между сущностями. Традиционные методы обучения на графах (MPNNs) имеют ограничения:
- Сложность моделирования дальних зависимостей
- Проблемы с graph bottlenecks
- Over-squashing

[Текст для выступления]
Графы окружают нас повсюду - от социальных сетей до молекулярных структур. Давайте рассмотрим конкретный пример: в социальной сети важная информация может распространяться через цепочку друзей, где каждый узел передает сообщение только своим непосредственным соседям. Это приводит к двум основным проблемам. Во-первых, информация может искажаться при прохождении через длинные цепочки. Во-вторых, в местах, где много связей сходятся в одном узле (graph bottlenecks), происходит перегрузка информации - эффект over-squashing. Традиционные методы, основанные на передаче сообщений между соседними узлами, не могут эффективно решить эти проблемы, что ограничивает их применение в реальных задачах.

### Слайд 3: Notations and Preliminaries
#### Graph Notations
Граф \( G = (V,E) \):
- \( V = \{v_1,v_2,\ldots,v_N\} \) - множество nodes
- \( E = \{e_1,e_2,\ldots,e_M\} \) - множество edges
- Основные представления: adjacency matrix, edge list, node features, edge features

[Текст для выступления]
Прежде чем мы перейдем к деталям Graph Transformers, давайте вспомним основные понятия. Граф формально определяется как пара множеств: множество вершин и множество ребер. Возьмем для примера граф социальной сети: каждый узел может представлять пользователя с его характеристиками (возраст, интересы, местоположение), а каждое ребро - тип связи между пользователями (дружба, подписка, взаимодействие). Для эффективной работы с такими данными мы используем различные представления. Матрица смежности позволяет быстро определить, связаны ли два узла, но требует много памяти для разреженных графов. Список ребер более эффективен для хранения, но требует больше времени для поиска связей. Особенности узлов и ребер (node features и edge features) позволяют кодировать дополнительную информацию, что критично для многих приложений.

### Слайд 4: Graph Neural Networks
Два основных подхода:
1. Spectral Methods
   - Graph signal processing
   - Graph Fourier transform
   - Высокая вычислительная сложность

2. Spatial Methods
   - Message-passing framework
   - Neighborhood aggregation
   - Ограничения в дальних зависимостях

[Текст для выступления]
Традиционные подходы к работе с графами можно разделить на два основных направления. Spectral методы используют преобразование Фурье для графов, что позволяет работать с глобальной структурой. Например, в задаче кластеризации социальной сети, spectral методы могут эффективно выявлять сообщества, но требуют больших вычислительных ресурсов - сложность может достигать O(N³). Spatial методы, основанные на передаче сообщений между соседними узлами, более эффективны - их сложность линейна по числу ребер. Однако они имеют фундаментальное ограничение: информация может передаваться только между соседними узлами. Это означает, что для передачи информации через k шагов требуется k слоев сети, что приводит к проблемам с градиентами и потере информации. Именно эти ограничения и привели к появлению Graph Transformers.

### Слайд 5: Transformer Architecture
Core компоненты:
- Self-attention mechanism
- Positional encodings
- Multi-head attention
- Encoder-Decoder структура

[Текст для выступления]
Основу Graph Transformers составляет архитектура трансформера, которая уже доказала свою эффективность в обработке естественного языка и компьютерном зрении. Ключевой компонент - механизм self-attention, который позволяет модели напрямую вычислять взаимодействия между любой парой узлов. Например, в задаче рекомендаций, self-attention может напрямую связать пользователя с релевантным товаром, даже если они находятся далеко друг от друга в графе взаимодействий. Positional encodings помогают сохранить информацию о структуре графа - они могут кодировать как локальные свойства (например, степень узла), так и глобальные (например, центральность). Multi-head attention позволяет модели фокусироваться на разных аспектах графовых данных одновременно - один head может обращать внимание на структурные свойства, другой на семантические характеристики.

### Слайд 6: Graph Inductive Bias
Четыре типа bias:
1. Node Positional Bias
   - Local encodings (hop distance)
   - Global encodings (Laplacian eigenvectors)

2. Edge Structural Bias
   - Local edge features
   - Global structural patterns

3. Message-passing Bias
4. Attention Bias

[Текст для выступления]
Одна из ключевых особенностей Graph Transformers - это способность инкорпорировать различные типы индуктивных смещений. Рассмотрим их подробнее. Node Positional Bias кодирует информацию о положении узлов. Например, в молекулярном графе, локальные кодировки могут отражать тип атома и его ближайшее окружение, а глобальные - положение в молекуле. Edge Structural Bias учитывает особенности ребер - в социальной сети это может быть тип связи (дружба, коллега, родственник) или сила взаимодействия. Message-passing Bias помогает контролировать поток информации - в рекомендательной системе мы можем ограничить передачу информации только между похожими пользователями. Attention Bias позволяет модели фокусироваться на наиболее релевантных связях - например, в задаче предсказания взаимодействия лекарств, модель может уделять больше внимания химически совместимым молекулам.

### Слайд 7: Graph Attention Mechanisms
#### Global Attention
- Quadratic attention (\( O(N^2) \))
- Linear attention (\( O(N) \))
- Kernel-based и locality-sensitive подходы

#### Local Attention
- Message-passing attention
- Spectral attention
- Эффективность для больших графов

[Текст для выступления]
Механизмы внимания в Graph Transformers можно разделить на глобальные и локальные. Глобальное внимание позволяет модели видеть весь граф целиком, но имеет квадратичную сложность O(N²). Это становится проблемой для больших графов - например, в социальной сети с миллионами пользователей. Для решения этой проблемы были разработаны линейные и kernel-based подходы. Linear attention снижает сложность до O(N), используя низкоранговые аппроксимации. Kernel-based подходы используют ядерные функции для эффективного вычисления сходства. Локальное внимание фокусируется на окрестности каждого узла - в задаче обнаружения аномалий в сети это позволяет эффективно выявлять локальные отклонения от нормального поведения. Однако локальное внимание может пропускать важные дальние зависимости - например, в задаче предсказания распространения вируса в социальной сети.

### Слайд 8: Taxonomy of Graph Transformers
1. Shallow Graph Transformers
   - Один слой
   - Эффективность
   - Ограниченная выразительность

2. Deep Graph Transformers
   - Множество слоев
   - Сложные паттерны
   - Проблемы оптимизации

3. Scalable Graph Transformers
   - Эффективность для больших графов
   - Оптимизация памяти
   - Sparsification

4. Pre-trained Graph Transformers
   - Transfer learning
   - Self-supervised learning
   - Fine-tuning

[Текст для выступления]
Graph Transformers можно классифицировать по нескольким критериям. Shallow модели используют один слой трансформера и эффективны для простых задач. Например, в задаче классификации узлов в небольшой социальной сети, shallow модель может достичь хороших результатов при минимальных вычислительных затратах. Deep модели, с множеством слоев, способны улавливать сложные паттерны - в задаче предсказания свойств молекул они могут учитывать взаимодействия между далекими атомами. Однако они требуют тщательной оптимизации из-за проблем с градиентами. Scalable модели специально разработаны для работы с большими графами - в веб-графе с миллиардами страниц они используют техники разрежения и выборки для эффективной обработки. Pre-trained модели используют предварительное обучение на больших наборах данных - например, в задаче предсказания взаимодействия лекарств, модель может быть предварительно обучена на большой базе известных взаимодействий, а затем дообучена на конкретной задаче.

### Слайд 9: Application Perspectives
#### Node-level Tasks
- Protein Structure Prediction
- Entity Resolution
- Anomaly Detection

#### Edge-level Tasks
- Drug-Drug Interaction Prediction
- Knowledge Graph Completion
- Recommender Systems

#### Graph-level Tasks
- Molecular Property Prediction
- Graph Clustering
- Graph Synthesis

[Текст для выступления]
Graph Transformers находят применение в самых разных областях. На уровне узлов они используются для предсказания структуры белков - здесь важно учитывать как локальные взаимодействия между аминокислотами, так и глобальную структуру белка. В задаче разрешения сущностей (Entity Resolution) модель должна находить соответствия между записями в разных базах данных, учитывая как атрибуты записей, так и связи между ними. На уровне ребер Graph Transformers успешно применяются для предсказания взаимодействия лекарств - модель анализирует как химическую структуру молекул, так и известные взаимодействия между ними. В рекомендательных системах они учитывают как предпочтения пользователей, так и структуру социального графа. На уровне всего графа они используются для предсказания свойств молекул, где важно учитывать всю структуру молекулы целиком.

### Слайд 10: Open Issues and Future Directions
1. Scalability and Efficiency
   - Оптимизация памяти
   - Эффективные attention механизмы
   - Graph sparsification

2. Generalization and Robustness
   - Адаптивные механизмы
   - Domain adaptation
   - Устойчивость к атакам

3. Interpretability and Explainability
   - Визуализация attention
   - XAI методы
   - Объяснимость решений

4. Learning on Dynamic Graphs
   - Временные механизмы
   - Continual learning
   - Мультимодальные подходы

5. Data Quality and Sparsity
   - Data augmentation
   - Semi-supervised learning
   - Graph imputation

[Текст для выступления]
Несмотря на успехи Graph Transformers, остается множество открытых вопросов. Масштабируемость и эффективность требуют разработки новых оптимизационных подходов - например, в социальных сетях с миллиардами пользователей текущие реализации могут быть слишком ресурсоемкими. Обобщаемость и устойчивость к атакам критичны для практического применения - в финансовых системах модель должна быть устойчива к попыткам манипуляции данными. Интерпретируемость решений важна во многих областях - в медицине врачи должны понимать, почему модель рекомендует определенное лечение. Работа с динамическими графами представляет особый вызов - в социальных сетях связи постоянно меняются, и модель должна адаптироваться к этим изменениям. Обработка разреженных данных требует новых подходов - в рекомендательных системах большинство пользователей взаимодействует только с небольшой частью контента.

### Слайд 11: Conclusion
Graph Transformers - мощный инструмент для работы с графовыми данными, но остаются вызовы:
- Масштабируемость
- Обобщаемость
- Интерпретируемость
- Работа с динамическими графами
- Качество данных

[Текст для выступления]
Подводя итоги, можно сказать, что Graph Transformers представляют собой мощный инструмент для работы с графовыми данными, объединяя преимущества трансформеров с возможностями работы с графами. Они успешно решают проблемы традиционных методов, такие как обработка дальних зависимостей и over-squashing. Однако для широкого практического применения необходимо решить ряд важных задач. Масштабируемость остается ключевым вызовом для работы с большими графами. Обобщаемость моделей на новые домены требует разработки новых подходов к transfer learning. Интерпретируемость решений критична для многих приложений, особенно в медицине и финансах. Работа с динамическими графами требует новых архитектурных решений. Будущее Graph Transformers видится в разработке более эффективных и устойчивых архитектур, способных работать с динамическими и мультимодальными данными.

[Финальный вопрос аудитории] Какие из рассмотренных направлений кажутся вам наиболее перспективными для применения в ваших задачах?

Спасибо за внимание! Буду рад ответить на ваши вопросы. 